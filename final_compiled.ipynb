{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b36490",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "df = pd.read_csv(\"MiNDAT.csv\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nInfo:\")\n",
    "print(df.info())\n",
    "print(\"\\nSummary Stats:\")\n",
    "print(df.describe().T.head())\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=missing[:20].values, y=missing[:20].index, palette=\"viridis\")\n",
    "plt.title(\"Missing Values per Feature (Top 20)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df['CORRUCYSTIC_DENSITY'].dropna(), bins=40, kde=True, color=\"royalblue\")\n",
    "plt.title(\"Distribution of CORRUCYSTIC_DENSITY\")\n",
    "plt.show()\n",
    "\n",
    "corr = df.corr(numeric_only=True)\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr[['CORRUCYSTIC_DENSITY']].sort_values(by='CORRUCYSTIC_DENSITY', ascending=False),\n",
    "            annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation of Features with Target\")\n",
    "plt.show()\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(\"\\nCategorical columns:\", categorical_cols.tolist())\n",
    "\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.boxplot(x=col, y=\"CORRUCYSTIC_DENSITY\", data=df)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Boxplot of {col} vs CORRUCYSTIC_DENSITY\")\n",
    "    plt.show()\n",
    "\n",
    "top_corr_features = corr['CORRUCYSTIC_DENSITY'].abs().sort_values(ascending=False).head(6).index\n",
    "print(\"\\nTop correlated features with target:\", top_corr_features.tolist())\n",
    "\n",
    "sns.pairplot(df[top_corr_features].dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a7667",
   "metadata": {},
   "source": [
    "# Initial Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv(r\"/kaggle/input/recruitment-task-for-gdsc-ml/MiNDAT.csv\")\n",
    "    test = pd.read_csv(r\"/kaggle/input/recruitment-task-for-gdsc-ml/MiNDAT_UNK.csv\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def preprocess(train, test, target=\"CORRUCYSTIC_DENSITY\", id_col=\"LOCAL_IDENTIFIER\"):\n",
    "    # drop rows with missing target\n",
    "    train = train.dropna(subset=[target])\n",
    "\n",
    "    features = [c for c in train.columns if c not in [target, id_col]]\n",
    "    X_train, y_train = train[features], train[target]\n",
    "    X_test = test[features]\n",
    "\n",
    "    # encode categoricals\n",
    "    cat_cols = X_train.select_dtypes(include=\"object\").columns\n",
    "    encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        data = pd.concat([X_train[col], X_test[col]]).fillna(\"missing\")\n",
    "        le.fit(data)\n",
    "        X_train[col] = le.transform(X_train[col].fillna(\"missing\"))\n",
    "        X_test[col] = le.transform(X_test[col].fillna(\"missing\"))\n",
    "        encoders[col] = le\n",
    "\n",
    "    # impute + scale numerics\n",
    "    num_cols = X_train.select_dtypes(include=np.number).columns\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train[num_cols] = imputer.fit_transform(X_train[num_cols])\n",
    "    X_test[num_cols] = imputer.transform(X_test[num_cols])\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=features)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=features)\n",
    "\n",
    "    return X_train, X_test, y_train\n",
    "\n",
    "\n",
    "def train_models(X, y):\n",
    "    models = {\n",
    "        \"RF\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        \"GB\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        \"Ridge\": Ridge(alpha=1.0),\n",
    "        \"Lasso\": Lasso(alpha=1.0),\n",
    "    }\n",
    "\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scores, best_rmse, best_model = {}, float(\"inf\"), None\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, preds, squared=False)\n",
    "        scores[name] = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mean_absolute_error(y_val, preds),\n",
    "            \"R2\": r2_score(y_val, preds),\n",
    "        }\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse, best_model = rmse, model\n",
    "\n",
    "    # retrain best on full data\n",
    "    best_model.fit(X, y)\n",
    "    return best_model, scores\n",
    "\n",
    "\n",
    "def tune_model(model, X, y):\n",
    "    grids = {\n",
    "        \"RandomForestRegressor\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [10, 20, None],\n",
    "        },\n",
    "        \"GradientBoostingRegressor\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [3, 5],\n",
    "        },\n",
    "        \"Ridge\": {\"alpha\": [0.1, 1, 10]},\n",
    "        \"Lasso\": {\"alpha\": [0.01, 0.1, 1]},\n",
    "    }\n",
    "\n",
    "    name = model.__class__.__name__\n",
    "    if name not in grids:\n",
    "        return model\n",
    "\n",
    "    grid = GridSearchCV(model, grids[name], cv=3, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "train, test = load_data()\n",
    "X_train, X_test, y_train = preprocess(train, test)\n",
    "\n",
    "best_model, scores = train_models(X_train, y_train)\n",
    "tuned = tune_model(best_model, X_train, y_train)\n",
    "\n",
    "preds = tuned.predict(X_test)\n",
    "sub = pd.DataFrame({\n",
    "    \"LOCAL_IDENTIFIER\": test[\"LOCAL_IDENTIFIER\"],\n",
    "    \"CORRUCYSTIC_DENSITY\": preds\n",
    "})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Saved submission:\", sub.shape)\n",
    "\n",
    "model, submission = tuned, sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe8d0e",
   "metadata": {},
   "source": [
    "# Version - 2 (Used better feature engineering, increased number of models...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e4463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb, lightgbm as lgb\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load Data\n",
    "train = pd.read_csv(r\"/kaggle/input/recruitment-task-for-gdsc-ml/MiNDAT.csv\")\n",
    "test = pd.read_csv(r\"/kaggle/input/recruitment-task-for-gdsc-ml/MiNDAT_UNK.csv\")\n",
    "\n",
    "# Target / ID\n",
    "target, id_col = \"CORRUCYSTIC_DENSITY\", \"LOCAL_IDENTIFIER\"\n",
    "train = train.dropna(subset=[target])\n",
    "\n",
    "# Outlier Removal\n",
    "Q1, Q3 = train[target].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lb, ub = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "train = train[(train[target] >= lb) & (train[target] <= ub)]\n",
    "\n",
    "features = [c for c in train.columns if c not in [target, id_col]]\n",
    "X, y = train[features], train[target]\n",
    "X_test = test[features]\n",
    "\n",
    "# Encode categoricals\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    data = pd.concat([X[col], X_test[col]]).fillna(\"missing\")\n",
    "    le.fit(data)\n",
    "    X[col] = le.transform(X[col].fillna(\"missing\"))\n",
    "    X_test[col] = le.transform(X_test[col].fillna(\"missing\"))\n",
    "\n",
    "# Handle numericals\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "for col in num_cols:\n",
    "    X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
    "    X_test[col] = X_test[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X[num_cols] = imputer.fit_transform(X[num_cols])\n",
    "X_test[num_cols] = imputer.transform(X_test[num_cols])\n",
    "\n",
    "# Feature Engineering\n",
    "imp_feats = num_cols[:10]\n",
    "for i, f1 in enumerate(imp_feats):\n",
    "    for f2 in imp_feats[i+1:i+3]:\n",
    "        X[f'{f1}_x_{f2}'] = X[f1]*X[f2]\n",
    "        X_test[f'{f1}_x_{f2}'] = X_test[f1]*X_test[f2]\n",
    "        X[f'{f1}_div_{f2}'] = X[f1]/(X[f2]+1e-8)\n",
    "        X_test[f'{f1}_div_{f2}'] = X_test[f1]/(X_test[f2]+1e-8)\n",
    "\n",
    "for f in num_cols[:5]:\n",
    "    X[f'{f}_squared'] = X[f]**2\n",
    "    X_test[f'{f}_squared'] = X_test[f]**2\n",
    "    X[f'{f}_sqrt'] = np.sqrt(np.abs(X[f]))\n",
    "    X_test[f'{f}_sqrt'] = np.sqrt(np.abs(X_test[f]))\n",
    "\n",
    "# col name fix for XGBoost / LightGBM\n",
    "import re\n",
    "\n",
    "# Clean feature names for LightGBM / XGBoost compatibility\n",
    "def clean_column(name):\n",
    "    # Replace any non-alphanumeric character with underscore\n",
    "    return re.sub(r'[^A-Za-z0-9_]', '_', str(name))\n",
    "\n",
    "X.columns = [clean_column(c) for c in X.columns]\n",
    "X_test.columns = [clean_column(c) for c in X_test.columns]\n",
    "# Scaling\n",
    "scaler = RobustScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"RF\": RandomForestRegressor(n_estimators=300, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    \"ET\": ExtraTreesRegressor(n_estimators=300, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    \"GB\": GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, random_state=42),\n",
    "    \"XGB\": xgb.XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42),\n",
    "    \"LGB\": lgb.LGBMRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42, verbose=-1),\n",
    "    \"Ridge\": Ridge(alpha=10.0),\n",
    "    \"Lasso\": Lasso(alpha=1.0),\n",
    "    \"ElasticNet\": ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "}\n",
    "\n",
    "# CV Training\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores, trained_models = {}, {}\n",
    "for name, model in models.items():\n",
    "    rmses, folds = [], []\n",
    "    for tr, val in kf.split(X):\n",
    "        m = type(model)(**model.get_params())\n",
    "        m.fit(X.iloc[tr], y.iloc[tr])\n",
    "        p = m.predict(X.iloc[val])\n",
    "        rmses.append(mean_squared_error(y.iloc[val], p, squared=False))\n",
    "        folds.append(m)\n",
    "    scores[name] = {\"CV_RMSE\": np.mean(rmses), \"CV_STD\": np.std(rmses)}\n",
    "    trained_models[name] = folds\n",
    "    print(f\"{name}: {np.mean(rmses):.4f} (+/- {np.std(rmses):.4f})\")\n",
    "\n",
    "best_model_name = min(scores, key=lambda x: scores[x][\"CV_RMSE\"])\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "# hyperparameter Tuning (grid only for best model)\n",
    "grids = {\n",
    "    \"RF\": {\"n_estimators\":[200,300,400],\"max_depth\":[10,15,20]},\n",
    "    \"ET\": {\"n_estimators\":[200,300,400],\"max_depth\":[10,15,20]},\n",
    "    \"GB\": {\"n_estimators\":[150,200,250],\"learning_rate\":[0.03,0.05,0.07]},\n",
    "    \"XGB\": {\"n_estimators\":[200,300,400],\"learning_rate\":[0.03,0.05,0.07]},\n",
    "    \"LGB\": {\"n_estimators\":[200,300,400],\"learning_rate\":[0.03,0.05,0.07]},\n",
    "    \"Ridge\": {\"alpha\":[0.1,1,10,100]},\n",
    "    \"Lasso\": {\"alpha\":[0.01,0.1,1,10]},\n",
    "    \"ElasticNet\": {\"alpha\":[0.1,1,10],\"l1_ratio\":[0.1,0.5,0.9]}\n",
    "}\n",
    "if best_model_name in grids:\n",
    "    base = models[best_model_name]\n",
    "    grid = GridSearchCV(base, grids[best_model_name], cv=3,\n",
    "                        scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose=1)\n",
    "    grid.fit(X, y)\n",
    "    tuned_model = grid.best_estimator_\n",
    "    print(f\"Best tuned params: {grid.best_params_}\")\n",
    "else:\n",
    "    tuned_model = None\n",
    "\n",
    "# Ensemble (top 3 weighted by RMSE)\n",
    "top3 = sorted(scores.items(), key=lambda x: x[1][\"CV_RMSE\"])[:3]\n",
    "ens, total_w = None, 0\n",
    "for name, score in top3:\n",
    "    w = 1.0/score[\"CV_RMSE\"]\n",
    "    total_w += w\n",
    "    fold_preds = np.mean([m.predict(X_test) for m in trained_models[name]], axis=0)\n",
    "    ens = w*fold_preds if ens is None else ens + w*fold_preds\n",
    "ensemble_preds = ens/total_w\n",
    "\n",
    "# Final Predictions\n",
    "if tuned_model is not None:\n",
    "    tuned_preds = tuned_model.predict(X_test)\n",
    "    final_preds = ensemble_preds\n",
    "else:\n",
    "    final_preds = ensemble_preds\n",
    "\n",
    "sub = pd.DataFrame({id_col: test[id_col], target: final_preds})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\n--- FINAL RESULTS ---\")\n",
    "for n, s in sorted(scores.items(), key=lambda x: x[1][\"CV_RMSE\"]):\n",
    "    print(f\"{n:10} - CV RMSE: {s['CV_RMSE']:.4f} (+/- {s['CV_STD']:.4f})\")\n",
    "print(f\"\\nSubmission shape: {sub.shape}\")\n",
    "print(f\"Pred stats - mean: {final_preds.mean():.2f}, std: {final_preds.std():.2f}, \"\n",
    "      f\"min: {final_preds.min():.2f}, max: {final_preds.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a59c50",
   "metadata": {},
   "source": [
    "# After this point I started running the models locally since online on Kaggle or Colab was slow and prone to runtime disconnections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3fd15",
   "metadata": {},
   "source": [
    "## Approach 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1857d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, warnings\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge, HuberRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import xgboost as xgb, lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load Data - CORRUCYST Fragment Analysis\n",
    "train = pd.read_csv(\"MiNDAT.csv\")\n",
    "test = pd.read_csv(\"MiNDAT_UNK.csv\")\n",
    "\n",
    "# Target / ID - CORRUCYSTIC_DENSITY represents fragment data integrity\n",
    "target, id_col = \"CORRUCYSTIC_DENSITY\", \"LOCAL_IDENTIFIER\"\n",
    "train = train.dropna(subset=[target])\n",
    "\n",
    "print(f\"Initial dataset: {train.shape[0]} fragments\")\n",
    "print(f\"Target distribution - Mean: {train[target].mean():.4f}, Std: {train[target].std():.4f}\")\n",
    "print(f\"Target range: [{train[target].min():.4f}, {train[target].max():.4f}]\")\n",
    "\n",
    "def detect_outliers_iqr(data, multiplier=1.5):\n",
    "    Q1, Q3 = data.quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    return (data < Q1 - multiplier*IQR) | (data > Q3 + multiplier*IQR)\n",
    "\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    z_scores = np.abs((data - data.mean()) / data.std())\n",
    "    return z_scores > threshold\n",
    "\n",
    "outliers_iqr = detect_outliers_iqr(train[target], multiplier=2.0)\n",
    "outliers_zscore = detect_outliers_zscore(train[target], threshold=3.5)\n",
    "extreme_outliers = outliers_iqr & outliers_zscore\n",
    "\n",
    "print(f\"Removing {extreme_outliers.sum()} extreme outliers ({extreme_outliers.sum()/len(train)*100:.2f}%)\")\n",
    "train = train[~extreme_outliers]\n",
    "\n",
    "features = [c for c in train.columns if c not in [target, id_col]]\n",
    "X, y = train[features], train[target]\n",
    "X_test = test[features]\n",
    "\n",
    "print(f\"Feature space: {len(features)} dimensions\")\n",
    "print(f\"Training fragments: {X.shape[0]}\")\n",
    "print(f\"Test fragments: {X_test.shape[0]}\")\n",
    "\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns\n",
    "print(f\"Categorical features (alien classifications): {len(cat_cols)}\")\n",
    "\n",
    "# Check for high cardinality categories that might need special handling\n",
    "for col in cat_cols:\n",
    "    unique_train = X[col].nunique()\n",
    "    unique_total = pd.concat([X[col], X_test[col]]).nunique()\n",
    "    print(f\"  {col}: {unique_train} train classes, {unique_total} total classes\")\n",
    "\n",
    "# Advanced categorical encoding\n",
    "for col in cat_cols:\n",
    "    combined_data = pd.concat([X[col], X_test[col]]).fillna(\"CORRUPTED_DATA\")\n",
    "    if combined_data.nunique() > 20:\n",
    "        target_means = train.groupby(col)[target].mean()\n",
    "        global_mean = train[target].mean()\n",
    "        counts = train.groupby(col).size()\n",
    "        smoothing_factor = 10\n",
    "        smoothed_means = (target_means * counts + global_mean * smoothing_factor) / (counts + smoothing_factor)\n",
    "        \n",
    "        X[col] = X[col].map(smoothed_means).fillna(global_mean)\n",
    "        X_test[col] = X_test[col].map(smoothed_means).fillna(global_mean)\n",
    "    else:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(combined_data)\n",
    "        X[col] = le.transform(X[col].fillna(\"CORRUPTED_DATA\"))\n",
    "        X_test[col] = le.transform(X_test[col].fillna(\"CORRUPTED_DATA\"))\n",
    "\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "print(f\"Numerical features (sensor readings): {len(num_cols)}\")\n",
    "\n",
    "for col in num_cols:\n",
    "    # Replace infinities with NaN for proper imputation\n",
    "    X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
    "    X_test[col] = X_test[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    if X[col].notna().sum() > 10:\n",
    "        mean_val = X[col].mean()\n",
    "        std_val = X[col].std()\n",
    "        if std_val > 0:\n",
    "            lower_bound = mean_val - 5 * std_val\n",
    "            upper_bound = mean_val + 5 * std_val\n",
    "            X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "            X_test[col] = X_test[col].clip(lower_bound, upper_bound)\n",
    "\n",
    "# Advanced Imputation Strategy\n",
    "print(\"Applying advanced imputation for missing alien data...\")\n",
    "\n",
    "imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X[num_cols]),\n",
    "    columns=num_cols,\n",
    "    index=X.index\n",
    ")\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_test[num_cols]),\n",
    "    columns=num_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "X[num_cols] = X_imputed\n",
    "X_test[num_cols] = X_test_imputed\n",
    "\n",
    "print(\"Engineering features for CORRUCYST fragment analysis...\")\n",
    "\n",
    "feature_importance_selector = SelectKBest(f_regression, k=min(15, len(num_cols)))\n",
    "feature_importance_selector.fit(X[num_cols], y)\n",
    "important_features = num_cols[feature_importance_selector.get_support()]\n",
    "\n",
    "print(f\"Selected {len(important_features)} most important features for engineering\")\n",
    "\n",
    "for i, f1 in enumerate(important_features[:8]):  \n",
    "    for j, f2 in enumerate(important_features[i+1:i+4]):  \n",
    "        X[f'{f1}_x_{f2}'] = X[f1] * X[f2]\n",
    "        X_test[f'{f1}_x_{f2}'] = X_test[f1] * X_test[f2]\n",
    "        \n",
    "        X[f'{f1}_div_{f2}'] = X[f1] / (np.abs(X[f2]) + 1e-8)\n",
    "        X_test[f'{f1}_div_{f2}'] = X_test[f1] / (np.abs(X_test[f2]) + 1e-8)\n",
    "\n",
    "for f in important_features[:6]:\n",
    "    X[f'{f}_squared'] = X[f] ** 2\n",
    "    X_test[f'{f}_squared'] = X_test[f] ** 2\n",
    "    \n",
    "    X[f'{f}_cbrt'] = np.sign(X[f]) * np.abs(X[f]) ** (1/3)\n",
    "    X_test[f'{f}_cbrt'] = np.sign(X_test[f]) * np.abs(X_test[f]) ** (1/3)\n",
    "    \n",
    "    X[f'{f}_log'] = np.log1p(np.abs(X[f]))\n",
    "    X_test[f'{f}_log'] = np.log1p(np.abs(X_test[f]))\n",
    "\n",
    "if len(important_features) > 5:\n",
    "    X['important_mean'] = X[important_features].mean(axis=1)\n",
    "    X['important_std'] = X[important_features].std(axis=1)\n",
    "    X['important_skew'] = X[important_features].skew(axis=1)\n",
    "    X['important_max'] = X[important_features].max(axis=1)\n",
    "    X['important_min'] = X[important_features].min(axis=1)\n",
    "    \n",
    "    X_test['important_mean'] = X_test[important_features].mean(axis=1)\n",
    "    X_test['important_std'] = X_test[important_features].std(axis=1)\n",
    "    X_test['important_skew'] = X_test[important_features].skew(axis=1)\n",
    "    X_test['important_max'] = X_test[important_features].max(axis=1)\n",
    "    X_test['important_min'] = X_test[important_features].min(axis=1)\n",
    "\n",
    "energy_features = [f for f in X.columns if any(keyword in f.lower() for keyword in ['energy', 'power', 'volt', 'amp'])]\n",
    "if len(energy_features) >= 2:\n",
    "    X['energy_coherence'] = X[energy_features].sum(axis=1) / (X[energy_features].std(axis=1) + 1e-8)\n",
    "    X_test['energy_coherence'] = X_test[energy_features].sum(axis=1) / (X_test[energy_features].std(axis=1) + 1e-8)\n",
    "\n",
    "print(f\"Feature engineering complete. New feature count: {X.shape[1]}\")\n",
    "\n",
    "def clean_column(name):\n",
    "    return re.sub(r'[^A-Za-z0-9_]', '_', str(name))\n",
    "\n",
    "X.columns = [clean_column(c) for c in X.columns]\n",
    "X_test.columns = [clean_column(c) for c in X_test.columns]\n",
    "\n",
    "print(\"Applying robust scaling for alien biocomputer measurements...\")\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Keep original data for tree-based models and scaled for linear models\n",
    "X_original = X.copy()\n",
    "X_test_original = X_test.copy()\n",
    "X = X_scaled\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(f\"Final feature matrix: {X.shape} for training, {X_test.shape} for prediction\")\n",
    "\n",
    "models = {\n",
    "    \"RF\": RandomForestRegressor(n_estimators=500, max_depth=20, random_state=42, n_jobs=-1, \n",
    "                               min_samples_split=3, min_samples_leaf=2),\n",
    "    \"ET\": ExtraTreesRegressor(n_estimators=500, max_depth=20, random_state=42, n_jobs=-1,\n",
    "                             min_samples_split=3, min_samples_leaf=2),\n",
    "    \"GB\": GradientBoostingRegressor(n_estimators=300, learning_rate=0.03, max_depth=8, \n",
    "                                   random_state=42, subsample=0.8),\n",
    "    \"XGB\": xgb.XGBRegressor(n_estimators=500, learning_rate=0.03, max_depth=8, \n",
    "                           random_state=42, subsample=0.8, colsample_bytree=0.8),\n",
    "    \"LGB\": lgb.LGBMRegressor(n_estimators=500, learning_rate=0.03, max_depth=8, \n",
    "                            random_state=42, subsample=0.8, colsample_bytree=0.8, verbose=-1),\n",
    "    \"CatBoost\": CatBoostRegressor(iterations=500, learning_rate=0.03, depth=8, \n",
    "                                 random_seed=42, verbose=False),\n",
    "    \"Ridge\": Ridge(alpha=10.0),\n",
    "    \"Lasso\": Lasso(alpha=1.0, max_iter=2000),\n",
    "    \"ElasticNet\": ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=2000),\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "    \"Huber\": HuberRegressor(epsilon=1.35, max_iter=500),\n",
    "}\n",
    "\n",
    "# Tree-based models use original data, linear models use scaled data\n",
    "tree_models = [\"RF\", \"ET\", \"GB\", \"XGB\", \"LGB\", \"CatBoost\"]\n",
    "linear_models = [\"Ridge\", \"Lasso\", \"ElasticNet\", \"BayesianRidge\", \"Huber\"]\n",
    "\n",
    "# Enhanced Cross-Validation with Multiple Metrics\n",
    "kf = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "scores, trained_models = {}, {}\n",
    "\n",
    "print(\"\\n=== CORRUCYST Fragment Analysis Results ===\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} for alien biocomputer analysis...\")\n",
    "\n",
    "    X_train = X_original if name in tree_models else X\n",
    "    X_test_model = X_test_original if name in tree_models else X_test\n",
    "    \n",
    "    rmses, maes, r2s, folds = [], [], [], []\n",
    "    for tr, val in kf.split(X_train):\n",
    "        m = type(model)(**model.get_params())\n",
    "        m.fit(X_train.iloc[tr], y.iloc[tr])\n",
    "        p = m.predict(X_train.iloc[val])\n",
    "        \n",
    "        rmses.append(mean_squared_error(y.iloc[val], p, squared=False))\n",
    "        maes.append(mean_absolute_error(y.iloc[val], p))\n",
    "        r2s.append(r2_score(y.iloc[val], p))\n",
    "        folds.append(m)\n",
    "    \n",
    "    scores[name] = {\n",
    "        \"CV_RMSE\": np.mean(rmses), \"CV_RMSE_STD\": np.std(rmses),\n",
    "        \"CV_MAE\": np.mean(maes), \"CV_MAE_STD\": np.std(maes),\n",
    "        \"CV_R2\": np.mean(r2s), \"CV_R2_STD\": np.std(r2s)\n",
    "    }\n",
    "    trained_models[name] = folds\n",
    "    \n",
    "    print(f\"{name:12} - RMSE: {np.mean(rmses):.4f} (±{np.std(rmses):.4f}) | \"\n",
    "          f\"MAE: {np.mean(maes):.4f} (±{np.std(maes):.4f}) | \"\n",
    "          f\"R²: {np.mean(r2s):.4f} (±{np.std(r2s):.4f})\")\n",
    "\n",
    "best_model_name = min(scores, key=lambda x: scores[x][\"CV_RMSE\"])\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "\n",
    "print(f\"\\nPerforming advanced hyperparameter tuning for {best_model_name}...\")\n",
    "\n",
    "hyperparameter_grids = {\n",
    "    \"RF\": {\n",
    "        \"n_estimators\": [300, 500, 700],\n",
    "        \"max_depth\": [15, 20, 25],\n",
    "        \"min_samples_split\": [2, 3, 5],\n",
    "        \"min_samples_leaf\": [1, 2, 3]\n",
    "    },\n",
    "    \"ET\": {\n",
    "        \"n_estimators\": [300, 500, 700],\n",
    "        \"max_depth\": [15, 20, 25],\n",
    "        \"min_samples_split\": [2, 3, 5],\n",
    "        \"min_samples_leaf\": [1, 2, 3]\n",
    "    },\n",
    "    \"GB\": {\n",
    "        \"n_estimators\": [200, 300, 400],\n",
    "        \"learning_rate\": [0.02, 0.03, 0.05],\n",
    "        \"max_depth\": [6, 8, 10],\n",
    "        \"subsample\": [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"n_estimators\": [300, 500, 700],\n",
    "        \"learning_rate\": [0.02, 0.03, 0.05],\n",
    "        \"max_depth\": [6, 8, 10],\n",
    "        \"subsample\": [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \"LGB\": {\n",
    "        \"n_estimators\": [300, 500, 700],\n",
    "        \"learning_rate\": [0.02, 0.03, 0.05],\n",
    "        \"max_depth\": [6, 8, 10],\n",
    "        \"subsample\": [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"iterations\": [300, 500, 700],\n",
    "        \"learning_rate\": [0.02, 0.03, 0.05],\n",
    "        \"depth\": [6, 8, 10]\n",
    "    },\n",
    "    \"Ridge\": {\"alpha\": [0.1, 1, 10, 100, 1000]},\n",
    "    \"Lasso\": {\"alpha\": [0.001, 0.01, 0.1, 1, 10]},\n",
    "    \"ElasticNet\": {\"alpha\": [0.1, 1, 10], \"l1_ratio\": [0.1, 0.5, 0.7, 0.9]},\n",
    "    \"BayesianRidge\": {\"alpha_1\": [1e-6, 1e-5, 1e-4], \"alpha_2\": [1e-6, 1e-5, 1e-4]},\n",
    "    \"Huber\": {\"epsilon\": [1.1, 1.35, 1.5, 2.0], \"alpha\": [0.0001, 0.001, 0.01]}\n",
    "}\n",
    "\n",
    "tuned_model = None\n",
    "if best_model_name in hyperparameter_grids:\n",
    "    base_model = models[best_model_name]\n",
    "    X_tune = X_original if best_model_name in tree_models else X\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        base_model, \n",
    "        hyperparameter_grids[best_model_name], \n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_squared_error\", \n",
    "        n_jobs=-1, \n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(X_tune, y)\n",
    "    tuned_model = grid.best_estimator_\n",
    "    print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "    print(f\"Tuned model CV score: {-grid.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "# Top performing models weighted ensemble\n",
    "top_models = sorted(scores.items(), key=lambda x: x[1][\"CV_RMSE\"])[:5]\n",
    "print(\"Top 5 models for ensemble:\")\n",
    "for name, score in top_models:\n",
    "    print(f\"  {name}: RMSE {score['CV_RMSE']:.4f}\")\n",
    "\n",
    "# Weight by inverse RMSE with exponential scaling\n",
    "ensemble_weights = {}\n",
    "total_weight = 0\n",
    "for name, score in top_models:\n",
    "    weight = np.exp(-score[\"CV_RMSE\"] * 2)\n",
    "    ensemble_weights[name] = weight\n",
    "    total_weight += weight\n",
    "\n",
    "# Normalize weights\n",
    "for name in ensemble_weights:\n",
    "    ensemble_weights[name] /= total_weight\n",
    "    print(f\"  {name} weight: {ensemble_weights[name]:.3f}\")\n",
    "\n",
    "# Create ensemble predictions\n",
    "ensemble_preds = np.zeros(X_test.shape[0])\n",
    "for name, weight in ensemble_weights.items():\n",
    "    X_pred = X_test_original if name in tree_models else X_test\n",
    "    fold_preds = np.mean([m.predict(X_pred) for m in trained_models[name]], axis=0)\n",
    "    ensemble_preds += weight * fold_preds\n",
    "\n",
    "# 2. Stacking ensemble (meta-learner)\n",
    "print(\"\\nCreating stacking ensemble...\")\n",
    "meta_features = np.zeros((X.shape[0], len(top_models)))\n",
    "meta_test_features = np.zeros((X_test.shape[0], len(top_models)))\n",
    "\n",
    "# Generate meta-features using cross-validation\n",
    "for fold_idx, (tr, val) in enumerate(kf.split(X)):\n",
    "    for model_idx, (name, _) in enumerate(top_models):\n",
    "        X_fold = X_original if name in tree_models else X\n",
    "        X_test_fold = X_test_original if name in tree_models else X_test\n",
    "        \n",
    "        # Train on fold and predict validation\n",
    "        model = trained_models[name][fold_idx]\n",
    "        meta_features[val, model_idx] = model.predict(X_fold.iloc[val])\n",
    "\n",
    "# Generate test meta-features\n",
    "for model_idx, (name, _) in enumerate(top_models):\n",
    "    X_pred = X_test_original if name in tree_models else X_test\n",
    "    meta_test_features[:, model_idx] = np.mean(\n",
    "        [m.predict(X_pred) for m in trained_models[name]], axis=0\n",
    "    )\n",
    "\n",
    "# Train meta-learner (simple Ridge regression)\n",
    "meta_learner = Ridge(alpha=1.0)\n",
    "meta_learner.fit(meta_features, y)\n",
    "stacking_preds = meta_learner.predict(meta_test_features)\n",
    "\n",
    "# Final predictions\n",
    "if tuned_model is not None:\n",
    "    X_final = X_original if best_model_name in tree_models else X_test\n",
    "    tuned_preds = tuned_model.predict(X_final)\n",
    "    final_preds = 0.4 * ensemble_preds + 0.3 * stacking_preds + 0.3 * tuned_preds\n",
    "else:\n",
    "    final_preds = 0.6 * ensemble_preds + 0.4 * stacking_preds\n",
    "\n",
    "# Generate submission\n",
    "submission = pd.DataFrame({\n",
    "    id_col: test[id_col], \n",
    "    target: final_preds\n",
    "})\n",
    "submission.to_csv(\"enhanced_corrucyst_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54418e6",
   "metadata": {},
   "source": [
    "# Approach 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64666f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv(\"MiNDAT.csv\")\n",
    "    test = pd.read_csv(\"MiNDAT_UNK.csv\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def preprocess(train, test, target=\"CORRUCYSTIC_DENSITY\", id_col=\"LOCAL_IDENTIFIER\"):\n",
    "    # drop rows with missing target\n",
    "    train = train.dropna(subset=[target])\n",
    "    \n",
    "    # Remove outliers using IQR method for better model performance\n",
    "    Q1 = train[target].quantile(0.25)\n",
    "    Q3 = train[target].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    train = train[(train[target] >= lower_bound) & (train[target] <= upper_bound)]\n",
    "    \n",
    "    print(f\"After outlier removal: {train.shape[0]} samples\")\n",
    "\n",
    "    features = [c for c in train.columns if c not in [target, id_col]]\n",
    "    X_train, y_train = train[features], train[target]\n",
    "    X_test = test[features]\n",
    "\n",
    "    # Advanced categorical encoding\n",
    "    cat_cols = X_train.select_dtypes(include=\"object\").columns\n",
    "    encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        data = pd.concat([X_train[col], X_test[col]]).fillna(\"missing\")\n",
    "        le.fit(data)\n",
    "        X_train[col] = le.transform(X_train[col].fillna(\"missing\"))\n",
    "        X_test[col] = le.transform(X_test[col].fillna(\"missing\"))\n",
    "        encoders[col] = le\n",
    "\n",
    "    # Handle numerical features\n",
    "    num_cols = X_train.select_dtypes(include=np.number).columns\n",
    "    \n",
    "    # Replace infinite values\n",
    "    for col in num_cols:\n",
    "        X_train[col] = X_train[col].replace([np.inf, -np.inf], np.nan)\n",
    "        X_test[col] = X_test[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Advanced imputation using KNN\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X_train[num_cols] = imputer.fit_transform(X_train[num_cols])\n",
    "    X_test[num_cols] = imputer.transform(X_test[num_cols])\n",
    "    \n",
    "    # Feature engineering - create interaction features\n",
    "    important_features = num_cols[:10]  # Top numerical features\n",
    "    for i, feat1 in enumerate(important_features):\n",
    "        for feat2 in important_features[i+1:i+3]:  # Limit combinations to avoid explosion\n",
    "            # Multiplication\n",
    "            X_train[f'{feat1}_x_{feat2}'] = X_train[feat1] * X_train[feat2]\n",
    "            X_test[f'{feat1}_x_{feat2}'] = X_test[feat1] * X_test[feat2]\n",
    "            \n",
    "            # Ratio (avoid division by zero)\n",
    "            X_train[f'{feat1}_div_{feat2}'] = X_train[feat1] / (X_train[feat2] + 1e-8)\n",
    "            X_test[f'{feat1}_div_{feat2}'] = X_test[feat1] / (X_test[feat2] + 1e-8)\n",
    "    \n",
    "    # Statistical features\n",
    "    for i, feat in enumerate(num_cols[:5]):  # Limit to prevent overfitting\n",
    "        X_train[f'{feat}_squared'] = X_train[feat] ** 2\n",
    "        X_test[f'{feat}_squared'] = X_test[feat] ** 2\n",
    "        \n",
    "        X_train[f'{feat}_sqrt'] = np.sqrt(np.abs(X_train[feat]))\n",
    "        X_test[f'{feat}_sqrt'] = np.sqrt(np.abs(X_test[feat]))\n",
    "    \n",
    "    # Update feature list\n",
    "    features = X_train.columns.tolist()\n",
    "    \n",
    "    # Advanced scaling using RobustScaler (less sensitive to outliers)\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train = pd.DataFrame(X_train_scaled, columns=features)\n",
    "    X_test = pd.DataFrame(X_test_scaled, columns=features)\n",
    "\n",
    "    return X_train, X_test, y_train\n",
    "\n",
    "\n",
    "def train_models(X, y):\n",
    "    models = {\n",
    "        \"RF\": RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1, max_depth=15),\n",
    "        \"ET\": ExtraTreesRegressor(n_estimators=300, random_state=42, n_jobs=-1, max_depth=15),\n",
    "        \"GB\": GradientBoostingRegressor(n_estimators=200, random_state=42, learning_rate=0.05),\n",
    "        \"XGB\": xgb.XGBRegressor(n_estimators=300, random_state=42, learning_rate=0.05, max_depth=6),\n",
    "        \"LGB\": lgb.LGBMRegressor(n_estimators=300, random_state=42, learning_rate=0.05, max_depth=6, verbose=-1),\n",
    "        \"Ridge\": Ridge(alpha=10.0),\n",
    "        \"Lasso\": Lasso(alpha=1.0),\n",
    "        \"ElasticNet\": ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "    }\n",
    "\n",
    "    # Use more robust validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    scores, cv_scores, trained_models = {}, {}, {}\n",
    "    \n",
    "    print(\"Training models with 5-fold cross-validation...\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_rmse_scores = []\n",
    "        fold_models = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            model_copy = type(model)(**model.get_params())\n",
    "            model_copy.fit(X_tr, y_tr)\n",
    "            preds = model_copy.predict(X_val)\n",
    "            rmse = mean_squared_error(y_val, preds, squared=False)\n",
    "            cv_rmse_scores.append(rmse)\n",
    "            fold_models.append(model_copy)\n",
    "        \n",
    "        avg_rmse = np.mean(cv_rmse_scores)\n",
    "        std_rmse = np.std(cv_rmse_scores)\n",
    "        \n",
    "        scores[name] = {\n",
    "            \"CV_RMSE\": avg_rmse,\n",
    "            \"CV_STD\": std_rmse,\n",
    "        }\n",
    "        \n",
    "        cv_scores[name] = cv_rmse_scores\n",
    "        trained_models[name] = fold_models\n",
    "        \n",
    "        print(f\"{name} - CV RMSE: {avg_rmse:.4f} (+/- {std_rmse:.4f})\")\n",
    "\n",
    "    # Find best model\n",
    "    best_model_name = min(scores.keys(), key=lambda x: scores[x][\"CV_RMSE\"])\n",
    "    print(f\"\\nBest model: {best_model_name} with CV RMSE: {scores[best_model_name]['CV_RMSE']:.4f}\")\n",
    "    \n",
    "    return trained_models, scores, best_model_name\n",
    "\n",
    "\n",
    "def tune_model(best_model_name, X, y):\n",
    "    print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n",
    "    \n",
    "    grids = {\n",
    "        \"RF\": {\n",
    "            \"n_estimators\": [200, 300, 400],\n",
    "            \"max_depth\": [10, 15, 20],\n",
    "            \"min_samples_split\": [2, 5],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "        },\n",
    "        \"ET\": {\n",
    "            \"n_estimators\": [200, 300, 400],\n",
    "            \"max_depth\": [10, 15, 20],\n",
    "            \"min_samples_split\": [2, 5],\n",
    "        },\n",
    "        \"GB\": {\n",
    "            \"n_estimators\": [150, 200, 250],\n",
    "            \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "            \"max_depth\": [4, 6, 8],\n",
    "        },\n",
    "        \"XGB\": {\n",
    "            \"n_estimators\": [200, 300, 400],\n",
    "            \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "            \"max_depth\": [4, 6, 8],\n",
    "            \"subsample\": [0.8, 0.9],\n",
    "        },\n",
    "        \"LGB\": {\n",
    "            \"n_estimators\": [200, 300, 400],\n",
    "            \"learning_rate\": [0.03, 0.05, 0.07],\n",
    "            \"max_depth\": [4, 6, 8],\n",
    "            \"subsample\": [0.8, 0.9],\n",
    "        },\n",
    "        \"Ridge\": {\"alpha\": [0.1, 1, 10, 100]},\n",
    "        \"Lasso\": {\"alpha\": [0.01, 0.1, 1, 10]},\n",
    "        \"ElasticNet\": {\"alpha\": [0.1, 1, 10], \"l1_ratio\": [0.1, 0.5, 0.9]},\n",
    "    }\n",
    "\n",
    "    if best_model_name not in grids:\n",
    "        print(\"No hyperparameter tuning defined for this model.\")\n",
    "        return None\n",
    "\n",
    "    # Create base model\n",
    "    base_models = {\n",
    "        \"RF\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        \"ET\": ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "        \"GB\": GradientBoostingRegressor(random_state=42),\n",
    "        \"XGB\": xgb.XGBRegressor(random_state=42),\n",
    "        \"LGB\": lgb.LGBMRegressor(random_state=42, verbose=-1),\n",
    "        \"Ridge\": Ridge(),\n",
    "        \"Lasso\": Lasso(),\n",
    "        \"ElasticNet\": ElasticNet(),\n",
    "    }\n",
    "    \n",
    "    base_model = base_models[best_model_name]\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        base_model, \n",
    "        grids[best_model_name], \n",
    "        cv=3, \n",
    "        scoring=\"neg_mean_squared_error\", \n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    print(f\"Best parameters: {grid.best_params_}\")\n",
    "    print(f\"Best CV RMSE: {np.sqrt(-grid.best_score_):.4f}\")\n",
    "    \n",
    "    return grid.best_estimator_\n",
    "\n",
    "def create_ensemble_predictions(trained_models, scores, X_test):\n",
    "    \"\"\"Create ensemble predictions using weighted average based on CV performance\"\"\"\n",
    "    print(\"\\nCreating ensemble predictions...\")\n",
    "    \n",
    "    # Get top 3 models based on CV RMSE\n",
    "    sorted_models = sorted(scores.items(), key=lambda x: x[1][\"CV_RMSE\"])[:3]\n",
    "    print(f\"Top 3 models for ensemble: {[name for name, _ in sorted_models]}\")\n",
    "    \n",
    "    ensemble_preds = None\n",
    "    total_weight = 0\n",
    "    \n",
    "    for model_name, score_info in sorted_models:\n",
    "        # Weight inversely proportional to RMSE (lower RMSE = higher weight)\n",
    "        weight = 1.0 / score_info[\"CV_RMSE\"]\n",
    "        total_weight += weight\n",
    "        \n",
    "        # Average predictions from all folds\n",
    "        fold_preds = []\n",
    "        for fold_model in trained_models[model_name]:\n",
    "            fold_preds.append(fold_model.predict(X_test))\n",
    "        \n",
    "        model_pred = np.mean(fold_preds, axis=0)\n",
    "        \n",
    "        if ensemble_preds is None:\n",
    "            ensemble_preds = weight * model_pred\n",
    "        else:\n",
    "            ensemble_preds += weight * model_pred\n",
    "        \n",
    "        print(f\"{model_name}: weight = {weight:.4f}\")\n",
    "    \n",
    "    # Normalize weights\n",
    "    ensemble_preds /= total_weight\n",
    "    \n",
    "    return ensemble_preds\n",
    "\n",
    "train, test = load_data()\n",
    "X_train, X_test, y_train = preprocess(train, test)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Train models with cross-validation\n",
    "trained_models, scores, best_model_name = train_models(X_train, y_train)\n",
    "\n",
    "# Tune best individual model\n",
    "tuned_model = tune_model(best_model_name, X_train, y_train)\n",
    "\n",
    "# Create ensemble predictions\n",
    "ensemble_preds = create_ensemble_predictions(trained_models, scores, X_test)\n",
    "\n",
    "# Also get tuned model predictions\n",
    "if tuned_model is not None:\n",
    "    tuned_preds = tuned_model.predict(X_test)\n",
    "    print(f\"\\nTuned {best_model_name} predictions vs Ensemble:\")\n",
    "    print(f\"Tuned model prediction range: {tuned_preds.min():.2f} to {tuned_preds.max():.2f}\")\n",
    "    print(f\"Ensemble prediction range: {ensemble_preds.min():.2f} to {ensemble_preds.max():.2f}\")\n",
    "    \n",
    "    # Use ensemble predictions (typically better)\n",
    "    final_preds = ensemble_preds\n",
    "else:\n",
    "    final_preds = ensemble_preds\n",
    "\n",
    "# Create submission\n",
    "sub = pd.DataFrame({\n",
    "    \"LOCAL_IDENTIFIER\": test[\"LOCAL_IDENTIFIER\"],\n",
    "    \"CORRUCYSTIC_DENSITY\": final_preds\n",
    "})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "for name, score_info in sorted(scores.items(), key=lambda x: x[1][\"CV_RMSE\"]):\n",
    "    print(f\"{name:12} - CV RMSE: {score_info['CV_RMSE']:.4f} (+/- {score_info['CV_STD']:.4f})\")\n",
    "\n",
    "print(f\"\\nSubmission saved: {sub.shape}\")\n",
    "print(f\"Final prediction stats:\")\n",
    "print(f\"  Mean: {final_preds.mean():.2f}\")\n",
    "print(f\"  Std:  {final_preds.std():.2f}\")\n",
    "print(f\"  Min:  {final_preds.min():.2f}\")\n",
    "print(f\"  Max:  {final_preds.max():.2f}\")\n",
    "\n",
    "model, submission = tuned_model, sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d35042",
   "metadata": {},
   "source": [
    "# Best Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# SKlearn Models\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Declaring Parameters to be used multiple times to avoid writing multiple times\n",
    "SEED = 42\n",
    "NFOLDS = 7\n",
    "NREPEATS = 2\n",
    "TARGET = \"CORRUCYSTIC_DENSITY\"\n",
    "ID_COL = \"LOCAL_IDENTIFIER\"\n",
    "AUTO_LOG_TARGET = True\n",
    "SMOOTHING_M = 25.0\n",
    "USE_FEATURE_SELECTION = True\n",
    "SELECT_TOP_K_FEATURES = 100\n",
    "SAVE_FEATURE_IMPORTANCES = True\n",
    "\n",
    "OUTLIER_METHOD = \"isolation_forest\"\n",
    "OUTLIER_CONTAMINATION = 0.05\n",
    "\n",
    "def seed_everything(seed: int = SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_first_existing(paths: List[str]) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(c)).strip(\"_\") for c in df.columns]\n",
    "    df.columns = [re.sub(r\"_+\", \"_\", c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def advanced_downcast(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        col = df[c]\n",
    "        c_min, c_max = col.min(), col.max()\n",
    "        \n",
    "        if pd.api.types.is_integer_dtype(col):\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[c] = col.astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[c] = col.astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[c] = col.astype(np.int32)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[c] = col.astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def detect_outliers_advanced(df: pd.DataFrame, target_col: str, method: str = \"isolation_forest\", contamination: float = 0.05) -> pd.Series:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    \n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    \n",
    "    if method == \"iqr\":\n",
    "        Q1, Q3 = df[target_col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lb, ub = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        mask = (df[target_col] >= lb) & (df[target_col] <= ub)\n",
    "    \n",
    "    elif method == \"zscore\":\n",
    "        z_scores = np.abs(stats.zscore(df[target_col]))\n",
    "        mask = z_scores < 3\n",
    "    \n",
    "    elif method == \"isolation_forest\":\n",
    "        iso_forest = IsolationForest(contamination=contamination, random_state=SEED)\n",
    "        outliers = iso_forest.fit_predict(df[[target_col]])\n",
    "        mask = outliers == 1\n",
    "    \n",
    "    elif method == \"local_outlier\":\n",
    "        lof = LocalOutlierFactor(contamination=contamination)\n",
    "        outliers = lof.fit_predict(df[[target_col]])\n",
    "        mask = outliers == 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def make_target_bins_improved(y: pd.Series, n_bins: int = 10) -> pd.Series:\n",
    "    try:\n",
    "        bins = pd.qcut(y, q=n_bins, labels=False, duplicates=\"drop\")\n",
    "        if bins.nunique() < n_bins // 2:  # If too few unique bins\n",
    "            raise ValueError(\"Too few unique bins\")\n",
    "        return bins.astype(int)\n",
    "    except:\n",
    "        bins = pd.cut(y, bins=n_bins, labels=False, include_lowest=True)\n",
    "        return bins.astype(int)\n",
    "\n",
    "def enhanced_target_encode(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    n_splits: int = NFOLDS,\n",
    "    n_repeats: int = NREPEATS,\n",
    "    smoothing_m: float = SMOOTHING_M,\n",
    "    seed: int = SEED,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    #Enhanced target encoding with repeated CV and noise addition for robustness.\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    global_mean = y.mean()\n",
    "    \n",
    "    # Use repeated K-fold for more stable encoding\n",
    "    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        oof_list = []\n",
    "        test_encoded_list = []\n",
    "        \n",
    "        for tr_idx, val_idx in rkf.split(X):\n",
    "            tr_y = y.iloc[tr_idx]\n",
    "            tr_col = X.iloc[tr_idx][col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            val_col = X.iloc[val_idx][col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            \n",
    "            # Calculate statistics with smoothing\n",
    "            stats_df = tr_y.groupby(tr_col).agg(['sum', 'count', 'mean', 'std']).reset_index()\n",
    "            stats_df.columns = [col, 'sum', 'count', 'mean', 'std']\n",
    "            stats_df['enc'] = (stats_df['sum'] + smoothing_m * global_mean) / (stats_df['count'] + smoothing_m)\n",
    "            \n",
    "            # Add uncertainty-based noise\n",
    "            stats_df['uncertainty'] = 1 / np.sqrt(stats_df['count'] + 1)\n",
    "            noise_scale = stats_df['uncertainty'] * stats_df['std'].fillna(y.std())\n",
    "            stats_df['enc'] += np.random.normal(0, noise_scale * 0.1, len(stats_df))\n",
    "            \n",
    "            # Create mapping dict\n",
    "            encode_map = dict(zip(stats_df[col], stats_df['enc']))\n",
    "            \n",
    "            # Encode validation set\n",
    "            oof_encoded = val_col.map(encode_map).fillna(global_mean)\n",
    "            oof_list.append(pd.Series(oof_encoded.values, index=val_col.index))\n",
    "            \n",
    "            # Encode test set\n",
    "            test_col = X_test[col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            test_encoded = test_col.map(encode_map).fillna(global_mean)\n",
    "            test_encoded_list.append(test_encoded.values)\n",
    "        \n",
    "        # Combine OOF predictions\n",
    "        oof_combined = pd.concat(oof_list).groupby(level=0).mean()\n",
    "        X[col + \"__te\"] = oof_combined.reindex(X.index).astype(np.float32)\n",
    "        \n",
    "        # Average test predictions\n",
    "        X_test[col + \"__te\"] = np.mean(test_encoded_list, axis=0).astype(np.float32)\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "def create_advanced_features(df: pd.DataFrame, num_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Create domain-specific features for the CORRUCYSTIC data.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if len(num_cols) == 0:\n",
    "        return df\n",
    "    \n",
    "    # Ensure we have numeric data\n",
    "    num_data = df[num_cols].select_dtypes(include=[np.number])\n",
    "    if num_data.empty:\n",
    "        return df\n",
    "    \n",
    "    # Basic statistical features\n",
    "    df[\"corr_density_mean\"] = num_data.mean(axis=1)\n",
    "    df[\"corr_density_std\"] = num_data.std(axis=1)\n",
    "    df[\"corr_density_skew\"] = num_data.skew(axis=1)\n",
    "    df[\"corr_density_kurt\"] = num_data.kurtosis(axis=1)\n",
    "    df[\"corr_density_range\"] = num_data.max(axis=1) - num_data.min(axis=1)\n",
    "    \n",
    "    # Percentile features\n",
    "    df[\"corr_density_q25\"] = num_data.quantile(0.25, axis=1)\n",
    "    df[\"corr_density_q75\"] = num_data.quantile(0.75, axis=1)\n",
    "    df[\"corr_density_iqr\"] = df[\"corr_density_q75\"] - df[\"corr_density_q25\"]\n",
    "    \n",
    "    # Count-based features\n",
    "    df[\"corr_positive_count\"] = (num_data > 0).sum(axis=1)\n",
    "    df[\"corr_negative_count\"] = (num_data < 0).sum(axis=1)\n",
    "    df[\"corr_zero_count\"] = (num_data == 0).sum(axis=1)\n",
    "    df[\"corr_missing_count\"] = num_data.isnull().sum(axis=1)\n",
    "    \n",
    "    # Advanced statistical measures\n",
    "    df[\"corr_cv\"] = df[\"corr_density_std\"] / (np.abs(df[\"corr_density_mean\"]) + 1e-8)\n",
    "    df[\"corr_mad\"] = num_data.apply(manual_mad, axis=1)  # Using manual MAD implementation\n",
    "    \n",
    "    # Energy and signal processing inspired features (domain-specific)\n",
    "    df[\"corr_energy\"] = (num_data ** 2).sum(axis=1)\n",
    "    df[\"corr_rms\"] = np.sqrt(df[\"corr_energy\"] / len(num_cols))\n",
    "    \n",
    "    # Handle infinite and NaN values\n",
    "    inf_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[inf_cols] = df[inf_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def manual_mad(series):\n",
    "    median = series.median()\n",
    "    return np.median(np.abs(series - median))\n",
    "\n",
    "def advanced_feature_selection(X: pd.DataFrame, y: pd.Series, k: int = 100) -> List[str]:\n",
    "    if len(X.columns) <= k:\n",
    "        return X.columns.tolist()\n",
    "    \n",
    "    valid_features = []\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() / len(X) < 0.8 and X[col].nunique() > 1:\n",
    "            valid_features.append(col)\n",
    "    \n",
    "    X_valid = X[valid_features].fillna(X[valid_features].median())\n",
    "    \n",
    "    # Combine multiple selection methods\n",
    "    scores = {}\n",
    "    \n",
    "    # F-regression scores\n",
    "    try:\n",
    "        f_selector = SelectKBest(score_func=f_regression, k='all')\n",
    "        f_selector.fit(X_valid, y)\n",
    "        f_scores = dict(zip(valid_features, f_selector.scores_))\n",
    "        scores['f_regression'] = f_scores\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Mutual information scores\n",
    "    try:\n",
    "        mi_scores = mutual_info_regression(X_valid, y, random_state=SEED)\n",
    "        mi_scores_dict = dict(zip(valid_features, mi_scores))\n",
    "        scores['mutual_info'] = mi_scores_dict\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Combine scores using rank aggregation\n",
    "    if scores:\n",
    "        feature_ranks = {}\n",
    "        for feature in valid_features:\n",
    "            ranks = []\n",
    "            for method, score_dict in scores.items():\n",
    "                if feature in score_dict:\n",
    "                    # Convert to rank (lower rank = better)\n",
    "                    sorted_features = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "                    rank = next(i for i, (f, _) in enumerate(sorted_features) if f == feature)\n",
    "                    ranks.append(rank)\n",
    "            feature_ranks[feature] = np.mean(ranks) if ranks else len(valid_features)\n",
    "        \n",
    "        # Select top k features\n",
    "        selected = sorted(feature_ranks.items(), key=lambda x: x[1])[:k]\n",
    "        return [f[0] for f in selected]\n",
    "    \n",
    "    return valid_features[:k]\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# -------------------- Load -------------------- #\n",
    "seed_everything(SEED)\n",
    "\n",
    "train_path = \"./MiNDAT.csv\"\n",
    "test_path = \"./MiNDAT_UNK.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train = clean_columns(train)\n",
    "\n",
    "test = pd.read_csv(test_path)\n",
    "test = clean_columns(test)\n",
    "\n",
    "# Basic hygiene\n",
    "if TARGET not in train.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET}' not in training data.\")\n",
    "\n",
    "# Drop rows with missing target\n",
    "train = train.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "# Remove outliers via IQR (robust)\n",
    "Q1, Q3 = train[TARGET].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lb, ub = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "train = train[(train[TARGET] >= lb) & (train[TARGET] <= ub)].reset_index(drop=True)\n",
    "\n",
    "# Identify columns (keep features simple: no extra row stats or poly features)\n",
    "features = [c for c in train.columns if c not in [TARGET, ID_COL]]\n",
    "X_full = train[features].copy()\n",
    "y_full = train[TARGET].copy()\n",
    "\n",
    "if test is not None:\n",
    "    X_test_full = test[features].copy()\n",
    "else:\n",
    "    X_test_full = None\n",
    "\n",
    "# Replace infs\n",
    "for df in [X_full] + ([X_test_full] if X_test_full is not None else []):\n",
    "    if df is None:\n",
    "        continue\n",
    "    numc = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numc] = df[numc].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Advanced memory optimization\n",
    "X_full = advanced_downcast(X_full)\n",
    "if X_test_full is not None:\n",
    "    X_test_full = advanced_downcast(X_test_full)\n",
    "\n",
    "# Split types\n",
    "cat_cols = X_full.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Simple imputers (per column)\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_full[num_cols] = num_imputer.fit_transform(X_full[num_cols])\n",
    "if X_test_full is not None:\n",
    "    X_test_full[num_cols] = num_imputer.transform(X_test_full[num_cols])\n",
    "\n",
    "X_full[cat_cols] = cat_imputer.fit_transform(X_full[cat_cols])\n",
    "if X_test_full is not None:\n",
    "    X_test_full[cat_cols] = cat_imputer.transform(X_test_full[cat_cols])\n",
    "\n",
    "# Cast categoricals to 'category' dtype (memory);\n",
    "for c in cat_cols:\n",
    "    X_full[c] = X_full[c].astype(\"category\")\n",
    "    if X_test_full is not None:\n",
    "        X_test_full[c] = X_test_full[c].astype(\"category\")\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    print(\"Performing enhanced target encoding...\")\n",
    "    X_te, X_test_te = enhanced_target_encode(\n",
    "        X_full,\n",
    "        y_full,\n",
    "        X_test_full if X_test_full is not None else X_full.iloc[:0],\n",
    "        cat_cols,\n",
    "    )\n",
    "else:\n",
    "    X_te, X_test_te = X_full.copy(), (X_test_full.copy() if X_test_full is not None else None)\n",
    "\n",
    "X_te = X_te.drop(columns=cat_cols, errors=\"ignore\")\n",
    "if X_test_te is not None:\n",
    "    X_test_te = X_test_te.drop(columns=cat_cols, errors=\"ignore\")\n",
    "\n",
    "# Feature selection\n",
    "if USE_FEATURE_SELECTION and len(X_te.columns) > SELECT_TOP_K_FEATURES:\n",
    "    print(f\"Selecting top {SELECT_TOP_K_FEATURES} features...\")\n",
    "    selected_features = advanced_feature_selection(X_te, y_full, SELECT_TOP_K_FEATURES)\n",
    "    X_te = X_te[selected_features]\n",
    "    if X_test_te is not None:\n",
    "        X_test_te = X_test_te[selected_features]\n",
    "    print(f\"Selected {len(selected_features)} features\")\n",
    "\n",
    "# Enhanced scaling for linear models\n",
    "scaler = QuantileTransformer(output_distribution='normal', random_state=SEED)\n",
    "X_lin = X_te.copy()\n",
    "lin_num_cols = X_lin.select_dtypes(include=[np.number]).columns\n",
    "X_lin[lin_num_cols] = scaler.fit_transform(X_lin[lin_num_cols])\n",
    "\n",
    "if X_test_te is not None:\n",
    "    X_test_lin = X_test_te.copy()\n",
    "    lin_test_num = X_test_lin.select_dtypes(include=[np.number]).columns\n",
    "    X_test_lin[lin_test_num] = scaler.transform(X_test_lin[lin_test_num])\n",
    "else:\n",
    "    X_test_lin = None\n",
    "\n",
    "use_log = False\n",
    "if AUTO_LOG_TARGET:\n",
    "    skew_val = pd.Series(y_full).skew()\n",
    "    if skew_val > 1.0:\n",
    "        use_log = True\n",
    "        y_work = np.log1p(y_full)\n",
    "    else:\n",
    "        y_work = y_full.copy()\n",
    "else:\n",
    "    y_work = y_full.copy()\n",
    "\n",
    "# Stratified folds by target bins (for stability)\n",
    "strat_bins = make_target_bins_improved(y_full, n_bins=8)\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "base_models: Dict[str, Tuple[object, str]] = {}\n",
    "\n",
    "base_models[\"LGBM\"] = (\n",
    "    LGBMRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        min_child_samples=20,\n",
    "        random_state=SEED,\n",
    "        n_jobs=1,\n",
    "        verbose=-1,\n",
    "        objective='regression',  \n",
    "        boosting_type='gbdt',  \n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"XGB\"] = (\n",
    "    XGBRegressor(\n",
    "        n_estimators=1200,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"CAT\"] = (\n",
    "    CatBoostRegressor(\n",
    "        depth=8,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=1500,\n",
    "        subsample=0.8,\n",
    "        random_state=SEED,\n",
    "        loss_function=\"RMSE\",\n",
    "        verbose=False,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "# Strong classical ensembles\n",
    "base_models[\"ET\"] = (\n",
    "    ExtraTreesRegressor(\n",
    "        n_estimators=1000, \n",
    "        max_depth=None,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"RF\"] = (\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=1000, \n",
    "        max_depth=None,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"GBDT\"] = (\n",
    "    GradientBoostingRegressor(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=3,\n",
    "        random_state=SEED,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"Ridge\"] = (\n",
    "    Ridge(alpha=5.0, random_state=SEED),\n",
    "    \"linear\",\n",
    ")\n",
    "base_models[\"Lasso\"] = (\n",
    "    Lasso(alpha=0.0005, random_state=SEED, max_iter=20000),\n",
    "    \"linear\",\n",
    ")\n",
    "\n",
    "# -------------- OOF Training / Prediction -------------- #\n",
    "models_oof = {}\n",
    "models_test_pred = {}\n",
    "models_fold_objs = {}\n",
    "fold_scores = []\n",
    "\n",
    "# Feature matrices: tree models use TE (numeric-only); linear models use scaled copy\n",
    "X_tree = X_te\n",
    "X_tree_test = X_test_te\n",
    "\n",
    "X_linear = X_lin\n",
    "X_linear_test = X_test_lin\n",
    "\n",
    "for name, (model, mtype) in base_models.items():\n",
    "    print(f\"\\n[Model={name}] training with {NFOLDS}-fold CV ...\")\n",
    "    oof = np.zeros(len(X_tree))\n",
    "    test_preds_folds = []\n",
    "    fold_importances = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_tree, strat_bins)):\n",
    "        if mtype == \"tree\":\n",
    "            Xtr, Xva = X_tree.iloc[tr_idx], X_tree.iloc[va_idx]\n",
    "        else:\n",
    "            Xtr, Xva = X_linear.iloc[tr_idx], X_linear.iloc[va_idx]\n",
    "\n",
    "        ytr, yva = y_work.iloc[tr_idx], y_work.iloc[va_idx]\n",
    "\n",
    "        # Fresh instance per fold\n",
    "        m = type(model)(**model.get_params())\n",
    "\n",
    "        # Special handling for different model types\n",
    "        if name == \"LGBM\":\n",
    "            # LightGBM specific handling with clean feature names and reset index\n",
    "            Xtr_clean = Xtr.copy().reset_index(drop=True)\n",
    "            Xva_clean = Xva.copy().reset_index(drop=True)\n",
    "            ytr_clean = ytr.reset_index(drop=True)\n",
    "            yva_clean = yva.reset_index(drop=True)\n",
    "            \n",
    "            # Ensure all data is numeric and clean feature names\n",
    "            Xtr_clean = Xtr_clean.select_dtypes(include=[np.number])\n",
    "            Xva_clean = Xva_clean.select_dtypes(include=[np.number])\n",
    "            \n",
    "            # Create simple sequential feature names to avoid any naming conflicts\n",
    "            n_features = len(Xtr_clean.columns)\n",
    "            simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "            \n",
    "            Xtr_clean.columns = simple_names\n",
    "            Xva_clean.columns = simple_names\n",
    "            \n",
    "            try:\n",
    "                # Use minimal parameters to avoid conflicts\n",
    "                lgb_params = {\n",
    "                    'n_estimators': 500,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'max_depth': 6,\n",
    "                    'num_leaves': 31,\n",
    "                    'subsample': 0.8,\n",
    "                    'colsample_bytree': 0.8,\n",
    "                    'random_state': SEED,\n",
    "                    'verbose': -1,\n",
    "                    'n_jobs': 1\n",
    "                }\n",
    "                m = LGBMRegressor(**lgb_params)\n",
    "                \n",
    "                # Fit without eval_set to avoid feature name conflicts\n",
    "                m.fit(Xtr_clean, ytr_clean)\n",
    "                pred_va = m.predict(Xva_clean)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: LGBM fold {fold} failed: {e}\")\n",
    "                pred_va = np.full(len(yva_clean), ytr_clean.mean())  # Fallback prediction\n",
    "                \n",
    "        elif name == \"CAT\":\n",
    "            m.fit(Xtr.astype(np.float32), ytr, eval_set=(Xva.astype(np.float32), yva), verbose=False)\n",
    "            pred_va = m.predict(Xva.astype(np.float32))\n",
    "        else:\n",
    "            m.fit(Xtr, ytr)\n",
    "            pred_va = m.predict(Xva)\n",
    "\n",
    "        oof[va_idx] = pred_va\n",
    "\n",
    "        if X_tree_test is not None:\n",
    "            Xte = X_tree_test if mtype == \"tree\" else X_linear_test\n",
    "            if name == \"LGBM\":\n",
    "                Xte_clean = Xte.copy().reset_index(drop=True)\n",
    "                Xte_clean = Xte_clean.select_dtypes(include=[np.number])\n",
    "                \n",
    "                n_features = len(Xte_clean.columns)\n",
    "                simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "                Xte_clean.columns = simple_names\n",
    "                \n",
    "                try:\n",
    "                    test_preds_folds.append(m.predict(Xte_clean))\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: LGBM test prediction failed: {e}\")\n",
    "                    test_preds_folds.append(np.full(len(Xte_clean), oof[oof != 0].mean() if len(oof[oof != 0]) > 0 else 0))\n",
    "            elif name == \"CAT\":\n",
    "                test_preds_folds.append(m.predict(Xte.astype(np.float32)))\n",
    "            else:\n",
    "                test_preds_folds.append(m.predict(Xte))\n",
    "\n",
    "        # Save feature importances when available\n",
    "        if SAVE_FEATURE_IMPORTANCES and hasattr(m, \"feature_importances_\"):\n",
    "            fi = pd.DataFrame({\n",
    "                \"feature\": Xtr.columns,\n",
    "                \"importance\": m.feature_importances_,\n",
    "                \"fold\": fold,\n",
    "                \"model\": name,\n",
    "            })\n",
    "            fold_importances.append(fi)\n",
    "\n",
    "        models_fold_objs.setdefault(name, []).append(m)\n",
    "\n",
    "    # Back-transform if log was used\n",
    "    oof_final = np.expm1(oof) if use_log else oof\n",
    "    rmse_score = rmse(y_full, oof_final)\n",
    "\n",
    "    models_oof[name] = oof_final\n",
    "    if test_preds_folds:\n",
    "        pred_test_mean = np.mean(test_preds_folds, axis=0)\n",
    "        pred_test_final = np.expm1(pred_test_mean) if use_log else pred_test_mean\n",
    "        models_test_pred[name] = pred_test_final\n",
    "\n",
    "    fold_scores.append({\"model\": name, \"cv_rmse\": rmse_score})\n",
    "    print(f\"[Model={name}] CV RMSE: {rmse_score:.5f}\")\n",
    "\n",
    "    if SAVE_FEATURE_IMPORTANCES and len(fold_importances) > 0:\n",
    "        imp_df = pd.concat(fold_importances, ignore_index=True)\n",
    "        imp_df.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).head(50).to_csv(\n",
    "            f\"feature_importances_{name}.csv\", index=True\n",
    "        )\n",
    "\n",
    "# -------------------- Stacking (meta-model) -------------------- #\n",
    "print(\"\\n[Stacking] Training meta-model (GradientBoostingRegressor) on OOF predictions ...\")\n",
    "oof_df = pd.DataFrame(models_oof)\n",
    "\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=SEED,\n",
    ")\n",
    "meta_model.fit(oof_df, y_full)\n",
    "meta_oof = meta_model.predict(oof_df)\n",
    "stack_rmse = rmse(y_full, meta_oof)\n",
    "print(f\"[Stacking] Meta CV RMSE: {stack_rmse:.5f}\")\n",
    "\n",
    "# Weighted Blend of the best individual models (by CV)\n",
    "sorted_models = sorted(fold_scores, key=lambda d: d[\"cv_rmse\"])[: min(3, len(fold_scores))]\n",
    "weights = []\n",
    "for d in sorted_models:\n",
    "    w = 1.0 / max(d[\"cv_rmse\"], 1e-6)\n",
    "    weights.append(w)\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "print(\"\\n[Blending] Top models & weights:\")\n",
    "for (d, w) in zip(sorted_models, weights):\n",
    "    print(f\"  - {d['model']}: weight={w:.3f}, cv_rmse={d['cv_rmse']:.5f}\")\n",
    "\n",
    "pd.DataFrame({\"id\": train[ID_COL], \"y\": y_full, **models_oof}).to_csv(\"oof_predictions.csv\", index=False)\n",
    "pd.DataFrame(fold_scores).to_csv(\"fold_scores.csv\", index=False)\n",
    "\n",
    "if X_test_full is not None and len(models_test_pred) > 0:\n",
    "    test_stack_mat = pd.DataFrame(models_test_pred)\n",
    "    stack_pred = meta_model.predict(test_stack_mat)\n",
    "\n",
    "    blend_pred = np.zeros(len(test_stack_mat))\n",
    "    for (d, w) in zip(sorted_models, weights):\n",
    "        blend_pred += w * test_stack_mat[d[\"model\"]].values\n",
    "\n",
    "    final_pred = 0.5 * stack_pred + 0.5 * blend_pred\n",
    "\n",
    "    sub = pd.DataFrame({ID_COL: test[ID_COL], TARGET: final_pred})\n",
    "    sub.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"\\nSaved submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22232a",
   "metadata": {},
   "source": [
    "# Approach 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea77459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    HAS_LGB = True\n",
    "except Exception:\n",
    "    HAS_LGB = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CAT = True\n",
    "except Exception:\n",
    "    HAS_CAT = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "NFOLDS = 7\n",
    "NREPEATS = 2\n",
    "TARGET = \"CORRUCYSTIC_DENSITY\"\n",
    "ID_COL = \"LOCAL_IDENTIFIER\"\n",
    "AUTO_LOG_TARGET = True\n",
    "SMOOTHING_M = 25.0\n",
    "USE_FEATURE_SELECTION = True\n",
    "SELECT_TOP_K_FEATURES = 120\n",
    "SAVE_FEATURE_IMPORTANCES = True\n",
    "\n",
    "# Enhanced outlier detection\n",
    "OUTLIER_METHOD = \"iqr\"  \n",
    "OUTLIER_CONTAMINATION = 0.05  \n",
    "\n",
    "# Feature engineering toggles\n",
    "USE_ADVANCED_FEATURES = True\n",
    "USE_INTERACTIONS = True\n",
    "USE_POLYNOMIAL = True\n",
    "USE_CLUSTERING = True\n",
    "\n",
    "def seed_everything(seed: int = SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_first_existing(paths: List[str]) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(c)).strip(\"_\") for c in df.columns]\n",
    "    df.columns = [re.sub(r\"_+\", \"_\", c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def advanced_downcast(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        col = df[c]\n",
    "        c_min, c_max = col.min(), col.max()\n",
    "        \n",
    "        if pd.api.types.is_integer_dtype(col):\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[c] = col.astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[c] = col.astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[c] = col.astype(np.int32)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[c] = col.astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def detect_outliers_advanced(df: pd.DataFrame, target_col: str, method: str = \"isolation_forest\", contamination: float = 0.05) -> pd.Series:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    \n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    \n",
    "    if method == \"iqr\":\n",
    "        Q1, Q3 = df[target_col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lb, ub = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        mask = (df[target_col] >= lb) & (df[target_col] <= ub)\n",
    "    \n",
    "    elif method == \"zscore\":\n",
    "        z_scores = np.abs(stats.zscore(df[target_col]))\n",
    "        mask = z_scores < 3\n",
    "    \n",
    "    elif method == \"isolation_forest\":\n",
    "        iso_forest = IsolationForest(contamination=contamination, random_state=SEED)\n",
    "        outliers = iso_forest.fit_predict(df[[target_col]])\n",
    "        mask = outliers == 1\n",
    "    \n",
    "    elif method == \"local_outlier\":\n",
    "        lof = LocalOutlierFactor(contamination=contamination)\n",
    "        outliers = lof.fit_predict(df[[target_col]])\n",
    "        mask = outliers == 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def make_target_bins_improved(y: pd.Series, n_bins: int = 10) -> pd.Series:\n",
    "    # Try quantile-based binning first\n",
    "    try:\n",
    "        bins = pd.qcut(y, q=n_bins, labels=False, duplicates=\"drop\")\n",
    "        if bins.nunique() < n_bins // 2:\n",
    "            raise ValueError(\"Too few unique bins\")\n",
    "        return bins.astype(int)\n",
    "    except:\n",
    "        bins = pd.cut(y, bins=n_bins, labels=False, include_lowest=True)\n",
    "        return bins.astype(int)\n",
    "\n",
    "def enhanced_target_encode(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    n_splits: int = NFOLDS,\n",
    "    n_repeats: int = NREPEATS,\n",
    "    smoothing_m: float = SMOOTHING_M,\n",
    "    seed: int = SEED,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    global_mean = y.mean()\n",
    "    \n",
    "    # Use repeated K-fold for more stable encoding\n",
    "    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        oof_list = []\n",
    "        test_encoded_list = []\n",
    "        \n",
    "        for tr_idx, val_idx in rkf.split(X):\n",
    "            tr_y = y.iloc[tr_idx]\n",
    "            tr_col = X.iloc[tr_idx][col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            val_col = X.iloc[val_idx][col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            \n",
    "            # Calculate statistics with smoothing\n",
    "            stats_df = tr_y.groupby(tr_col).agg(['sum', 'count', 'mean', 'std']).reset_index()\n",
    "            stats_df.columns = [col, 'sum', 'count', 'mean', 'std']\n",
    "            stats_df['enc'] = (stats_df['sum'] + smoothing_m * global_mean) / (stats_df['count'] + smoothing_m)\n",
    "            \n",
    "            # Add uncertainty-based noise\n",
    "            stats_df['uncertainty'] = 1 / np.sqrt(stats_df['count'] + 1)\n",
    "            noise_scale = stats_df['uncertainty'] * stats_df['std'].fillna(y.std())\n",
    "            stats_df['enc'] += np.random.normal(0, noise_scale * 0.1, len(stats_df))\n",
    "            \n",
    "            # Create mapping dict\n",
    "            encode_map = dict(zip(stats_df[col], stats_df['enc']))\n",
    "            \n",
    "            # Encode validation set\n",
    "            oof_encoded = val_col.map(encode_map).fillna(global_mean)\n",
    "            oof_list.append(pd.Series(oof_encoded.values, index=val_col.index))\n",
    "            \n",
    "            # Encode test set\n",
    "            test_col = X_test[col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            test_encoded = test_col.map(encode_map).fillna(global_mean)\n",
    "            test_encoded_list.append(test_encoded.values)\n",
    "        \n",
    "        # Combine OOF predictions\n",
    "        oof_combined = pd.concat(oof_list).groupby(level=0).mean()\n",
    "        X[col + \"__te\"] = oof_combined.reindex(X.index).astype(np.float32)\n",
    "        \n",
    "        # Average test predictions\n",
    "        X_test[col + \"__te\"] = np.mean(test_encoded_list, axis=0).astype(np.float32)\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "def create_advanced_features(df: pd.DataFrame, num_cols: List[str]) -> pd.DataFrame:\n",
    "    \n",
    "    if len(num_cols) == 0:\n",
    "        return df\n",
    "    \n",
    "    # Ensure we have numeric data\n",
    "    num_data = df[num_cols].select_dtypes(include=[np.number])\n",
    "    if num_data.empty:\n",
    "        return df\n",
    "    \n",
    "    # Basic statistical features\n",
    "    df[\"corr_density_mean\"] = num_data.mean(axis=1)\n",
    "    df[\"corr_density_std\"] = num_data.std(axis=1)\n",
    "    df[\"corr_density_skew\"] = num_data.skew(axis=1)\n",
    "    df[\"corr_density_kurt\"] = num_data.kurtosis(axis=1)\n",
    "    df[\"corr_density_range\"] = num_data.max(axis=1) - num_data.min(axis=1)\n",
    "    \n",
    "    # Percentile features\n",
    "    df[\"corr_density_q25\"] = num_data.quantile(0.25, axis=1)\n",
    "    df[\"corr_density_q75\"] = num_data.quantile(0.75, axis=1)\n",
    "    df[\"corr_density_iqr\"] = df[\"corr_density_q75\"] - df[\"corr_density_q25\"]\n",
    "    \n",
    "    # Count-based features\n",
    "    df[\"corr_positive_count\"] = (num_data > 0).sum(axis=1)\n",
    "    df[\"corr_negative_count\"] = (num_data < 0).sum(axis=1)\n",
    "    df[\"corr_zero_count\"] = (num_data == 0).sum(axis=1)\n",
    "    df[\"corr_missing_count\"] = num_data.isnull().sum(axis=1)\n",
    "    \n",
    "    # Advanced statistical measures\n",
    "    df[\"corr_cv\"] = df[\"corr_density_std\"] / (np.abs(df[\"corr_density_mean\"]) + 1e-8)\n",
    "    df[\"corr_mad\"] = num_data.apply(manual_mad, axis=1)  # Using manual MAD implementation\n",
    "    \n",
    "    # Energy and signal processing inspired features (domain-specific)\n",
    "    df[\"corr_energy\"] = (num_data ** 2).sum(axis=1)\n",
    "    df[\"corr_rms\"] = np.sqrt(df[\"corr_energy\"] / len(num_cols))\n",
    "    \n",
    "    # Data integrity and stability measures (alien biocomputer context)\n",
    "    df[\"corr_stability\"] = 1 / (1 + df[\"corr_cv\"])  # Inverse CV as stability measure\n",
    "    df[\"corr_coherence\"] = df[\"corr_positive_count\"] / (df[\"corr_positive_count\"] + df[\"corr_negative_count\"] + 1e-8)\n",
    "    df[\"corr_completeness\"] = 1 - (df[\"corr_missing_count\"] / len(num_cols))\n",
    "    df[\"corr_balance\"] = 1 - np.abs(df[\"corr_positive_count\"] - df[\"corr_negative_count\"]) / len(num_cols)\n",
    "    \n",
    "    # Memory/Neural network inspired features\n",
    "    df[\"corr_activation\"] = num_data.apply(lambda x: (x > x.mean()).sum() / len(x), axis=1)  # Fraction above mean\n",
    "    df[\"corr_inhibition\"] = num_data.apply(lambda x: (x < x.mean()).sum() / len(x), axis=1)  # Fraction below mean\n",
    "    df[\"corr_signal_noise\"] = np.abs(df[\"corr_density_mean\"]) / (df[\"corr_density_std\"] + 1e-8)\n",
    "    \n",
    "    # Frequency domain features (alien signal analysis)\n",
    "    df[\"corr_peak_to_avg\"] = num_data.max(axis=1) / (np.abs(df[\"corr_density_mean\"]) + 1e-8)\n",
    "    df[\"corr_trough_to_avg\"] = num_data.min(axis=1) / (np.abs(df[\"corr_density_mean\"]) + 1e-8)\n",
    "    \n",
    "    # Handle infinite and NaN values\n",
    "    inf_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[inf_cols] = df[inf_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_biocomputer_interactions(df: pd.DataFrame, df_test: Optional[pd.DataFrame], important_cols: List[str]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy() if df_test is not None else None\n",
    "    \n",
    "    # Limit to available columns\n",
    "    available_cols = [col for col in important_cols if col in df.columns][:5]  # Top 5 to avoid explosion\n",
    "    \n",
    "    if len(available_cols) < 2:\n",
    "        return df, df_test\n",
    "    \n",
    "    # Create pairwise interactions\n",
    "    for i, col1 in enumerate(available_cols):\n",
    "        for j, col2 in enumerate(available_cols[i+1:], i+1):\n",
    "            # Multiplicative interactions (signal amplification)\n",
    "            interaction_name = f\"interaction_{col1}_x_{col2}\"\n",
    "            df[interaction_name] = df[col1] * df[col2]\n",
    "            if df_test is not None:\n",
    "                df_test[interaction_name] = df_test[col1] * df_test[col2]\n",
    "            \n",
    "            # Ratio interactions (relative signal strength)\n",
    "            ratio_name = f\"ratio_{col1}_div_{col2}\"\n",
    "            df[ratio_name] = df[col1] / (np.abs(df[col2]) + 1e-8)\n",
    "            if df_test is not None:\n",
    "                df_test[ratio_name] = df_test[col1] / (np.abs(df_test[col2]) + 1e-8)\n",
    "    \n",
    "    # Clean up infinite values\n",
    "    for dataset in [df] + ([df_test] if df_test is not None else []):\n",
    "        if dataset is None:\n",
    "            continue\n",
    "        numeric_cols = dataset.select_dtypes(include=[np.number]).columns\n",
    "        dataset[numeric_cols] = dataset[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "def create_memory_sequence_features(df: pd.DataFrame, df_test: Optional[pd.DataFrame], num_cols: List[str]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy() if df_test is not None else None\n",
    "    \n",
    "    if len(num_cols) == 0:\n",
    "        return df, df_test\n",
    "    \n",
    "    # Get numeric data\n",
    "    for dataset, name in [(df, \"train\"), (df_test, \"test\")]:\n",
    "        if dataset is None:\n",
    "            continue\n",
    "            \n",
    "        num_data = dataset[num_cols].select_dtypes(include=[np.number])\n",
    "        if num_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Memory sequence patterns (treating columns as temporal sequence)\n",
    "        dataset[\"memory_trend\"] = num_data.apply(lambda x: np.corrcoef(x.values, np.arange(len(x)))[0,1] if len(x) > 1 else 0, axis=1)\n",
    "        dataset[\"memory_volatility\"] = num_data.rolling(window=3, axis=1).std().mean(axis=1)\n",
    "        dataset[\"memory_persistence\"] = num_data.apply(lambda x: np.sum(np.diff(x) > 0) / (len(x) - 1) if len(x) > 1 else 0, axis=1)\n",
    "        \n",
    "        # Alien signal patterns\n",
    "        dataset[\"signal_periodicity\"] = num_data.apply(\n",
    "            lambda x: np.abs(np.fft.fft(x.fillna(0).values)).mean() if len(x) > 2 else 0, axis=1\n",
    "        )\n",
    "        dataset[\"signal_complexity\"] = num_data.apply(\n",
    "            lambda x: -np.sum(x / x.sum() * np.log(x / x.sum() + 1e-8)) if x.sum() != 0 else 0, axis=1\n",
    "        )\n",
    "        \n",
    "        # Memory fragmentation patterns\n",
    "        dataset[\"memory_fragmentation\"] = (num_data.diff(axis=1) != 0).sum(axis=1) / len(num_cols)\n",
    "        dataset[\"memory_coherence_seq\"] = num_data.apply(\n",
    "            lambda x: np.corrcoef(x[:-1].values, x[1:].values)[0,1] if len(x) > 2 else 0, axis=1\n",
    "        )\n",
    "    \n",
    "    # Clean up infinite and NaN values\n",
    "    for dataset in [df] + ([df_test] if df_test is not None else []):\n",
    "        if dataset is None:\n",
    "            continue\n",
    "        numeric_cols = dataset.select_dtypes(include=[np.number]).columns\n",
    "        dataset[numeric_cols] = dataset[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        dataset[numeric_cols] = dataset[numeric_cols].fillna(0)\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "def create_network_topology_features(df: pd.DataFrame, df_test: Optional[pd.DataFrame], num_cols: List[str]) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy() if df_test is not None else None\n",
    "    \n",
    "    if len(num_cols) == 0:\n",
    "        return df, df_test\n",
    "    \n",
    "    # Create network-inspired features\n",
    "    for dataset, name in [(df, \"train\"), (df_test, \"test\")]:\n",
    "        if dataset is None:\n",
    "            continue\n",
    "            \n",
    "        num_data = dataset[num_cols].select_dtypes(include=[np.number])\n",
    "        if num_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Network connectivity patterns\n",
    "        dataset[\"network_density\"] = (num_data != 0).sum(axis=1) / len(num_cols)\n",
    "        # Simplified clustering coefficient using autocorrelation\n",
    "        dataset[\"network_clustering\"] = num_data.apply(\n",
    "            lambda x: np.corrcoef(x.values[:-1], x.values[1:])[0,1] if len(x) > 2 and x.std() > 0 else 0, axis=1\n",
    "        )\n",
    "        \n",
    "        # Information flow patterns\n",
    "        dataset[\"info_entropy\"] = num_data.apply(\n",
    "            lambda x: -np.sum((x / (x.sum() + 1e-8)) * np.log(x / (x.sum() + 1e-8) + 1e-8)), axis=1\n",
    "        )\n",
    "        dataset[\"info_concentration\"] = num_data.apply(\n",
    "            lambda x: np.max(np.abs(x)) / (np.sum(np.abs(x)) + 1e-8), axis=1\n",
    "        )\n",
    "        \n",
    "        # Distributed processing patterns\n",
    "        dataset[\"processing_load\"] = np.sqrt((num_data ** 2).sum(axis=1))\n",
    "        dataset[\"processing_efficiency\"] = num_data.apply(\n",
    "            lambda x: np.sum(x > x.mean()) / len(x), axis=1\n",
    "        )\n",
    "        \n",
    "        # Network resilience features\n",
    "        dataset[\"network_robustness\"] = 1 - (num_data == 0).sum(axis=1) / len(num_cols)\n",
    "        dataset[\"network_redundancy\"] = num_data.apply(\n",
    "            lambda x: len(x[x > x.quantile(0.5)]) / len(x), axis=1\n",
    "        )\n",
    "    \n",
    "    # Clean up infinite and NaN values\n",
    "    for dataset in [df] + ([df_test] if df_test is not None else []):\n",
    "        if dataset is None:\n",
    "            continue\n",
    "        numeric_cols = dataset.select_dtypes(include=[np.number]).columns\n",
    "        dataset[numeric_cols] = dataset[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        dataset[numeric_cols] = dataset[numeric_cols].fillna(0)\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "def manual_mad(series):\n",
    "    median = series.median()\n",
    "    return np.median(np.abs(series - median))\n",
    "\n",
    "def advanced_feature_selection(X: pd.DataFrame, y: pd.Series, k: int = 100) -> List[str]:\n",
    "    if len(X.columns) <= k:\n",
    "        return X.columns.tolist()\n",
    "    \n",
    "    # Remove features with too many missing values or zero variance\n",
    "    valid_features = []\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() / len(X) < 0.8 and X[col].nunique() > 1:\n",
    "            valid_features.append(col)\n",
    "    \n",
    "    X_valid = X[valid_features].fillna(X[valid_features].median())\n",
    "    \n",
    "    # Combine multiple selection methods\n",
    "    scores = {}\n",
    "    \n",
    "    # F-regression scores\n",
    "    try:\n",
    "        f_selector = SelectKBest(score_func=f_regression, k='all')\n",
    "        f_selector.fit(X_valid, y)\n",
    "        f_scores = dict(zip(valid_features, f_selector.scores_))\n",
    "        scores['f_regression'] = f_scores\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Mutual information scores\n",
    "    try:\n",
    "        mi_scores = mutual_info_regression(X_valid, y, random_state=SEED)\n",
    "        mi_scores_dict = dict(zip(valid_features, mi_scores))\n",
    "        scores['mutual_info'] = mi_scores_dict\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Combine scores using rank aggregation\n",
    "    if scores:\n",
    "        feature_ranks = {}\n",
    "        for feature in valid_features:\n",
    "            ranks = []\n",
    "            for method, score_dict in scores.items():\n",
    "                if feature in score_dict:\n",
    "                    # Convert to rank (lower rank = better)\n",
    "                    sorted_features = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "                    rank = next(i for i, (f, _) in enumerate(sorted_features) if f == feature)\n",
    "                    ranks.append(rank)\n",
    "            feature_ranks[feature] = np.mean(ranks) if ranks else len(valid_features)\n",
    "        \n",
    "        # Select top k features\n",
    "        selected = sorted(feature_ranks.items(), key=lambda x: x[1])[:k]\n",
    "        return [f[0] for f in selected]\n",
    "    \n",
    "    return valid_features[:k]\n",
    "\n",
    "def enhanced_feature_selection(X: pd.DataFrame, y: pd.Series, k: int = 100) -> List[str]:\n",
    "    if len(X.columns) <= k:\n",
    "        return X.columns.tolist()\n",
    "    \n",
    "    # Enhanced validation with stability check\n",
    "    valid_features = []\n",
    "    for col in X.columns:\n",
    "        missing_rate = X[col].isnull().sum() / len(X)\n",
    "        unique_count = X[col].nunique()\n",
    "        \n",
    "        if missing_rate < 0.8 and unique_count > 1:\n",
    "            if unique_count > 2:\n",
    "                try:\n",
    "                    col_std = X[col].std()\n",
    "                    col_mean = X[col].mean()\n",
    "                    if col_std > 0 and not np.isnan(col_std) and not np.isinf(col_std):\n",
    "                        cv = np.abs(col_std / (col_mean + 1e-8))\n",
    "                        if cv < 100:\n",
    "                            valid_features.append(col)\n",
    "                except:\n",
    "                    valid_features.append(col)\n",
    "            else:\n",
    "                valid_features.append(col)\n",
    "    \n",
    "    if len(valid_features) <= k:\n",
    "        return valid_features\n",
    "    \n",
    "    X_valid = X[valid_features].copy()\n",
    "    \n",
    "    # Enhanced imputation for selection\n",
    "    for col in X_valid.columns:\n",
    "        if X_valid[col].dtype in [np.number]:\n",
    "            X_valid[col] = X_valid[col].fillna(X_valid[col].median())\n",
    "        else:\n",
    "            X_valid[col] = X_valid[col].fillna(X_valid[col].mode().iloc[0] if len(X_valid[col].mode()) > 0 else 0)\n",
    "    \n",
    "    scores = {}\n",
    "    try:\n",
    "        f_selector = SelectKBest(score_func=f_regression, k='all')\n",
    "        f_selector.fit(X_valid, y)\n",
    "        f_scores = dict(zip(valid_features, f_selector.scores_))\n",
    "        # Normalize scores\n",
    "        max_score = max(f_scores.values())\n",
    "        f_scores = {k: v/max_score for k, v in f_scores.items()}\n",
    "        scores['f_regression'] = f_scores\n",
    "    except Exception as e:\n",
    "        print(f\"F-regression failed: {e}\")\n",
    "    \n",
    "    # Mutual information scores (non-linear relationships)\n",
    "    try:\n",
    "        mi_scores = mutual_info_regression(X_valid, y, random_state=SEED, n_neighbors=5)\n",
    "        mi_scores_dict = dict(zip(valid_features, mi_scores))\n",
    "\n",
    "        max_score = max(mi_scores_dict.values()) if max(mi_scores_dict.values()) > 0 else 1\n",
    "        mi_scores_dict = {k: v/max_score for k, v in mi_scores_dict.items()}\n",
    "        scores['mutual_info'] = mi_scores_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Mutual information failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        corr_scores = {}\n",
    "        for col in valid_features:\n",
    "            corr = np.abs(np.corrcoef(X_valid[col], y)[0, 1])\n",
    "            corr_scores[col] = corr if not np.isnan(corr) else 0\n",
    "        # Normalize scores\n",
    "        max_score = max(corr_scores.values()) if max(corr_scores.values()) > 0 else 1\n",
    "        corr_scores = {k: v/max_score for k, v in corr_scores.items()}\n",
    "        scores['correlation'] = corr_scores\n",
    "    except Exception as e:\n",
    "        print(f\"Correlation scoring failed: {e}\")\n",
    "    \n",
    "    # Enhanced rank aggregation with weighted voting\n",
    "    if scores:\n",
    "        feature_ranks = {}\n",
    "        weights = {'f_regression': 0.4, 'mutual_info': 0.4, 'correlation': 0.2}  # Prioritize advanced methods\n",
    "        \n",
    "        for feature in valid_features:\n",
    "            weighted_score = 0\n",
    "            total_weight = 0\n",
    "            \n",
    "            for method, score_dict in scores.items():\n",
    "                if feature in score_dict and method in weights:\n",
    "                    weighted_score += score_dict[feature] * weights[method]\n",
    "                    total_weight += weights[method]\n",
    "            \n",
    "            feature_ranks[feature] = weighted_score / total_weight if total_weight > 0 else 0\n",
    "        \n",
    "        # Select top k features based on weighted scores\n",
    "        selected = sorted(feature_ranks.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        return [f[0] for f in selected]\n",
    "    \n",
    "    # Fallback to simple variance-based selection\n",
    "    print(\"Using variance-based feature selection as fallback\")\n",
    "    try:\n",
    "        variances = X_valid.var()\n",
    "        top_var_features = variances.nlargest(k).index.tolist()\n",
    "        return top_var_features\n",
    "    except:\n",
    "        return valid_features[:k]\n",
    "\n",
    "def create_polynomial_features(df: pd.DataFrame, df_test: Optional[pd.DataFrame], top_n: int = 5) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy() if df_test is not None else None\n",
    "    \n",
    "    # Select top numeric features by variance (simple heuristic)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) == 0:\n",
    "        return df, df_test\n",
    "    \n",
    "    # Get top features by variance (proxy for informativeness)\n",
    "    try:\n",
    "        variances = df[numeric_cols].var()\n",
    "        top_features = variances.nlargest(min(top_n, len(numeric_cols))).index.tolist()\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "        \n",
    "        df_poly = df[top_features].fillna(0)\n",
    "        poly_features = poly.fit_transform(df_poly)\n",
    "        \n",
    "        feature_names = poly.get_feature_names_out(top_features)\n",
    "        \n",
    "        interaction_names = [name for name in feature_names if ' ' in name]  # Has interaction\n",
    "        interaction_indices = [i for i, name in enumerate(feature_names) if ' ' in name]\n",
    "        \n",
    "        if len(interaction_indices) > 0:\n",
    "            poly_df = pd.DataFrame(\n",
    "                poly_features[:, interaction_indices], \n",
    "                columns=[f\"poly_{name.replace(' ', '_')}\" for name in interaction_names],\n",
    "                index=df.index\n",
    "            )\n",
    "            df = pd.concat([df, poly_df], axis=1)\n",
    "            \n",
    "            if df_test is not None:\n",
    "                df_test_poly = df_test[top_features].fillna(0)\n",
    "                poly_features_test = poly.transform(df_test_poly)\n",
    "                poly_df_test = pd.DataFrame(\n",
    "                    poly_features_test[:, interaction_indices],\n",
    "                    columns=[f\"poly_{name.replace(' ', '_')}\" for name in interaction_names],\n",
    "                    index=df_test.index\n",
    "                )\n",
    "                df_test = pd.concat([df_test, poly_df_test], axis=1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Polynomial feature creation failed: {e}\")\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "def create_cluster_features(df: pd.DataFrame, df_test: Optional[pd.DataFrame], n_clusters: int = 8) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    df = df.copy()\n",
    "    df_test = df_test.copy() if df_test is not None else None\n",
    "    \n",
    "    # Get numeric features for clustering\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) < 2:\n",
    "        return df, df_test\n",
    "    \n",
    "    try:\n",
    "        cluster_data = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        scaler = StandardScaler()\n",
    "        cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
    "        clusters = kmeans.fit_predict(cluster_data_scaled)\n",
    "        \n",
    "        df['biocomputer_cluster'] = clusters\n",
    "        \n",
    "        cluster_distances = kmeans.transform(cluster_data_scaled)\n",
    "        for i in range(n_clusters):\n",
    "            df[f'distance_to_cluster_{i}'] = cluster_distances[:, i]\n",
    "        \n",
    "        df['min_cluster_distance'] = cluster_distances.min(axis=1)\n",
    "        \n",
    "        if df_test is not None:\n",
    "            test_cluster_data = df_test[numeric_cols].fillna(df[numeric_cols].median())\n",
    "            test_cluster_data_scaled = scaler.transform(test_cluster_data)\n",
    "            \n",
    "            test_clusters = kmeans.predict(test_cluster_data_scaled)\n",
    "            df_test['biocomputer_cluster'] = test_clusters\n",
    "            \n",
    "            test_cluster_distances = kmeans.transform(test_cluster_data_scaled)\n",
    "            for i in range(n_clusters):\n",
    "                df_test[f'distance_to_cluster_{i}'] = test_cluster_distances[:, i]\n",
    "            \n",
    "            df_test['min_cluster_distance'] = test_cluster_distances.min(axis=1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Cluster feature creation failed: {e}\")\n",
    "    \n",
    "    return df, df_test\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# -------------------- Load -------------------- #\n",
    "seed_everything(SEED)\n",
    "\n",
    "train_path = \"./MiNDAT.csv\"\n",
    "test_path = \"./MiNDAT_UNK.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train = clean_columns(train)\n",
    "\n",
    "# Basic hygiene\n",
    "if TARGET not in train.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET}' not in training data.\")\n",
    "\n",
    "# Drop rows with missing target\n",
    "train = train.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "# Remove outliers via IQR (robust)\n",
    "Q1, Q3 = train[TARGET].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lb, ub = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "train = train[(train[TARGET] >= lb) & (train[TARGET] <= ub)].reset_index(drop=True)\n",
    "\n",
    "# Identify columns (keep features simple: no extra row stats or poly features)\n",
    "features = [c for c in train.columns if c not in [TARGET, ID_COL]]\n",
    "X_full = train[features].copy()\n",
    "y_full = train[TARGET].copy()\n",
    "\n",
    "if test is not None:\n",
    "    X_test_full = test[features].copy()\n",
    "else:\n",
    "    X_test_full = None\n",
    "\n",
    "# Replace infs\n",
    "for df in [X_full] + ([X_test_full] if X_test_full is not None else []):\n",
    "    if df is None:\n",
    "        continue\n",
    "    numc = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numc] = df[numc].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Advanced memory optimization\n",
    "X_full = advanced_downcast(X_full)\n",
    "if X_test_full is not None:\n",
    "    X_test_full = advanced_downcast(X_test_full)\n",
    "\n",
    "# Split types\n",
    "cat_cols = X_full.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Simple imputers (per column)\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_full[num_cols] = num_imputer.fit_transform(X_full[num_cols])\n",
    "if X_test_full is not None:\n",
    "    X_test_full[num_cols] = num_imputer.transform(X_test_full[num_cols])\n",
    "\n",
    "X_full[cat_cols] = cat_imputer.fit_transform(X_full[cat_cols])\n",
    "if X_test_full is not None:\n",
    "    X_test_full[cat_cols] = cat_imputer.transform(X_test_full[cat_cols])\n",
    "\n",
    "# Cast categoricals to 'category' dtype (memory);\n",
    "for c in cat_cols:\n",
    "    X_full[c] = X_full[c].astype(\"category\")\n",
    "    if X_test_full is not None:\n",
    "        X_test_full[c] = X_test_full[c].astype(\"category\")\n",
    "\n",
    "# -------------------- Advanced Feature Engineering -------------------- #\n",
    "print(\"\\nADVANCED FEATURE ENGINEERING FOR CORRUCYST FRAGMENTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if USE_ADVANCED_FEATURES:\n",
    "    print(\"Creating domain-specific biocomputer features...\")\n",
    "    X_fe = create_advanced_features(X_full.copy(), num_cols)\n",
    "    if X_test_full is not None:\n",
    "        X_test_fe = create_advanced_features(X_test_full.copy(), num_cols)\n",
    "    else:\n",
    "        X_test_fe = None\n",
    "    print(f\"✓ Feature engineering complete. Added {X_fe.shape[1] - X_full.shape[1]} new features\")\n",
    "else:\n",
    "    X_fe, X_test_fe = X_full.copy(), (X_test_full.copy() if X_test_full is not None else None)\n",
    "\n",
    "# Create interaction features for most important original features\n",
    "if USE_INTERACTIONS:\n",
    "    print(\"Creating interaction features for alien data patterns...\")\n",
    "    X_fe, X_test_fe = create_biocomputer_interactions(X_fe, X_test_fe, num_cols[:10])  # Top 10 original features\n",
    "\n",
    "# Create temporal/sequence features (alien memory simulation context)\n",
    "if USE_ADVANCED_FEATURES:\n",
    "    print(\"Creating memory sequence simulation features...\")\n",
    "    X_fe, X_test_fe = create_memory_sequence_features(X_fe, X_test_fe, num_cols)\n",
    "\n",
    "# Create network topology features (distributed biocomputer network context)\n",
    "if USE_ADVANCED_FEATURES:\n",
    "    print(\"Creating biocomputer network topology features...\")\n",
    "    X_fe, X_test_fe = create_network_topology_features(X_fe, X_test_fe, num_cols)\n",
    "\n",
    "print(f\"✓ All feature engineering complete. Final shape: {X_fe.shape}\")\n",
    "\n",
    "# KFold Target Encoding (leakage-safe)\n",
    "if len(cat_cols) > 0:\n",
    "    print(\"Performing enhanced target encoding...\")\n",
    "    X_te, X_test_te = enhanced_target_encode(\n",
    "        X_fe,\n",
    "        y_full,\n",
    "        X_test_fe if X_test_fe is not None else X_fe.iloc[:0],\n",
    "        cat_cols,\n",
    "    )\n",
    "else:\n",
    "    X_te, X_test_te = X_fe.copy(), (X_test_fe.copy() if X_test_fe is not None else None)\n",
    "\n",
    "X_te = X_te.drop(columns=cat_cols, errors=\"ignore\")\n",
    "if X_test_te is not None:\n",
    "    X_test_te = X_test_te.drop(columns=cat_cols, errors=\"ignore\")\n",
    "\n",
    "# Feature selection with enhanced strategy\n",
    "if USE_FEATURE_SELECTION and len(X_te.columns) > SELECT_TOP_K_FEATURES:\n",
    "    print(f\"Selecting top {SELECT_TOP_K_FEATURES} features from {len(X_te.columns)} candidates...\")\n",
    "    selected_features = enhanced_feature_selection(X_te, y_full, SELECT_TOP_K_FEATURES)\n",
    "    X_te = X_te[selected_features]\n",
    "    if X_test_te is not None:\n",
    "        X_test_te = X_test_te[selected_features]\n",
    "    print(f\"✓ Selected {len(selected_features)} most informative features\")\n",
    "\n",
    "if USE_POLYNOMIAL:\n",
    "    print(\"Creating polynomial combinations for top predictive features...\")\n",
    "    X_te, X_test_te = create_polynomial_features(X_te, X_test_te, top_n=5)\n",
    "\n",
    "if USE_CLUSTERING:\n",
    "    print(\"Creating clustering-based biocomputer network features...\")\n",
    "    X_te, X_test_te = create_cluster_features(X_te, X_test_te, n_clusters=8)\n",
    "\n",
    "print(f\"✓ Final feature matrix ready: {X_te.shape[1]} features for {len(X_te)} CORRUCYST fragments\")\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='normal', random_state=SEED)\n",
    "X_lin = X_te.copy()\n",
    "lin_num_cols = X_lin.select_dtypes(include=[np.number]).columns\n",
    "X_lin[lin_num_cols] = scaler.fit_transform(X_lin[lin_num_cols])\n",
    "\n",
    "if X_test_te is not None:\n",
    "    X_test_lin = X_test_te.copy()\n",
    "    lin_test_num = X_test_lin.select_dtypes(include=[np.number]).columns\n",
    "    X_test_lin[lin_test_num] = scaler.transform(X_test_lin[lin_test_num])\n",
    "else:\n",
    "    X_test_lin = None\n",
    "\n",
    "use_log = False\n",
    "if AUTO_LOG_TARGET:\n",
    "    skew_val = pd.Series(y_full).skew()\n",
    "    if skew_val > 1.0:\n",
    "        use_log = True\n",
    "        y_work = np.log1p(y_full)\n",
    "    else:\n",
    "        y_work = y_full.copy()\n",
    "else:\n",
    "    y_work = y_full.copy()\n",
    "\n",
    "# Stratified folds by target bins (for stability)\n",
    "strat_bins = make_target_bins_improved(y_full, n_bins=8)\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)# -------------------- Base Models -------------------- #\n",
    "base_models: Dict[str, Tuple[object, str]] = {}\n",
    "\n",
    "if HAS_LGB:\n",
    "    base_models[\"LGBM\"] = (\n",
    "        LGBMRegressor(\n",
    "            n_estimators=2000,  # Increased from 800\n",
    "            learning_rate=0.05,  # Slightly increased to compensate\n",
    "            max_depth=8,  # INcreased from 6\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            min_child_samples=20,\n",
    "            random_state=SEED,\n",
    "            n_jobs=1,  # Changed from -1 to avoid threading issues\n",
    "            verbose=-1,  # Added explicit verbose parameter\n",
    "            objective='regression',  # Explicit objective\n",
    "            boosting_type='gbdt',  # Explicit boosting type\n",
    "        ),\n",
    "        \"tree\",\n",
    "    )\n",
    "\n",
    "if HAS_XGB:\n",
    "    base_models[\"XGB\"] = (\n",
    "        XGBRegressor(\n",
    "            n_estimators=1200,\n",
    "            learning_rate=0.03,\n",
    "            max_depth=8,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.0,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\",\n",
    "        ),\n",
    "        \"tree\",\n",
    "    )\n",
    "\n",
    "if HAS_CAT:\n",
    "    base_models[\"CAT\"] = (\n",
    "        CatBoostRegressor(\n",
    "            depth=8,\n",
    "            learning_rate=0.03,\n",
    "            n_estimators=1500,\n",
    "            subsample=0.8,\n",
    "            random_state=SEED,\n",
    "            loss_function=\"RMSE\",\n",
    "            verbose=False,\n",
    "        ),\n",
    "        \"tree\",\n",
    "    )\n",
    "\n",
    "# Strong classical ensembles\n",
    "base_models[\"ET\"] = (\n",
    "    ExtraTreesRegressor(\n",
    "        n_estimators=1000, #increased from 800\n",
    "        max_depth=None,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"RF\"] = (\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=1000, #increased from 800\n",
    "        max_depth=None,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"GBDT\"] = (\n",
    "    GradientBoostingRegressor(\n",
    "        n_estimators=800, #increased from 600\n",
    "        learning_rate=0.03,\n",
    "        max_depth=3,\n",
    "        random_state=SEED,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "# Linear baselines on scaled features (kept for diversity)\n",
    "base_models[\"Ridge\"] = (\n",
    "    Ridge(alpha=5.0, random_state=SEED),\n",
    "    \"linear\",\n",
    ")\n",
    "base_models[\"Lasso\"] = (\n",
    "    Lasso(alpha=0.0005, random_state=SEED, max_iter=20000),\n",
    "    \"linear\",\n",
    ")\n",
    "\n",
    "# -------------- OOF Training / Prediction -------------- #\n",
    "models_oof = {}\n",
    "models_test_pred = {}\n",
    "models_fold_objs = {}\n",
    "fold_scores = []\n",
    "\n",
    "# Feature matrices: tree models use TE (numeric-only); linear models use scaled copy\n",
    "X_tree = X_te\n",
    "X_tree_test = X_test_te\n",
    "\n",
    "X_linear = X_lin\n",
    "X_linear_test = X_test_lin\n",
    "\n",
    "for name, (model, mtype) in base_models.items():\n",
    "    print(f\"\\n[Model={name}] training with {NFOLDS}-fold CV ...\")\n",
    "    oof = np.zeros(len(X_tree))\n",
    "    test_preds_folds = []\n",
    "    fold_importances = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_tree, strat_bins)):\n",
    "        if mtype == \"tree\":\n",
    "            Xtr, Xva = X_tree.iloc[tr_idx], X_tree.iloc[va_idx]\n",
    "        else:\n",
    "            Xtr, Xva = X_linear.iloc[tr_idx], X_linear.iloc[va_idx]\n",
    "\n",
    "        ytr, yva = y_work.iloc[tr_idx], y_work.iloc[va_idx]\n",
    "\n",
    "        # Fresh instance per fold\n",
    "        m = type(model)(**model.get_params())\n",
    "\n",
    "        # Special handling for different model types\n",
    "        if name == \"LGBM\" and HAS_LGB:\n",
    "            # LightGBM specific handling with clean feature names and reset index\n",
    "            Xtr_clean = Xtr.copy().reset_index(drop=True)\n",
    "            Xva_clean = Xva.copy().reset_index(drop=True)\n",
    "            ytr_clean = ytr.reset_index(drop=True)\n",
    "            yva_clean = yva.reset_index(drop=True)\n",
    "            \n",
    "            # Ensure all data is numeric and clean feature names\n",
    "            Xtr_clean = Xtr_clean.select_dtypes(include=[np.number])\n",
    "            Xva_clean = Xva_clean.select_dtypes(include=[np.number])\n",
    "            \n",
    "            # Create simple sequential feature names to avoid any naming conflicts\n",
    "            n_features = len(Xtr_clean.columns)\n",
    "            simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "            \n",
    "            Xtr_clean.columns = simple_names\n",
    "            Xva_clean.columns = simple_names\n",
    "            \n",
    "            try:\n",
    "                # Use minimal parameters to avoid conflicts\n",
    "                lgb_params = {\n",
    "                    'n_estimators': 500,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'max_depth': 6,\n",
    "                    'num_leaves': 31,\n",
    "                    'subsample': 0.8,\n",
    "                    'colsample_bytree': 0.8,\n",
    "                    'random_state': SEED,\n",
    "                    'verbose': -1,\n",
    "                    'n_jobs': 1\n",
    "                }\n",
    "                m = LGBMRegressor(**lgb_params)\n",
    "                \n",
    "                # Fit without eval_set to avoid feature name conflicts\n",
    "                m.fit(Xtr_clean, ytr_clean)\n",
    "                pred_va = m.predict(Xva_clean)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: LGBM fold {fold} failed: {e}\")\n",
    "                pred_va = np.full(len(yva_clean), ytr_clean.mean())  # Fallback prediction\n",
    "                \n",
    "        elif name == \"CAT\" and HAS_CAT:\n",
    "            # CatBoost requires numeric input here (since we dropped original categorical cols).\n",
    "            m.fit(Xtr.astype(np.float32), ytr, eval_set=(Xva.astype(np.float32), yva), verbose=False)\n",
    "            pred_va = m.predict(Xva.astype(np.float32))\n",
    "        else:\n",
    "            m.fit(Xtr, ytr)\n",
    "            pred_va = m.predict(Xva)\n",
    "\n",
    "        oof[va_idx] = pred_va\n",
    "\n",
    "        if X_tree_test is not None:\n",
    "            Xte = X_tree_test if mtype == \"tree\" else X_linear_test\n",
    "            if name == \"LGBM\" and HAS_LGB:\n",
    "                # Apply same feature name cleaning for test predictions\n",
    "                Xte_clean = Xte.copy().reset_index(drop=True)\n",
    "                Xte_clean = Xte_clean.select_dtypes(include=[np.number])\n",
    "                \n",
    "                # Use same simple sequential names\n",
    "                n_features = len(Xte_clean.columns)\n",
    "                simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "                Xte_clean.columns = simple_names\n",
    "                \n",
    "                try:\n",
    "                    test_preds_folds.append(m.predict(Xte_clean))\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: LGBM test prediction failed: {e}\")\n",
    "                    test_preds_folds.append(np.full(len(Xte_clean), oof[oof != 0].mean() if len(oof[oof != 0]) > 0 else 0))\n",
    "            elif name == \"CAT\" and HAS_CAT:\n",
    "                test_preds_folds.append(m.predict(Xte.astype(np.float32)))\n",
    "            else:\n",
    "                test_preds_folds.append(m.predict(Xte))\n",
    "\n",
    "        # Save feature importances when available\n",
    "        if SAVE_FEATURE_IMPORTANCES and hasattr(m, \"feature_importances_\"):\n",
    "            fi = pd.DataFrame({\n",
    "                \"feature\": Xtr.columns,\n",
    "                \"importance\": m.feature_importances_,\n",
    "                \"fold\": fold,\n",
    "                \"model\": name,\n",
    "            })\n",
    "            fold_importances.append(fi)\n",
    "\n",
    "        models_fold_objs.setdefault(name, []).append(m)\n",
    "\n",
    "    # Back-transform if log was used\n",
    "    oof_final = np.expm1(oof) if use_log else oof\n",
    "    rmse_score = rmse(y_full, oof_final)\n",
    "\n",
    "    models_oof[name] = oof_final\n",
    "    if test_preds_folds:\n",
    "        pred_test_mean = np.mean(test_preds_folds, axis=0)\n",
    "        pred_test_final = np.expm1(pred_test_mean) if use_log else pred_test_mean\n",
    "        models_test_pred[name] = pred_test_final\n",
    "\n",
    "    fold_scores.append({\"model\": name, \"cv_rmse\": rmse_score})\n",
    "    print(f\"[Model={name}] CV RMSE: {rmse_score:.5f}\")\n",
    "\n",
    "    if SAVE_FEATURE_IMPORTANCES and len(fold_importances) > 0:\n",
    "        imp_df = pd.concat(fold_importances, ignore_index=True)\n",
    "        imp_df.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).head(50).to_csv(\n",
    "            f\"feature_importances_{name}.csv\", index=True\n",
    "        )\n",
    "\n",
    "# -------------------- Stacking (meta-model) -------------------- #\n",
    "print(\"\\n[Stacking] Training meta-model (GradientBoostingRegressor) on OOF predictions ...\")\n",
    "oof_df = pd.DataFrame(models_oof)\n",
    "\n",
    "# Non-linear meta learner to capture complex relations between model preds\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=SEED,\n",
    ")\n",
    "meta_model.fit(oof_df, y_full)\n",
    "meta_oof = meta_model.predict(oof_df)\n",
    "stack_rmse = rmse(y_full, meta_oof)\n",
    "print(f\"[Stacking] Meta CV RMSE: {stack_rmse:.5f}\")\n",
    "\n",
    "# Weighted Blend of the best individual models (by CV)\n",
    "sorted_models = sorted(fold_scores, key=lambda d: d[\"cv_rmse\"])[: min(3, len(fold_scores))]\n",
    "weights = []\n",
    "for d in sorted_models:\n",
    "    w = 1.0 / max(d[\"cv_rmse\"], 1e-6)\n",
    "    weights.append(w)\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "print(\"\\n[Blending] Top models & weights:\")\n",
    "for (d, w) in zip(sorted_models, weights):\n",
    "    print(f\"  - {d['model']}: weight={w:.3f}, cv_rmse={d['cv_rmse']:.5f}\")\n",
    "\n",
    "# -------------------- Export OOF / Test Predictions -------------------- #\n",
    "pd.DataFrame({\"id\": train[ID_COL], \"y\": y_full, **models_oof}).to_csv(\"oof_predictions.csv\", index=False)\n",
    "pd.DataFrame(fold_scores).to_csv(\"fold_scores.csv\", index=False)\n",
    "\n",
    "# Final test prediction (if test exists)\n",
    "if X_test_full is not None and len(models_test_pred) > 0:\n",
    "    # Stacking test preds\n",
    "    test_stack_mat = pd.DataFrame(models_test_pred)\n",
    "    stack_pred = meta_model.predict(test_stack_mat)\n",
    "\n",
    "    # Weighted blend of top models\n",
    "    blend_pred = np.zeros(len(test_stack_mat))\n",
    "    for (d, w) in zip(sorted_models, weights):\n",
    "        blend_pred += w * test_stack_mat[d[\"model\"]].values\n",
    "\n",
    "    # Final combo: average of stack & blend (often stabilizes)\n",
    "    final_pred = 0.5 * stack_pred + 0.5 * blend_pred\n",
    "\n",
    "    sub = pd.DataFrame({ID_COL: test[ID_COL], TARGET: final_pred})\n",
    "    sub.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"\\nSaved submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0724a5",
   "metadata": {},
   "source": [
    "# Approach 6 (Tried Optimising the Best Approach by tuning parameters furhter by using Optuna...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869db54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "HAS_XGB = True\n",
    "from lightgbm import LGBMRegressor\n",
    "HAS_LGB = True\n",
    "from catboost import CatBoostRegressor\n",
    "HAS_CAT = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "NFOLDS = 7\n",
    "NREPEATS = 2\n",
    "TARGET = \"CORRUCYSTIC_DENSITY\"\n",
    "ID_COL = \"LOCAL_IDENTIFIER\"\n",
    "AUTO_LOG_TARGET = True\n",
    "SMOOTHING_M = 25.0\n",
    "USE_FEATURE_SELECTION = True\n",
    "SELECT_TOP_K_FEATURES = 100\n",
    "SAVE_FEATURE_IMPORTANCES = True\n",
    "\n",
    "# Optuna Configuration\n",
    "USE_OPTUNA = True\n",
    "OPTUNA_N_TRIALS = 50  # Number of optimization trials per model\n",
    "OPTUNA_TIMEOUT = 300  # 5 minutes timeout per model optimization\n",
    "OPTUNA_N_JOBS = 1  # Parallel jobs for Optuna (keep at 1 to avoid conflicts)\n",
    "OPTUNA_CV_FOLDS = 3  # Reduced CV folds for faster optimization\n",
    "OUTLIER_METHOD = \"isolation_forest\"  \n",
    "OUTLIER_CONTAMINATION = 0.05\n",
    "\n",
    "def seed_everything(seed: int = SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_first_existing(paths: List[str]) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(c)).strip(\"_\") for c in df.columns]\n",
    "    df.columns = [re.sub(r\"_+\", \"_\", c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def advanced_downcast(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        col = df[c]\n",
    "        c_min, c_max = col.min(), col.max()\n",
    "        \n",
    "        if pd.api.types.is_integer_dtype(col):\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[c] = col.astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[c] = col.astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[c] = col.astype(np.int32)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[c] = col.astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def detect_outliers_advanced(df: pd.DataFrame, target_col: str, method: str = \"isolation_forest\", contamination: float = 0.05) -> pd.Series:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    \n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    \n",
    "    if method == \"iqr\":\n",
    "        Q1, Q3 = df[target_col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lb, ub = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        mask = (df[target_col] >= lb) & (df[target_col] <= ub)\n",
    "    \n",
    "    elif method == \"zscore\":\n",
    "        z_scores = np.abs(stats.zscore(df[target_col]))\n",
    "        mask = z_scores < 3\n",
    "    \n",
    "    elif method == \"isolation_forest\":\n",
    "        iso_forest = IsolationForest(contamination=contamination, random_state=SEED)\n",
    "        outliers = iso_forest.fit_predict(df[[target_col]])\n",
    "        mask = outliers == 1\n",
    "    \n",
    "    elif method == \"local_outlier\":\n",
    "        lof = LocalOutlierFactor(contamination=contamination)\n",
    "        outliers = lof.fit_predict(df[[target_col]])\n",
    "        mask = outliers == 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def make_target_bins_improved(y: pd.Series, n_bins: int = 10) -> pd.Series:\n",
    "    try:\n",
    "        bins = pd.qcut(y, q=n_bins, labels=False, duplicates=\"drop\")\n",
    "        if bins.nunique() < n_bins // 2:\n",
    "            raise ValueError(\"Too few unique bins\")\n",
    "        return bins.astype(int)\n",
    "    except:\n",
    "        bins = pd.cut(y, bins=n_bins, labels=False, include_lowest=True)\n",
    "        return bins.astype(int)\n",
    "\n",
    "def enhanced_target_encode(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    n_splits: int = NFOLDS,\n",
    "    n_repeats: int = NREPEATS,\n",
    "    smoothing_m: float = SMOOTHING_M,\n",
    "    seed: int = SEED,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    global_mean = y.mean()\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        oof_list = []\n",
    "        test_encoded_list = []\n",
    "        \n",
    "        for tr_idx, val_idx in rkf.split(X):\n",
    "            tr_y = y.iloc[tr_idx]\n",
    "            tr_col = X.iloc[tr_idx][col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            val_col = X.iloc[val_idx][col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            \n",
    "            # Calculate statistics with smoothing\n",
    "            stats_df = tr_y.groupby(tr_col).agg(['sum', 'count', 'mean', 'std']).reset_index()\n",
    "            stats_df.columns = [col, 'sum', 'count', 'mean', 'std']\n",
    "            stats_df['enc'] = (stats_df['sum'] + smoothing_m * global_mean) / (stats_df['count'] + smoothing_m)\n",
    "            \n",
    "            # Add uncertainty-based noise\n",
    "            stats_df['uncertainty'] = 1 / np.sqrt(stats_df['count'] + 1)\n",
    "            noise_scale = stats_df['uncertainty'] * stats_df['std'].fillna(y.std())\n",
    "            stats_df['enc'] += np.random.normal(0, noise_scale * 0.1, len(stats_df))\n",
    "            \n",
    "            # Create mapping dict\n",
    "            encode_map = dict(zip(stats_df[col], stats_df['enc']))\n",
    "            \n",
    "            # Encode validation set\n",
    "            oof_encoded = val_col.map(encode_map).fillna(global_mean)\n",
    "            oof_list.append(pd.Series(oof_encoded.values, index=val_col.index))\n",
    "            \n",
    "            # Encode test set\n",
    "            test_col = X_test[col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            test_encoded = test_col.map(encode_map).fillna(global_mean)\n",
    "            test_encoded_list.append(test_encoded.values)\n",
    "        \n",
    "        # Combine OOF predictions\n",
    "        oof_combined = pd.concat(oof_list).groupby(level=0).mean()\n",
    "        X[col + \"__te\"] = oof_combined.reindex(X.index).astype(np.float32)\n",
    "        \n",
    "        # Average test predictions\n",
    "        X_test[col + \"__te\"] = np.mean(test_encoded_list, axis=0).astype(np.float32)\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "def create_advanced_features(df: pd.DataFrame, num_cols: List[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    if len(num_cols) == 0:\n",
    "        return df\n",
    "    \n",
    "    num_data = df[num_cols].select_dtypes(include=[np.number])\n",
    "    if num_data.empty:\n",
    "        return df\n",
    "    \n",
    "    # Basic statistical features\n",
    "    df[\"corr_density_mean\"] = num_data.mean(axis=1)\n",
    "    df[\"corr_density_std\"] = num_data.std(axis=1)\n",
    "    df[\"corr_density_skew\"] = num_data.skew(axis=1)\n",
    "    df[\"corr_density_kurt\"] = num_data.kurtosis(axis=1)\n",
    "    df[\"corr_density_range\"] = num_data.max(axis=1) - num_data.min(axis=1)\n",
    "    \n",
    "    # Percentile features\n",
    "    df[\"corr_density_q25\"] = num_data.quantile(0.25, axis=1)\n",
    "    df[\"corr_density_q75\"] = num_data.quantile(0.75, axis=1)\n",
    "    df[\"corr_density_iqr\"] = df[\"corr_density_q75\"] - df[\"corr_density_q25\"]\n",
    "    \n",
    "    # Count-based features\n",
    "    df[\"corr_positive_count\"] = (num_data > 0).sum(axis=1)\n",
    "    df[\"corr_negative_count\"] = (num_data < 0).sum(axis=1)\n",
    "    df[\"corr_zero_count\"] = (num_data == 0).sum(axis=1)\n",
    "    df[\"corr_missing_count\"] = num_data.isnull().sum(axis=1)\n",
    "    \n",
    "    # Advanced statistical measures\n",
    "    df[\"corr_cv\"] = df[\"corr_density_std\"] / (np.abs(df[\"corr_density_mean\"]) + 1e-8)\n",
    "    df[\"corr_mad\"] = num_data.apply(manual_mad, axis=1)  # Using manual MAD implementation\n",
    "    \n",
    "    # Energy and signal processing inspired features (domain-specific)\n",
    "    df[\"corr_energy\"] = (num_data ** 2).sum(axis=1)\n",
    "    df[\"corr_rms\"] = np.sqrt(df[\"corr_energy\"] / len(num_cols))\n",
    "    \n",
    "    # Handle infinite and NaN values\n",
    "    inf_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[inf_cols] = df[inf_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def manual_mad(series):\n",
    "    median = series.median()\n",
    "    return np.median(np.abs(series - median))\n",
    "\n",
    "def advanced_feature_selection(X: pd.DataFrame, y: pd.Series, k: int = 100) -> List[str]:\n",
    "    if len(X.columns) <= k:\n",
    "        return X.columns.tolist()\n",
    "    \n",
    "    # Remove features with too many missing values or zero variance\n",
    "    valid_features = []\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() / len(X) < 0.8 and X[col].nunique() > 1:\n",
    "            valid_features.append(col)\n",
    "    \n",
    "    X_valid = X[valid_features].fillna(X[valid_features].median())\n",
    "    \n",
    "    # Combine multiple selection methods\n",
    "    scores = {}\n",
    "    \n",
    "    # F-regression scores\n",
    "    try:\n",
    "        f_selector = SelectKBest(score_func=f_regression, k='all')\n",
    "        f_selector.fit(X_valid, y)\n",
    "        f_scores = dict(zip(valid_features, f_selector.scores_))\n",
    "        scores['f_regression'] = f_scores\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Mutual information scores\n",
    "    try:\n",
    "        mi_scores = mutual_info_regression(X_valid, y, random_state=SEED)\n",
    "        mi_scores_dict = dict(zip(valid_features, mi_scores))\n",
    "        scores['mutual_info'] = mi_scores_dict\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Combine scores using rank aggregation\n",
    "    if scores:\n",
    "        feature_ranks = {}\n",
    "        for feature in valid_features:\n",
    "            ranks = []\n",
    "            for method, score_dict in scores.items():\n",
    "                if feature in score_dict:\n",
    "                    # Convert to rank (lower rank = better)\n",
    "                    sorted_features = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "                    rank = next(i for i, (f, _) in enumerate(sorted_features) if f == feature)\n",
    "                    ranks.append(rank)\n",
    "            feature_ranks[feature] = np.mean(ranks) if ranks else len(valid_features)\n",
    "        \n",
    "        # Select top k features\n",
    "        selected = sorted(feature_ranks.items(), key=lambda x: x[1])[:k]\n",
    "        return [f[0] for f in selected]\n",
    "    \n",
    "    return valid_features[:k]\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# -------------------- Optuna Hyperparameter Optimization -------------------- #\n",
    "def optimize_lgbm(X, y, cv_folds):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "            'random_state': SEED,\n",
    "            'verbose': -1,\n",
    "            'n_jobs': 1\n",
    "        }\n",
    "        \n",
    "        scores = []\n",
    "        for train_idx, val_idx in cv_folds.split(X, make_target_bins_improved(y, n_bins=8)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            # Clean feature names for LightGBM\n",
    "            X_train_clean = X_train.select_dtypes(include=[np.number]).reset_index(drop=True)\n",
    "            X_val_clean = X_val.select_dtypes(include=[np.number]).reset_index(drop=True)\n",
    "            n_features = len(X_train_clean.columns)\n",
    "            simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "            X_train_clean.columns = simple_names\n",
    "            X_val_clean.columns = simple_names\n",
    "            \n",
    "            try:\n",
    "                model = LGBMRegressor(**params)\n",
    "                model.fit(X_train_clean, y_train.reset_index(drop=True))\n",
    "                pred = model.predict(X_val_clean)\n",
    "                scores.append(rmse(y_val.reset_index(drop=True), pred))\n",
    "            except Exception as e:\n",
    "                return float('inf')  # Return worst score if error\n",
    "                \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def optimize_xgb(X, y, cv_folds):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1500, step=50),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "            'random_state': SEED,\n",
    "            'n_jobs': 1,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "        \n",
    "        scores = []\n",
    "        for train_idx, val_idx in cv_folds.split(X, make_target_bins_improved(y, n_bins=8)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            try:\n",
    "                model = XGBRegressor(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                pred = model.predict(X_val)\n",
    "                scores.append(rmse(y_val, pred))\n",
    "            except Exception as e:\n",
    "                return float('inf')\n",
    "                \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def optimize_catboost(X, y, cv_folds):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1500, step=50),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'depth': trial.suggest_int('depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'random_state': SEED,\n",
    "            'verbose': False,\n",
    "            'loss_function': 'RMSE'\n",
    "        }\n",
    "        \n",
    "        scores = []\n",
    "        for train_idx, val_idx in cv_folds.split(X, make_target_bins_improved(y, n_bins=8)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            try:\n",
    "                model = CatBoostRegressor(**params)\n",
    "                model.fit(X_train.astype(np.float32), y_train)\n",
    "                pred = model.predict(X_val.astype(np.float32))\n",
    "                scores.append(rmse(y_val, pred))\n",
    "            except Exception as e:\n",
    "                return float('inf')\n",
    "                \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def optimize_random_forest(X, y, cv_folds):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'random_state': SEED,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        scores = []\n",
    "        for train_idx, val_idx in cv_folds.split(X, make_target_bins_improved(y, n_bins=8)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            try:\n",
    "                model = RandomForestRegressor(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                pred = model.predict(X_val)\n",
    "                scores.append(rmse(y_val, pred))\n",
    "            except Exception as e:\n",
    "                return float('inf')\n",
    "                \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def run_optuna_optimization(X, y, model_name):\n",
    "    print(f\"\\n[Optuna] Optimizing {model_name} hyperparameters...\")\n",
    "    \n",
    "    # Create CV folds for optimization\n",
    "    strat_bins = make_target_bins_improved(y, n_bins=8)\n",
    "    cv_folds = StratifiedKFold(n_splits=OPTUNA_CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Select objective function based on model\n",
    "    if model_name == \"LGBM\" and HAS_LGB:\n",
    "        objective_func = optimize_lgbm(X, y, cv_folds)\n",
    "    elif model_name == \"XGB\" and HAS_XGB:\n",
    "        objective_func = optimize_xgb(X, y, cv_folds)\n",
    "    elif model_name == \"CAT\" and HAS_CAT:\n",
    "        objective_func = optimize_catboost(X, y, cv_folds)\n",
    "    elif model_name == \"RF\":\n",
    "        objective_func = optimize_random_forest(X, y, cv_folds)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=TPESampler(seed=SEED),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    # Optimize\n",
    "    study.optimize(\n",
    "        objective_func,\n",
    "        n_trials=OPTUNA_N_TRIALS,\n",
    "        timeout=OPTUNA_TIMEOUT,\n",
    "        n_jobs=OPTUNA_N_JOBS,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(f\"[Optuna] {model_name} - Best score: {study.best_value:.5f}\")\n",
    "    print(f\"[Optuna] {model_name} - Best params: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# -------------------- Load -------------------- #\n",
    "seed_everything(SEED)\n",
    "\n",
    "train_path = \"./MinDAT.csv\"\n",
    "test_path = \"./MiDAT_UNK.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train = clean_columns(train)\n",
    "\n",
    "test = pd.read_csv(test_path)\n",
    "test = clean_columns(test)\n",
    "\n",
    "# Basic hygiene\n",
    "if TARGET not in train.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET}' not in training data.\")\n",
    "\n",
    "# Drop rows with missing target\n",
    "train = train.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "# Remove outliers via IQR (robust)\n",
    "Q1, Q3 = train[TARGET].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lb, ub = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "train = train[(train[TARGET] >= lb) & (train[TARGET] <= ub)].reset_index(drop=True)\n",
    "\n",
    "# Identify columns (keep features simple: no extra row stats or poly features)\n",
    "features = [c for c in train.columns if c not in [TARGET, ID_COL]]\n",
    "X_full = train[features].copy()\n",
    "y_full = train[TARGET].copy()\n",
    "\n",
    "if test is not None:\n",
    "    X_test_full = test[features].copy()\n",
    "else:\n",
    "    X_test_full = None\n",
    "\n",
    "# Replace infs\n",
    "for df in [X_full] + ([X_test_full] if X_test_full is not None else []):\n",
    "    if df is None:\n",
    "        continue\n",
    "    numc = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numc] = df[numc].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Advanced memory optimization\n",
    "X_full = advanced_downcast(X_full)\n",
    "if X_test_full is not None:\n",
    "    X_test_full = advanced_downcast(X_test_full)\n",
    "\n",
    "# Split types\n",
    "cat_cols = X_full.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Simple imputers (per column)\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_full[num_cols] = num_imputer.fit_transform(X_full[num_cols])\n",
    "if X_test_full is not None:\n",
    "    X_test_full[num_cols] = num_imputer.transform(X_test_full[num_cols])\n",
    "\n",
    "X_full[cat_cols] = cat_imputer.fit_transform(X_full[cat_cols])\n",
    "if X_test_full is not None:\n",
    "    X_test_full[cat_cols] = cat_imputer.transform(X_test_full[cat_cols])\n",
    "\n",
    "# Cast categoricals to 'category' dtype (memory);\n",
    "for c in cat_cols:\n",
    "    X_full[c] = X_full[c].astype(\"category\")\n",
    "    if X_test_full is not None:\n",
    "        X_test_full[c] = X_test_full[c].astype(\"category\")\n",
    "\n",
    "if len(cat_cols) > 0:\n",
    "    print(\"Performing enhanced target encoding...\")\n",
    "    X_te, X_test_te = enhanced_target_encode(\n",
    "        X_full,\n",
    "        y_full,\n",
    "        X_test_full if X_test_full is not None else X_full.iloc[:0],\n",
    "        cat_cols,\n",
    "    )\n",
    "else:\n",
    "    X_te, X_test_te = X_full.copy(), (X_test_full.copy() if X_test_full is not None else None)\n",
    "\n",
    "X_te = X_te.drop(columns=cat_cols, errors=\"ignore\")\n",
    "if X_test_te is not None:\n",
    "    X_test_te = X_test_te.drop(columns=cat_cols, errors=\"ignore\")\n",
    "\n",
    "# Feature selection\n",
    "if USE_FEATURE_SELECTION and len(X_te.columns) > SELECT_TOP_K_FEATURES:\n",
    "    print(f\"Selecting top {SELECT_TOP_K_FEATURES} features...\")\n",
    "    selected_features = advanced_feature_selection(X_te, y_full, SELECT_TOP_K_FEATURES)\n",
    "    X_te = X_te[selected_features]\n",
    "    if X_test_te is not None:\n",
    "        X_test_te = X_test_te[selected_features]\n",
    "    print(f\"Selected {len(selected_features)} features\")\n",
    "\n",
    "# Enhanced scaling for linear models\n",
    "scaler = QuantileTransformer(output_distribution='normal', random_state=SEED)\n",
    "X_lin = X_te.copy()\n",
    "lin_num_cols = X_lin.select_dtypes(include=[np.number]).columns\n",
    "X_lin[lin_num_cols] = scaler.fit_transform(X_lin[lin_num_cols])\n",
    "\n",
    "if X_test_te is not None:\n",
    "    X_test_lin = X_test_te.copy()\n",
    "    lin_test_num = X_test_lin.select_dtypes(include=[np.number]).columns\n",
    "    X_test_lin[lin_test_num] = scaler.transform(X_test_lin[lin_test_num])\n",
    "else:\n",
    "    X_test_lin = None\n",
    "\n",
    "# Optional target log transform\n",
    "use_log = False\n",
    "if AUTO_LOG_TARGET:\n",
    "    skew_val = pd.Series(y_full).skew()\n",
    "    if skew_val > 1.0:\n",
    "        use_log = True\n",
    "        y_work = np.log1p(y_full)\n",
    "    else:\n",
    "        y_work = y_full.copy()\n",
    "else:\n",
    "    y_work = y_full.copy()\n",
    "\n",
    "# Stratified folds by target bins (for stability)\n",
    "strat_bins = make_target_bins_improved(y_full, n_bins=8)\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "base_models: Dict[str, Tuple[object, str]] = {}\n",
    "\n",
    "# Run Optuna optimization for key models if enabled\n",
    "optimized_params = {}\n",
    "if USE_OPTUNA:\n",
    "    print(\"\\n=== OPTUNA HYPERPARAMETER OPTIMIZATION ===\")\n",
    "    \n",
    "    # List of models to optimize\n",
    "    models_to_optimize = []\n",
    "    if HAS_LGB:\n",
    "        models_to_optimize.append(\"LGBM\")\n",
    "    if HAS_XGB:\n",
    "        models_to_optimize.append(\"XGB\")\n",
    "    if HAS_CAT:\n",
    "        models_to_optimize.append(\"CAT\")\n",
    "    models_to_optimize.append(\"RF\")\n",
    "    \n",
    "    # Run optimization for each model\n",
    "    for model_name in models_to_optimize:\n",
    "        best_params = run_optuna_optimization(X_te, y_work, model_name)\n",
    "        if best_params is not None:\n",
    "            optimized_params[model_name] = best_params\n",
    "    \n",
    "    print(f\"\\n[Optuna] Optimization complete. Optimized {len(optimized_params)} models.\")\n",
    "\n",
    "# Build models with optimized or default parameters\n",
    "if HAS_LGB:\n",
    "    if \"LGBM\" in optimized_params:\n",
    "        lgb_params = optimized_params[\"LGBM\"]\n",
    "        lgb_params.update({\n",
    "            'random_state': SEED,\n",
    "            'verbose': -1,\n",
    "            'n_jobs': 1,\n",
    "            'objective': 'regression',\n",
    "            'boosting_type': 'gbdt'\n",
    "        })\n",
    "    else:\n",
    "        lgb_params = {\n",
    "            'n_estimators': 800,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 6,\n",
    "            'num_leaves': 31,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 1.0,\n",
    "            'min_child_samples': 20,\n",
    "            'random_state': SEED,\n",
    "            'verbose': -1,\n",
    "            'n_jobs': 1,\n",
    "            'objective': 'regression',\n",
    "            'boosting_type': 'gbdt'\n",
    "        }\n",
    "    \n",
    "    base_models[\"LGBM\"] = (LGBMRegressor(**lgb_params), \"tree\")\n",
    "\n",
    "if HAS_XGB:\n",
    "    if \"XGB\" in optimized_params:\n",
    "        xgb_params = optimized_params[\"XGB\"]\n",
    "        xgb_params.update({\n",
    "            'random_state': SEED,\n",
    "            'n_jobs': 1,\n",
    "            'tree_method': 'hist'\n",
    "        })\n",
    "    else:\n",
    "        xgb_params = {\n",
    "            'n_estimators': 1200,\n",
    "            'learning_rate': 0.03,\n",
    "            'max_depth': 8,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.0,\n",
    "            'reg_lambda': 1.0,\n",
    "            'random_state': SEED,\n",
    "            'n_jobs': 1,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "    \n",
    "    base_models[\"XGB\"] = (XGBRegressor(**xgb_params), \"tree\")\n",
    "\n",
    "if HAS_CAT:\n",
    "    if \"CAT\" in optimized_params:\n",
    "        cat_params = optimized_params[\"CAT\"]\n",
    "        cat_params.update({\n",
    "            'random_state': SEED,\n",
    "            'verbose': False,\n",
    "            'loss_function': 'RMSE'\n",
    "        })\n",
    "    else:\n",
    "        cat_params = {\n",
    "            'depth': 8,\n",
    "            'learning_rate': 0.03,\n",
    "            'iterations': 1500,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': SEED,\n",
    "            'loss_function': 'RMSE',\n",
    "            'verbose': False\n",
    "        }\n",
    "    \n",
    "    base_models[\"CAT\"] = (CatBoostRegressor(**cat_params), \"tree\")\n",
    "\n",
    "# Random Forest with optimization\n",
    "if \"RF\" in optimized_params:\n",
    "    rf_params = optimized_params[\"RF\"]\n",
    "    rf_params.update({\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "else:\n",
    "    rf_params = {\n",
    "        'n_estimators': 800,\n",
    "        'max_depth': None,\n",
    "        'max_features': 'sqrt',\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "base_models[\"RF\"] = (RandomForestRegressor(**rf_params), \"tree\")\n",
    "\n",
    "# Strong classical ensembles (not optimized with Optuna for simplicity)\n",
    "base_models[\"ET\"] = (\n",
    "    ExtraTreesRegressor(\n",
    "        n_estimators=800,\n",
    "        max_depth=None,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"GBDT\"] = (\n",
    "    GradientBoostingRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=3,\n",
    "        random_state=SEED,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "# Linear baselines on scaled features (kept for diversity)\n",
    "base_models[\"Ridge\"] = (\n",
    "    Ridge(alpha=5.0, random_state=SEED),\n",
    "    \"linear\",\n",
    ")\n",
    "base_models[\"Lasso\"] = (\n",
    "    Lasso(alpha=0.0005, random_state=SEED, max_iter=20000),\n",
    "    \"linear\",\n",
    ")\n",
    "\n",
    "# -------------- OOF Training / Prediction -------------- #\n",
    "models_oof = {}\n",
    "models_test_pred = {}\n",
    "models_fold_objs = {}\n",
    "fold_scores = []\n",
    "\n",
    "# Feature matrices: tree models use TE (numeric-only)\n",
    "X_tree = X_te\n",
    "X_tree_test = X_test_te\n",
    "\n",
    "X_linear = X_lin\n",
    "X_linear_test = X_test_lin\n",
    "\n",
    "for name, (model, mtype) in base_models.items():\n",
    "    print(f\"\\n[Model={name}] training with {NFOLDS}-fold CV ...\")\n",
    "    oof = np.zeros(len(X_tree))\n",
    "    test_preds_folds = []\n",
    "    fold_importances = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_tree, strat_bins)):\n",
    "        if mtype == \"tree\":\n",
    "            Xtr, Xva = X_tree.iloc[tr_idx], X_tree.iloc[va_idx]\n",
    "        else:\n",
    "            Xtr, Xva = X_linear.iloc[tr_idx], X_linear.iloc[va_idx]\n",
    "\n",
    "        ytr, yva = y_work.iloc[tr_idx], y_work.iloc[va_idx]\n",
    "\n",
    "        # Fresh instance per fold\n",
    "        m = type(model)(**model.get_params())\n",
    "\n",
    "        # Special handling for different model types\n",
    "        if name == \"LGBM\" and HAS_LGB:\n",
    "            # LightGBM specific handling with clean feature names and reset index\n",
    "            Xtr_clean = Xtr.copy().reset_index(drop=True)\n",
    "            Xva_clean = Xva.copy().reset_index(drop=True)\n",
    "            ytr_clean = ytr.reset_index(drop=True)\n",
    "            yva_clean = yva.reset_index(drop=True)\n",
    "            \n",
    "            # Ensure all data is numeric and clean feature names\n",
    "            Xtr_clean = Xtr_clean.select_dtypes(include=[np.number])\n",
    "            Xva_clean = Xva_clean.select_dtypes(include=[np.number])\n",
    "            \n",
    "            # Create simple sequential feature names to avoid any naming conflicts\n",
    "            n_features = len(Xtr_clean.columns)\n",
    "            simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "            \n",
    "            Xtr_clean.columns = simple_names\n",
    "            Xva_clean.columns = simple_names\n",
    "            \n",
    "            try:\n",
    "                # Use minimal parameters to avoid conflicts\n",
    "                lgb_params = {\n",
    "                    'n_estimators': 500,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'max_depth': 6,\n",
    "                    'num_leaves': 31,\n",
    "                    'subsample': 0.8,\n",
    "                    'colsample_bytree': 0.8,\n",
    "                    'random_state': SEED,\n",
    "                    'verbose': -1,\n",
    "                    'n_jobs': 1\n",
    "                }\n",
    "                m = LGBMRegressor(**lgb_params)\n",
    "                \n",
    "                # Fit without eval_set to avoid feature name conflicts\n",
    "                m.fit(Xtr_clean, ytr_clean)\n",
    "                pred_va = m.predict(Xva_clean)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: LGBM fold {fold} failed: {e}\")\n",
    "                pred_va = np.full(len(yva_clean), ytr_clean.mean())  # Fallback prediction\n",
    "                \n",
    "        elif name == \"CAT\" and HAS_CAT:\n",
    "            # CatBoost requires numeric input here (since we dropped original categorical cols).\n",
    "            m.fit(Xtr.astype(np.float32), ytr, eval_set=(Xva.astype(np.float32), yva), verbose=False)\n",
    "            pred_va = m.predict(Xva.astype(np.float32))\n",
    "        else:\n",
    "            m.fit(Xtr, ytr)\n",
    "            pred_va = m.predict(Xva)\n",
    "\n",
    "        oof[va_idx] = pred_va\n",
    "\n",
    "        if X_tree_test is not None:\n",
    "            Xte = X_tree_test if mtype == \"tree\" else X_linear_test\n",
    "            if name == \"LGBM\" and HAS_LGB:\n",
    "                # Apply same feature name cleaning for test predictions\n",
    "                Xte_clean = Xte.copy().reset_index(drop=True)\n",
    "                Xte_clean = Xte_clean.select_dtypes(include=[np.number])\n",
    "                \n",
    "                # Use same simple sequential names\n",
    "                n_features = len(Xte_clean.columns)\n",
    "                simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "                Xte_clean.columns = simple_names\n",
    "                \n",
    "                try:\n",
    "                    test_preds_folds.append(m.predict(Xte_clean))\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: LGBM test prediction failed: {e}\")\n",
    "                    test_preds_folds.append(np.full(len(Xte_clean), oof[oof != 0].mean() if len(oof[oof != 0]) > 0 else 0))\n",
    "            elif name == \"CAT\" and HAS_CAT:\n",
    "                test_preds_folds.append(m.predict(Xte.astype(np.float32)))\n",
    "            else:\n",
    "                test_preds_folds.append(m.predict(Xte))\n",
    "\n",
    "        # Save feature importances when available\n",
    "        if SAVE_FEATURE_IMPORTANCES and hasattr(m, \"feature_importances_\"):\n",
    "            fi = pd.DataFrame({\n",
    "                \"feature\": Xtr.columns,\n",
    "                \"importance\": m.feature_importances_,\n",
    "                \"fold\": fold,\n",
    "                \"model\": name,\n",
    "            })\n",
    "            fold_importances.append(fi)\n",
    "\n",
    "        models_fold_objs.setdefault(name, []).append(m)\n",
    "\n",
    "    # Back-transform if log was used\n",
    "    oof_final = np.expm1(oof) if use_log else oof\n",
    "    rmse_score = rmse(y_full, oof_final)\n",
    "\n",
    "    models_oof[name] = oof_final\n",
    "    if test_preds_folds:\n",
    "        pred_test_mean = np.mean(test_preds_folds, axis=0)\n",
    "        pred_test_final = np.expm1(pred_test_mean) if use_log else pred_test_mean\n",
    "        models_test_pred[name] = pred_test_final\n",
    "\n",
    "    fold_scores.append({\"model\": name, \"cv_rmse\": rmse_score})\n",
    "    print(f\"[Model={name}] CV RMSE: {rmse_score:.5f}\")\n",
    "\n",
    "    if SAVE_FEATURE_IMPORTANCES and len(fold_importances) > 0:\n",
    "        imp_df = pd.concat(fold_importances, ignore_index=True)\n",
    "        imp_df.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).head(50).to_csv(\n",
    "            f\"feature_importances_{name}.csv\", index=True\n",
    "        )\n",
    "\n",
    "# -------------------- Stacking (meta-model) -------------------- #\n",
    "print(\"\\n[Stacking] Training meta-model (GradientBoostingRegressor) on OOF predictions ...\")\n",
    "oof_df = pd.DataFrame(models_oof)\n",
    "\n",
    "meta_model = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=SEED,\n",
    ")\n",
    "meta_model.fit(oof_df, y_full)\n",
    "meta_oof = meta_model.predict(oof_df)\n",
    "stack_rmse = rmse(y_full, meta_oof)\n",
    "print(f\"[Stacking] Meta CV RMSE: {stack_rmse:.5f}\")\n",
    "\n",
    "# Weighted Blend of the best individual models (by CV)\n",
    "sorted_models = sorted(fold_scores, key=lambda d: d[\"cv_rmse\"])[: min(3, len(fold_scores))]\n",
    "weights = []\n",
    "for d in sorted_models:\n",
    "    w = 1.0 / max(d[\"cv_rmse\"], 1e-6)\n",
    "    weights.append(w)\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "print(\"\\n[Blending] Top models & weights:\")\n",
    "for (d, w) in zip(sorted_models, weights):\n",
    "    print(f\"  - {d['model']}: weight={w:.3f}, cv_rmse={d['cv_rmse']:.5f}\")\n",
    "\n",
    "pd.DataFrame({\"id\": train[ID_COL], \"y\": y_full, **models_oof}).to_csv(\"oof_predictions.csv\", index=False)\n",
    "pd.DataFrame(fold_scores).to_csv(\"fold_scores.csv\", index=False)\n",
    "\n",
    "if X_test_full is not None and len(models_test_pred) > 0:\n",
    "    # Stacking test preds\n",
    "    test_stack_mat = pd.DataFrame(models_test_pred)\n",
    "    stack_pred = meta_model.predict(test_stack_mat)\n",
    "\n",
    "    # Weighted blend of top models\n",
    "    blend_pred = np.zeros(len(test_stack_mat))\n",
    "    for (d, w) in zip(sorted_models, weights):\n",
    "        blend_pred += w * test_stack_mat[d[\"model\"]].values\n",
    "\n",
    "    # Final combo: average of stack & blend (often stabilizes)\n",
    "    final_pred = 0.5 * stack_pred + 0.5 * blend_pred\n",
    "\n",
    "    sub = pd.DataFrame({ID_COL: test[ID_COL], TARGET: final_pred})\n",
    "    sub.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"\\nSaved submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb24519",
   "metadata": {},
   "source": [
    "# Another Simpler approach explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, warnings\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge, HuberRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, QuantileTransformer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import xgboost as xgb, lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train = pd.read_csv(\"MiNDAT.csv\")\n",
    "test = pd.read_csv(\"MiNDAT_UNK.csv\")\n",
    "\n",
    "target, id_col = \"CORRUCYSTIC_DENSITY\", \"LOCAL_IDENTIFIER\"\n",
    "train = train.dropna(subset=[target])\n",
    "\n",
    "print(f\"Initial dataset: {train.shape[0]} fragments\")\n",
    "print(f\"Target distribution - Mean: {train[target].mean():.4f}, Std: {train[target].std():.4f}\")\n",
    "print(f\"Target range: [{train[target].min():.4f}, {train[target].max():.4f}]\")\n",
    "\n",
    "# Advanced Outlier Detection - Using multiple methods for alien data\n",
    "def detect_outliers_iqr(data, multiplier=1.5):\n",
    "    Q1, Q3 = data.quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    return (data < Q1 - multiplier*IQR) | (data > Q3 + multiplier*IQR)\n",
    "\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    z_scores = np.abs((data - data.mean()) / data.std())\n",
    "    return z_scores > threshold\n",
    "\n",
    "# More conservative outlier removal for alien biocomputer data\n",
    "outliers_iqr = detect_outliers_iqr(train[target], multiplier=2.0)  # More conservative\n",
    "outliers_zscore = detect_outliers_zscore(train[target], threshold=3.5)\n",
    "extreme_outliers = outliers_iqr & outliers_zscore  # Only remove extreme outliers\n",
    "\n",
    "print(f\"Removing {extreme_outliers.sum()} extreme outliers ({extreme_outliers.sum()/len(train)*100:.2f}%)\")\n",
    "train = train[~extreme_outliers]\n",
    "\n",
    "features = [c for c in train.columns if c not in [target, id_col]]\n",
    "X, y = train[features], train[target]\n",
    "X_test = test[features]\n",
    "\n",
    "print(f\"Feature space: {len(features)} dimensions\")\n",
    "print(f\"Training fragments: {X.shape[0]}\")\n",
    "print(f\"Test fragments: {X_test.shape[0]}\")\n",
    "\n",
    "# Enhanced Categorical Encoding - Important for MINDSPIKE_VERSION and other alien categories\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns\n",
    "print(f\"Categorical features (alien classifications): {len(cat_cols)}\")\n",
    "\n",
    "# Check for high cardinality categories that might need special handling\n",
    "for col in cat_cols:\n",
    "    unique_train = X[col].nunique()\n",
    "    unique_total = pd.concat([X[col], X_test[col]]).nunique()\n",
    "    print(f\"  {col}: {unique_train} train classes, {unique_total} total classes\")\n",
    "\n",
    "# Advanced categorical encoding\n",
    "for col in cat_cols:\n",
    "    # Combine train and test for consistent encoding\n",
    "    combined_data = pd.concat([X[col], X_test[col]]).fillna(\"CORRUPTED_DATA\")\n",
    "    \n",
    "    # For high cardinality features, use target encoding\n",
    "    if combined_data.nunique() > 20:\n",
    "        # Target encoding for high cardinality categories\n",
    "        target_means = train.groupby(col)[target].mean()\n",
    "        global_mean = train[target].mean()\n",
    "        \n",
    "        # Smooth target encoding to prevent overfitting\n",
    "        counts = train.groupby(col).size()\n",
    "        smoothing_factor = 10\n",
    "        smoothed_means = (target_means * counts + global_mean * smoothing_factor) / (counts + smoothing_factor)\n",
    "        \n",
    "        X[col] = X[col].map(smoothed_means).fillna(global_mean)\n",
    "        X_test[col] = X_test[col].map(smoothed_means).fillna(global_mean)\n",
    "    else:\n",
    "        # Standard label encoding for low cardinality\n",
    "        le = LabelEncoder()\n",
    "        le.fit(combined_data)\n",
    "        X[col] = le.transform(X[col].fillna(\"CORRUPTED_DATA\"))\n",
    "        X_test[col] = le.transform(X_test[col].fillna(\"CORRUPTED_DATA\"))\n",
    "\n",
    "# Advanced Numerical Feature Processing for Alien Biocomputer Data\n",
    "num_cols = X.select_dtypes(include=np.number).columns\n",
    "print(f\"Numerical features (sensor readings): {len(num_cols)}\")\n",
    "\n",
    "# Handle infinite values and extreme outliers\n",
    "for col in num_cols:\n",
    "    # Replace infinities with NaN for proper imputation\n",
    "    X[col] = X[col].replace([np.inf, -np.inf], np.nan)\n",
    "    X_test[col] = X_test[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Clip extreme values (beyond 5 standard deviations) - important for alien data\n",
    "    if X[col].notna().sum() > 10:  # Only if we have enough data points\n",
    "        mean_val = X[col].mean()\n",
    "        std_val = X[col].std()\n",
    "        if std_val > 0:\n",
    "            lower_bound = mean_val - 5 * std_val\n",
    "            upper_bound = mean_val + 5 * std_val\n",
    "            X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "            X_test[col] = X_test[col].clip(lower_bound, upper_bound)\n",
    "\n",
    "# Advanced Imputation Strategy\n",
    "print(\"Applying advanced imputation for missing alien data...\")\n",
    "\n",
    "# Use IterativeImputer for better handling of complex patterns\n",
    "imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X[num_cols]),\n",
    "    columns=num_cols,\n",
    "    index=X.index\n",
    ")\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_test[num_cols]),\n",
    "    columns=num_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Replace original numerical columns with imputed ones\n",
    "X[num_cols] = X_imputed\n",
    "X_test[num_cols] = X_test_imputed\n",
    "\n",
    "# Advanced Feature Engineering for Alien Biocomputer Analysis\n",
    "print(\"Engineering features for CORRUCYST fragment analysis...\")\n",
    "\n",
    "# Select most important features for interaction terms\n",
    "feature_importance_selector = SelectKBest(f_regression, k=min(15, len(num_cols)))\n",
    "feature_importance_selector.fit(X[num_cols], y)\n",
    "important_features = num_cols[feature_importance_selector.get_support()]\n",
    "\n",
    "print(f\"Selected {len(important_features)} most important features for engineering\")\n",
    "\n",
    "# 1. Interaction features between critical measurements\n",
    "for i, f1 in enumerate(important_features[:8]):  # Limit to prevent explosion\n",
    "    for j, f2 in enumerate(important_features[i+1:i+4]):  # More selective\n",
    "        # Multiplicative interactions (energy resonance patterns)\n",
    "        X[f'{f1}_x_{f2}'] = X[f1] * X[f2]\n",
    "        X_test[f'{f1}_x_{f2}'] = X_test[f1] * X_test[f2]\n",
    "        \n",
    "        # Ratio features (stability ratios)\n",
    "        X[f'{f1}_div_{f2}'] = X[f1] / (np.abs(X[f2]) + 1e-8)\n",
    "        X_test[f'{f1}_div_{f2}'] = X_test[f1] / (np.abs(X_test[f2]) + 1e-8)\n",
    "\n",
    "# 2. Polynomial transformations for non-linear alien patterns\n",
    "for f in important_features[:6]:\n",
    "    # Quadratic terms for resonance patterns\n",
    "    X[f'{f}_squared'] = X[f] ** 2\n",
    "    X_test[f'{f}_squared'] = X_test[f] ** 2\n",
    "    \n",
    "    # Cube root for stability measurements\n",
    "    X[f'{f}_cbrt'] = np.sign(X[f]) * np.abs(X[f]) ** (1/3)\n",
    "    X_test[f'{f}_cbrt'] = np.sign(X_test[f]) * np.abs(X_test[f]) ** (1/3)\n",
    "    \n",
    "    # Log transformations for exponential relationships\n",
    "    X[f'{f}_log'] = np.log1p(np.abs(X[f]))\n",
    "    X_test[f'{f}_log'] = np.log1p(np.abs(X_test[f]))\n",
    "\n",
    "# 3. Statistical aggregations across feature groups\n",
    "if len(important_features) > 5:\n",
    "    # Group statistics for pattern recognition\n",
    "    X['important_mean'] = X[important_features].mean(axis=1)\n",
    "    X['important_std'] = X[important_features].std(axis=1)\n",
    "    X['important_skew'] = X[important_features].skew(axis=1)\n",
    "    X['important_max'] = X[important_features].max(axis=1)\n",
    "    X['important_min'] = X[important_features].min(axis=1)\n",
    "    \n",
    "    X_test['important_mean'] = X_test[important_features].mean(axis=1)\n",
    "    X_test['important_std'] = X_test[important_features].std(axis=1)\n",
    "    X_test['important_skew'] = X_test[important_features].skew(axis=1)\n",
    "    X_test['important_max'] = X_test[important_features].max(axis=1)\n",
    "    X_test['important_min'] = X_test[important_features].min(axis=1)\n",
    "\n",
    "# 4. Alien-specific patterns (based on domain knowledge)\n",
    "# Energy coherence ratios\n",
    "energy_features = [f for f in X.columns if any(keyword in f.lower() for keyword in ['energy', 'power', 'volt', 'amp'])]\n",
    "if len(energy_features) >= 2:\n",
    "    X['energy_coherence'] = X[energy_features].sum(axis=1) / (X[energy_features].std(axis=1) + 1e-8)\n",
    "    X_test['energy_coherence'] = X_test[energy_features].sum(axis=1) / (X_test[energy_features].std(axis=1) + 1e-8)\n",
    "\n",
    "print(f\"Feature engineering complete. New feature count: {X.shape[1]}\")\n",
    "\n",
    "# Clean feature names for LightGBM / XGBoost compatibility\n",
    "def clean_column(name):\n",
    "    # Replace any non-alphanumeric character with underscore\n",
    "    return re.sub(r'[^A-Za-z0-9_]', '_', str(name))\n",
    "\n",
    "X.columns = [clean_column(c) for c in X.columns]\n",
    "X_test.columns = [clean_column(c) for c in X_test.columns]\n",
    "\n",
    "# Advanced Scaling Strategy for Alien Data\n",
    "print(\"Applying robust scaling for alien biocomputer measurements...\")\n",
    "\n",
    "# Use QuantileTransformer for non-normal distributions (common in alien data)\n",
    "scaler = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Keep original data for tree-based models and scaled for linear models\n",
    "X_original = X.copy()\n",
    "X_test_original = X_test.copy()\n",
    "X = X_scaled\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(f\"Final feature matrix: {X.shape} for training, {X_test.shape} for prediction\")\n",
    "\n",
    "# Enhanced Model Suite for Alien Data Analysis\n",
    "models = {\n",
    "    \"RF\": RandomForestRegressor(n_estimators=500, max_depth=20, random_state=42, n_jobs=-1, \n",
    "                               min_samples_split=3, min_samples_leaf=2),\n",
    "    \"ET\": ExtraTreesRegressor(n_estimators=500, max_depth=20, random_state=42, n_jobs=-1,\n",
    "                             min_samples_split=3, min_samples_leaf=2),\n",
    "    \"GB\": GradientBoostingRegressor(n_estimators=300, learning_rate=0.03, max_depth=8, \n",
    "                                   random_state=42, subsample=0.8),\n",
    "    \"XGB\": xgb.XGBRegressor(n_estimators=500, learning_rate=0.03, max_depth=8, \n",
    "                           random_state=42, subsample=0.8, colsample_bytree=0.8),\n",
    "    \"LGB\": lgb.LGBMRegressor(n_estimators=500, learning_rate=0.03, max_depth=8, \n",
    "                            random_state=42, subsample=0.8, colsample_bytree=0.8, verbose=-1),\n",
    "    \"CatBoost\": CatBoostRegressor(iterations=500, learning_rate=0.03, depth=8, \n",
    "                                 random_seed=42, verbose=False),\n",
    "    \"Ridge\": Ridge(alpha=10.0),\n",
    "    \"Lasso\": Lasso(alpha=1.0, max_iter=2000),\n",
    "    \"ElasticNet\": ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=2000),\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "    \"Huber\": HuberRegressor(epsilon=1.35, max_iter=500),\n",
    "}\n",
    "\n",
    "# Tree-based models use original data, linear models use scaled data\n",
    "tree_models = [\"RF\", \"ET\", \"GB\", \"XGB\", \"LGB\", \"CatBoost\"]\n",
    "linear_models = [\"Ridge\", \"Lasso\", \"ElasticNet\", \"BayesianRidge\", \"Huber\"]\n",
    "\n",
    "# Enhanced Cross-Validation with Multiple Metrics\n",
    "kf = KFold(n_splits=7, shuffle=True, random_state=42)  # More folds for better validation\n",
    "scores, trained_models = {}, {}\n",
    "\n",
    "print(\"\\n=== CORRUCYST Fragment Analysis Results ===\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} for alien biocomputer analysis...\")\n",
    "    \n",
    "    # Choose appropriate data\n",
    "    X_train = X_original if name in tree_models else X\n",
    "    X_test_model = X_test_original if name in tree_models else X_test\n",
    "    \n",
    "    rmses, maes, r2s, folds = [], [], [], []\n",
    "    for tr, val in kf.split(X_train):\n",
    "        m = type(model)(**model.get_params())\n",
    "        m.fit(X_train.iloc[tr], y.iloc[tr])\n",
    "        p = m.predict(X_train.iloc[val])\n",
    "        \n",
    "        rmses.append(mean_squared_error(y.iloc[val], p, squared=False))\n",
    "        maes.append(mean_absolute_error(y.iloc[val], p))\n",
    "        r2s.append(r2_score(y.iloc[val], p))\n",
    "        folds.append(m)\n",
    "    \n",
    "    scores[name] = {\n",
    "        \"CV_RMSE\": np.mean(rmses), \"CV_RMSE_STD\": np.std(rmses),\n",
    "        \"CV_MAE\": np.mean(maes), \"CV_MAE_STD\": np.std(maes),\n",
    "        \"CV_R2\": np.mean(r2s), \"CV_R2_STD\": np.std(r2s)\n",
    "    }\n",
    "    trained_models[name] = folds\n",
    "    \n",
    "    print(f\"{name:12} - RMSE: {np.mean(rmses):.4f} (±{np.std(rmses):.4f}) | \"\n",
    "          f\"MAE: {np.mean(maes):.4f} (±{np.std(maes):.4f}) | \"\n",
    "          f\"R²: {np.mean(r2s):.4f} (±{np.std(r2s):.4f})\")\n",
    "\n",
    "best_model_name = min(scores, key=lambda x: scores[x][\"CV_RMSE\"])\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "\n",
    "# Advanced Hyperparameter Tuning for Best Model\n",
    "print(f\"\\nPerforming advanced hyperparameter tuning for {best_model_name}...\")\n",
    "\n",
    "hyperparameter_grids = {\n",
    "    \"RF\": {\n",
    "        \"n_estimators\": [300, 500, 700],\n",
    "        \"max_depth\": [15, 20, 25],\n",
    "        \"min_samples_split\": [2, 3, 5],\n",
    "        \"min_samples_leaf\": [1, 2, 3]\n",
    "    },\n",
    "    \"ET\": {\n",
    "        \"n_estimators\": [300, 500, 700],\n",
    "        \"max_depth\": [15, 20, 25],\n",
    "        \"min_samples_split\": [2, 3, 5],\n",
    "        \"min_samples_leaf\": [1, 2, 3]\n",
    "    },\n",
    "    \"GB\": {\n",
    "        \"n_estimators\": [200, 300, 400],\n",
    "        \"learning_rate\": [0.02, 0.03, 0.05],\n",
    "        \"max_depth\": [6, 8, 10],\n",
    "        \"subsample\": [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"n_estimators\": [300, 500, 700],\n",
    "        \"learning_rate\": [0.02, 0.03, 0.05],\n",
    "        \"max_depth\": [6, 8, 10],\n",
    "        \"subsample\": [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \"LGB\": {\n",
    "        \"n_estimators\": [300, 500, 700],\n",
    "        \"learning_rate\": [0.02, 0.03, 0.05],\n",
    "        \"max_depth\": [6, 8, 10],\n",
    "        \"subsample\": [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"iterations\": [300, 500, 700],\n",
    "        \"learning_rate\": [0.02, 0.03, 0.05],\n",
    "        \"depth\": [6, 8, 10]\n",
    "    },\n",
    "    \"Ridge\": {\"alpha\": [0.1, 1, 10, 100, 1000]},\n",
    "    \"Lasso\": {\"alpha\": [0.001, 0.01, 0.1, 1, 10]},\n",
    "    \"ElasticNet\": {\"alpha\": [0.1, 1, 10], \"l1_ratio\": [0.1, 0.5, 0.7, 0.9]},\n",
    "    \"BayesianRidge\": {\"alpha_1\": [1e-6, 1e-5, 1e-4], \"alpha_2\": [1e-6, 1e-5, 1e-4]},\n",
    "    \"Huber\": {\"epsilon\": [1.1, 1.35, 1.5, 2.0], \"alpha\": [0.0001, 0.001, 0.01]}\n",
    "}\n",
    "\n",
    "tuned_model = None\n",
    "if best_model_name in hyperparameter_grids:\n",
    "    base_model = models[best_model_name]\n",
    "    X_tune = X_original if best_model_name in tree_models else X\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        base_model, \n",
    "        hyperparameter_grids[best_model_name], \n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_squared_error\", \n",
    "        n_jobs=-1, \n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(X_tune, y)\n",
    "    tuned_model = grid.best_estimator_\n",
    "    print(f\"Best hyperparameters: {grid.best_params_}\")\n",
    "    print(f\"Tuned model CV score: {-grid.best_score_:.4f}\")\n",
    "\n",
    "# Advanced Ensemble Strategy\n",
    "print(\"\\n=== Creating Advanced Ensemble for CORRUCYST Prediction ===\")\n",
    "\n",
    "# 1. Top performing models weighted ensemble\n",
    "top_models = sorted(scores.items(), key=lambda x: x[1][\"CV_RMSE\"])[:5]\n",
    "print(\"Top 5 models for ensemble:\")\n",
    "for name, score in top_models:\n",
    "    print(f\"  {name}: RMSE {score['CV_RMSE']:.4f}\")\n",
    "\n",
    "# Weight by inverse RMSE with exponential scaling\n",
    "ensemble_weights = {}\n",
    "total_weight = 0\n",
    "for name, score in top_models:\n",
    "    weight = np.exp(-score[\"CV_RMSE\"] * 2)  # Exponential weighting\n",
    "    ensemble_weights[name] = weight\n",
    "    total_weight += weight\n",
    "\n",
    "# Normalize weights\n",
    "for name in ensemble_weights:\n",
    "    ensemble_weights[name] /= total_weight\n",
    "    print(f\"  {name} weight: {ensemble_weights[name]:.3f}\")\n",
    "\n",
    "# Create ensemble predictions\n",
    "ensemble_preds = np.zeros(X_test.shape[0])\n",
    "for name, weight in ensemble_weights.items():\n",
    "    X_pred = X_test_original if name in tree_models else X_test\n",
    "    fold_preds = np.mean([m.predict(X_pred) for m in trained_models[name]], axis=0)\n",
    "    ensemble_preds += weight * fold_preds\n",
    "\n",
    "# 2. Stacking ensemble (meta-learner)\n",
    "print(\"\\nCreating stacking ensemble...\")\n",
    "meta_features = np.zeros((X.shape[0], len(top_models)))\n",
    "meta_test_features = np.zeros((X_test.shape[0], len(top_models)))\n",
    "\n",
    "# Generate meta-features using cross-validation\n",
    "for fold_idx, (tr, val) in enumerate(kf.split(X)):\n",
    "    for model_idx, (name, _) in enumerate(top_models):\n",
    "        X_fold = X_original if name in tree_models else X\n",
    "        X_test_fold = X_test_original if name in tree_models else X_test\n",
    "        \n",
    "        # Train on fold and predict validation\n",
    "        model = trained_models[name][fold_idx]\n",
    "        meta_features[val, model_idx] = model.predict(X_fold.iloc[val])\n",
    "\n",
    "# Generate test meta-features\n",
    "for model_idx, (name, _) in enumerate(top_models):\n",
    "    X_pred = X_test_original if name in tree_models else X_test\n",
    "    meta_test_features[:, model_idx] = np.mean(\n",
    "        [m.predict(X_pred) for m in trained_models[name]], axis=0\n",
    "    )\n",
    "\n",
    "# Train meta-learner (simple Ridge regression)\n",
    "meta_learner = Ridge(alpha=1.0)\n",
    "meta_learner.fit(meta_features, y)\n",
    "stacking_preds = meta_learner.predict(meta_test_features)\n",
    "\n",
    "# Final predictions - ensemble of ensemble and stacking\n",
    "if tuned_model is not None:\n",
    "    X_final = X_original if best_model_name in tree_models else X_test\n",
    "    tuned_preds = tuned_model.predict(X_final)\n",
    "    # Weighted combination: 40% ensemble, 30% stacking, 30% tuned best model\n",
    "    final_preds = 0.4 * ensemble_preds + 0.3 * stacking_preds + 0.3 * tuned_preds\n",
    "else:\n",
    "    # Weighted combination: 60% ensemble, 40% stacking\n",
    "    final_preds = 0.6 * ensemble_preds + 0.4 * stacking_preds\n",
    "\n",
    "# Generate submission\n",
    "submission = pd.DataFrame({\n",
    "    id_col: test[id_col], \n",
    "    target: final_preds\n",
    "})\n",
    "submission.to_csv(\"enhanced_corrucyst_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295eae04",
   "metadata": {},
   "source": [
    "# Approach where i treid featuer engineering wiht optuna optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53991fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "NFOLDS = 10  # Increased for better stability\n",
    "NREPEATS = 3  # More repeats for robust estimates\n",
    "TARGET = \"CORRUCYSTIC_DENSITY\"\n",
    "ID_COL = \"LOCAL_IDENTIFIER\"\n",
    "AUTO_LOG_TARGET = True\n",
    "SMOOTHING_M = 50.0  # Higher smoothing for more conservative TE\n",
    "USE_FEATURE_SELECTION = True\n",
    "SELECT_TOP_K_FEATURES = 150  # More features for complex models\n",
    "SAVE_FEATURE_IMPORTANCES = True\n",
    "\n",
    "# Advanced Tuning Configuration\n",
    "USE_PSEUDO_LABELING = True\n",
    "PSEUDO_LABEL_THRESHOLD = 0.95  # Confidence threshold for pseudo-labels\n",
    "USE_ADVANCED_FEATURES = True\n",
    "USE_INTERACTION_FEATURES = True\n",
    "USE_NOISE_INJECTION = True\n",
    "NOISE_LEVEL = 0.01  # Small noise for regularization\n",
    "USE_MULTI_LEVEL_STACKING = True\n",
    "USE_ADAPTIVE_WEIGHTS = True\n",
    "\n",
    "# Optuna Configuration (reduced for faster tuning)\n",
    "USE_OPTUNA = True\n",
    "OPTUNA_N_TRIALS = 30  \n",
    "OPTUNA_TIMEOUT = 200\n",
    "OPTUNA_N_JOBS = 1\n",
    "OPTUNA_CV_FOLDS = 3\n",
    "OUTLIER_METHOD = \"isolation_forest\"\n",
    "OUTLIER_CONTAMINATION = 0.05 \n",
    "\n",
    "\n",
    "# -------------------- Enhanced Utils -------------------- #\n",
    "def seed_everything(seed: int = SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_first_existing(paths: List[str]) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean column names more thoroughly.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(c)).strip(\"_\") for c in df.columns]\n",
    "    # Remove consecutive underscores\n",
    "    df.columns = [re.sub(r\"_+\", \"_\", c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def advanced_downcast(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"More aggressive memory optimization.\"\"\"\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        col = df[c]\n",
    "        c_min, c_max = col.min(), col.max()\n",
    "        \n",
    "        if pd.api.types.is_integer_dtype(col):\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[c] = col.astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[c] = col.astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[c] = col.astype(np.int32)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[c] = col.astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def detect_outliers_advanced(df: pd.DataFrame, target_col: str, method: str = \"isolation_forest\", contamination: float = 0.05) -> pd.Series:\n",
    "    \"\"\"Advanced outlier detection using multiple methods.\"\"\"\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    \n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    \n",
    "    if method == \"iqr\":\n",
    "        Q1, Q3 = df[target_col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lb, ub = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        mask = (df[target_col] >= lb) & (df[target_col] <= ub)\n",
    "    \n",
    "    elif method == \"zscore\":\n",
    "        z_scores = np.abs(stats.zscore(df[target_col]))\n",
    "        mask = z_scores < 3\n",
    "    \n",
    "    elif method == \"isolation_forest\":\n",
    "        iso_forest = IsolationForest(contamination=contamination, random_state=SEED)\n",
    "        outliers = iso_forest.fit_predict(df[[target_col]])\n",
    "        mask = outliers == 1\n",
    "    \n",
    "    elif method == \"local_outlier\":\n",
    "        lof = LocalOutlierFactor(contamination=contamination)\n",
    "        outliers = lof.fit_predict(df[[target_col]])\n",
    "        mask = outliers == 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def make_target_bins_improved(y: pd.Series, n_bins: int = 10) -> pd.Series:\n",
    "    \"\"\"Improved target binning with better handling of edge cases.\"\"\"\n",
    "    # Try quantile-based binning first\n",
    "    try:\n",
    "        bins = pd.qcut(y, q=n_bins, labels=False, duplicates=\"drop\")\n",
    "        if bins.nunique() < n_bins // 2:  # If too few unique bins\n",
    "            raise ValueError(\"Too few unique bins\")\n",
    "        return bins.astype(int)\n",
    "    except:\n",
    "        # Fallback to equal-width binning\n",
    "        bins = pd.cut(y, bins=n_bins, labels=False, include_lowest=True)\n",
    "        return bins.astype(int)\n",
    "\n",
    "def enhanced_target_encode(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    n_splits: int = NFOLDS,\n",
    "    n_repeats: int = NREPEATS,\n",
    "    smoothing_m: float = SMOOTHING_M,\n",
    "    seed: int = SEED,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Enhanced target encoding with repeated CV and noise addition for robustness.\"\"\"\n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    global_mean = y.mean()\n",
    "    \n",
    "    # Use repeated K-fold for more stable encoding\n",
    "    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        oof_list = []\n",
    "        test_encoded_list = []\n",
    "        \n",
    "        for tr_idx, val_idx in rkf.split(X):\n",
    "            tr_y = y.iloc[tr_idx]\n",
    "            tr_col = X.iloc[tr_idx][col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            val_col = X.iloc[val_idx][col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            \n",
    "            # Calculate statistics with smoothing\n",
    "            stats_df = tr_y.groupby(tr_col).agg(['sum', 'count', 'mean', 'std']).reset_index()\n",
    "            stats_df.columns = [col, 'sum', 'count', 'mean', 'std']\n",
    "            stats_df['enc'] = (stats_df['sum'] + smoothing_m * global_mean) / (stats_df['count'] + smoothing_m)\n",
    "            \n",
    "            # Add uncertainty-based noise\n",
    "            stats_df['uncertainty'] = 1 / np.sqrt(stats_df['count'] + 1)\n",
    "            noise_scale = stats_df['uncertainty'] * stats_df['std'].fillna(y.std())\n",
    "            stats_df['enc'] += np.random.normal(0, noise_scale * 0.1, len(stats_df))\n",
    "            \n",
    "            # Create mapping dict\n",
    "            encode_map = dict(zip(stats_df[col], stats_df['enc']))\n",
    "            \n",
    "            # Encode validation set\n",
    "            oof_encoded = val_col.map(encode_map).fillna(global_mean)\n",
    "            oof_list.append(pd.Series(oof_encoded.values, index=val_col.index))\n",
    "            \n",
    "            # Encode test set\n",
    "            test_col = X_test[col].astype(\"object\").fillna(\"__MISSING__\")\n",
    "            test_encoded = test_col.map(encode_map).fillna(global_mean)\n",
    "            test_encoded_list.append(test_encoded.values)\n",
    "        \n",
    "        # Combine OOF predictions\n",
    "        oof_combined = pd.concat(oof_list).groupby(level=0).mean()\n",
    "        X[col + \"__te\"] = oof_combined.reindex(X.index).astype(np.float32)\n",
    "        \n",
    "        # Average test predictions\n",
    "        X_test[col + \"__te\"] = np.mean(test_encoded_list, axis=0).astype(np.float32)\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "def create_advanced_features(df: pd.DataFrame, num_cols: List[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    if len(num_cols) == 0:\n",
    "        return df\n",
    "    \n",
    "    # Ensure we have numeric data\n",
    "    num_data = df[num_cols].select_dtypes(include=[np.number])\n",
    "    if num_data.empty:\n",
    "        return df\n",
    "    \n",
    "    # Basic statistical features\n",
    "    df[\"corr_density_mean\"] = num_data.mean(axis=1)\n",
    "    df[\"corr_density_std\"] = num_data.std(axis=1)\n",
    "    df[\"corr_density_skew\"] = num_data.skew(axis=1)\n",
    "    df[\"corr_density_kurt\"] = num_data.kurtosis(axis=1)\n",
    "    df[\"corr_density_range\"] = num_data.max(axis=1) - num_data.min(axis=1)\n",
    "    \n",
    "    # Percentile features\n",
    "    df[\"corr_density_q25\"] = num_data.quantile(0.25, axis=1)\n",
    "    df[\"corr_density_q75\"] = num_data.quantile(0.75, axis=1)\n",
    "    df[\"corr_density_iqr\"] = df[\"corr_density_q75\"] - df[\"corr_density_q25\"]\n",
    "    \n",
    "    # Count-based features\n",
    "    df[\"corr_positive_count\"] = (num_data > 0).sum(axis=1)\n",
    "    df[\"corr_negative_count\"] = (num_data < 0).sum(axis=1)\n",
    "    df[\"corr_zero_count\"] = (num_data == 0).sum(axis=1)\n",
    "    df[\"corr_missing_count\"] = num_data.isnull().sum(axis=1)\n",
    "    \n",
    "    # Advanced statistical measures\n",
    "    df[\"corr_cv\"] = df[\"corr_density_std\"] / (np.abs(df[\"corr_density_mean\"]) + 1e-8)\n",
    "    df[\"corr_mad\"] = num_data.apply(manual_mad, axis=1)  # Using manual MAD implementation\n",
    "    \n",
    "    # Energy and signal processing inspired features (domain-specific)\n",
    "    df[\"corr_energy\"] = (num_data ** 2).sum(axis=1)\n",
    "    df[\"corr_rms\"] = np.sqrt(df[\"corr_energy\"] / len(num_cols))\n",
    "    \n",
    "    # Handle infinite and NaN values\n",
    "    inf_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[inf_cols] = df[inf_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def manual_mad(series):\n",
    "    median = series.median()\n",
    "    return np.median(np.abs(series - median))\n",
    "\n",
    "def create_interaction_features(df: pd.DataFrame, num_cols: List[str], max_interactions: int = 20) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    if len(num_cols) < 2:\n",
    "        return df\n",
    "    \n",
    "    # Select top correlated features with target for interactions\n",
    "    # For now, use first few numerical features\n",
    "    top_cols = num_cols[:min(6, len(num_cols))]\n",
    "    \n",
    "    interaction_count = 0\n",
    "    for i, col1 in enumerate(top_cols):\n",
    "        for j, col2 in enumerate(top_cols[i+1:], i+1):\n",
    "            if interaction_count >= max_interactions:\n",
    "                break\n",
    "            \n",
    "            # Create interaction features\n",
    "            df[f\"{col1}_x_{col2}\"] = df[col1] * df[col2]\n",
    "            df[f\"{col1}_div_{col2}\"] = df[col1] / (df[col2] + 1e-8)\n",
    "            df[f\"{col1}_add_{col2}\"] = df[col1] + df[col2]\n",
    "            df[f\"{col1}_sub_{col2}\"] = df[col1] - df[col2]\n",
    "            \n",
    "            interaction_count += 4\n",
    "            \n",
    "        if interaction_count >= max_interactions:\n",
    "            break\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_polynomial_features(df: pd.DataFrame, num_cols: List[str], degree: int = 2, max_features: int = 10) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    if len(num_cols) == 0:\n",
    "        return df\n",
    "    \n",
    "    # Select top features for polynomial expansion\n",
    "    top_cols = num_cols[:min(max_features, len(num_cols))]\n",
    "    \n",
    "    for col in top_cols:\n",
    "        if degree >= 2:\n",
    "            df[f\"{col}_squared\"] = df[col] ** 2\n",
    "        if degree >= 3:\n",
    "            df[f\"{col}_cubed\"] = df[col] ** 3\n",
    "        \n",
    "        # Log and sqrt transformations\n",
    "        df[f\"{col}_log\"] = np.log1p(np.abs(df[col]))\n",
    "        df[f\"{col}_sqrt\"] = np.sqrt(np.abs(df[col]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_binning_features(df: pd.DataFrame, num_cols: List[str], n_bins: int = 5) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in num_cols[:10]:  # Limit to top 10 features\n",
    "        try:\n",
    "            # Equal frequency binning\n",
    "            df[f\"{col}_bin_freq\"] = pd.qcut(df[col], q=n_bins, labels=False, duplicates='drop')\n",
    "            # Equal width binning\n",
    "            df[f\"{col}_bin_width\"] = pd.cut(df[col], bins=n_bins, labels=False)\n",
    "        except:\n",
    "            continue  # Skip if binning fails\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_noise_features(df: pd.DataFrame, noise_level: float = 0.01, n_features: int = 5) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        df[f\"noise_{i}\"] = np.random.normal(0, noise_level, len(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_cluster_features(df: pd.DataFrame, num_cols: List[str], n_clusters: int = 8) -> pd.DataFrame:\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    if len(num_cols) < 2:\n",
    "        return df\n",
    "    \n",
    "    # Use top numerical features for clustering\n",
    "    cluster_data = df[num_cols[:10]].fillna(0)\n",
    "    \n",
    "    if cluster_data.shape[1] >= 2:\n",
    "        scaler = StandardScaler()\n",
    "        cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
    "        df['cluster_id'] = kmeans.fit_predict(cluster_data_scaled)\n",
    "        \n",
    "        # Distance to cluster centers\n",
    "        distances = kmeans.transform(cluster_data_scaled)\n",
    "        df['min_cluster_dist'] = distances.min(axis=1)\n",
    "        df['mean_cluster_dist'] = distances.mean(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_pseudo_labels(X_test: pd.DataFrame, models_oof: Dict, models_test_pred: Dict, threshold: float = 0.95) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Create pseudo-labels from high-confidence predictions.\"\"\"\n",
    "    if len(models_test_pred) == 0:\n",
    "        return pd.DataFrame(), pd.Series(dtype=float)\n",
    "    \n",
    "    # Calculate ensemble prediction and confidence\n",
    "    predictions = np.array(list(models_test_pred.values())).T\n",
    "    ensemble_pred = np.mean(predictions, axis=1)\n",
    "    prediction_std = np.std(predictions, axis=1)\n",
    "    \n",
    "    # Use coefficient of variation as confidence measure\n",
    "    confidence = 1 / (1 + prediction_std / (np.abs(ensemble_pred) + 1e-8))\n",
    "    \n",
    "    # Select high-confidence predictions\n",
    "    high_conf_mask = confidence >= threshold\n",
    "    \n",
    "    if high_conf_mask.sum() > 0:\n",
    "        pseudo_X = X_test[high_conf_mask].copy()\n",
    "        pseudo_y = pd.Series(ensemble_pred[high_conf_mask], index=pseudo_X.index)\n",
    "        return pseudo_X, pseudo_y\n",
    "    \n",
    "    return pd.DataFrame(), pd.Series(dtype=float)\n",
    "\n",
    "def calculate_adaptive_weights(fold_scores: List[Dict], diversity_factor: float = 0.1) -> Dict[str, float]:\n",
    "    models = list(fold_scores[0].keys())\n",
    "    if 'model' in models:\n",
    "        models.remove('model')\n",
    "    \n",
    "    # Calculate performance weights (inverse of RMSE)\n",
    "    perf_weights = {}\n",
    "    for model in models:\n",
    "        avg_score = np.mean([score.get(model, float('inf')) for score in fold_scores if model in score])\n",
    "        perf_weights[model] = 1.0 / max(avg_score, 1e-6)\n",
    "    \n",
    "    # Normalize performance weights\n",
    "    total_perf = sum(perf_weights.values())\n",
    "    perf_weights = {k: v/total_perf for k, v in perf_weights.items()}\n",
    "    \n",
    "    # Add diversity bonus (simplified - could use correlation-based diversity)\n",
    "    final_weights = {}\n",
    "    for model in models:\n",
    "        diversity_bonus = diversity_factor * np.random.uniform(0.8, 1.2)  # Random diversity factor\n",
    "        final_weights[model] = perf_weights[model] * (1 + diversity_bonus)\n",
    "    \n",
    "    # Normalize final weights\n",
    "    total_final = sum(final_weights.values())\n",
    "    final_weights = {k: v/total_final for k, v in final_weights.items()}\n",
    "    \n",
    "    return final_weights\n",
    "\n",
    "def create_meta_features(X: pd.DataFrame, models_oof: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Create meta-features from base model predictions.\"\"\"\n",
    "    meta_df = X.copy()\n",
    "    \n",
    "    # Add OOF predictions as features\n",
    "    for model_name, oof_pred in models_oof.items():\n",
    "        meta_df[f\"oof_{model_name}\"] = oof_pred\n",
    "    \n",
    "    # Add ensemble statistics\n",
    "    if len(models_oof) > 1:\n",
    "        oof_matrix = np.array(list(models_oof.values())).T\n",
    "        meta_df[\"oof_mean\"] = np.mean(oof_matrix, axis=1)\n",
    "        meta_df[\"oof_std\"] = np.std(oof_matrix, axis=1)\n",
    "        meta_df[\"oof_min\"] = np.min(oof_matrix, axis=1)\n",
    "        meta_df[\"oof_max\"] = np.max(oof_matrix, axis=1)\n",
    "        meta_df[\"oof_range\"] = meta_df[\"oof_max\"] - meta_df[\"oof_min\"]\n",
    "    \n",
    "    return meta_df\n",
    "\n",
    "def advanced_feature_selection(X: pd.DataFrame, y: pd.Series, k: int = 100) -> List[str]:\n",
    "    \"\"\"Advanced feature selection using multiple methods.\"\"\"\n",
    "    if len(X.columns) <= k:\n",
    "        return X.columns.tolist()\n",
    "    \n",
    "    # Remove features with too many missing values or zero variance\n",
    "    valid_features = []\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() / len(X) < 0.8 and X[col].nunique() > 1:\n",
    "            valid_features.append(col)\n",
    "    \n",
    "    X_valid = X[valid_features].fillna(X[valid_features].median())\n",
    "    \n",
    "    # Combine multiple selection methods\n",
    "    scores = {}\n",
    "    \n",
    "    # F-regression scores\n",
    "    try:\n",
    "        f_selector = SelectKBest(score_func=f_regression, k='all')\n",
    "        f_selector.fit(X_valid, y)\n",
    "        f_scores = dict(zip(valid_features, f_selector.scores_))\n",
    "        scores['f_regression'] = f_scores\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Mutual information scores\n",
    "    try:\n",
    "        mi_scores = mutual_info_regression(X_valid, y, random_state=SEED)\n",
    "        mi_scores_dict = dict(zip(valid_features, mi_scores))\n",
    "        scores['mutual_info'] = mi_scores_dict\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Combine scores using rank aggregation\n",
    "    if scores:\n",
    "        feature_ranks = {}\n",
    "        for feature in valid_features:\n",
    "            ranks = []\n",
    "            for method, score_dict in scores.items():\n",
    "                if feature in score_dict:\n",
    "                    # Convert to rank (lower rank = better)\n",
    "                    sorted_features = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "                    rank = next(i for i, (f, _) in enumerate(sorted_features) if f == feature)\n",
    "                    ranks.append(rank)\n",
    "            feature_ranks[feature] = np.mean(ranks) if ranks else len(valid_features)\n",
    "        \n",
    "        # Select top k features\n",
    "        selected = sorted(feature_ranks.items(), key=lambda x: x[1])[:k]\n",
    "        return [f[0] for f in selected]\n",
    "    \n",
    "    return valid_features[:k]\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# -------------------- Optuna Hyperparameter Optimization -------------------- #\n",
    "def optimize_lgbm(X, y, cv_folds):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "            'random_state': SEED,\n",
    "            'verbose': -1,\n",
    "            'n_jobs': 1\n",
    "        }\n",
    "        \n",
    "        scores = []\n",
    "        for train_idx, val_idx in cv_folds.split(X, make_target_bins_improved(y, n_bins=8)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            # Clean feature names for LightGBM\n",
    "            X_train_clean = X_train.select_dtypes(include=[np.number]).reset_index(drop=True)\n",
    "            X_val_clean = X_val.select_dtypes(include=[np.number]).reset_index(drop=True)\n",
    "            n_features = len(X_train_clean.columns)\n",
    "            simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "            X_train_clean.columns = simple_names\n",
    "            X_val_clean.columns = simple_names\n",
    "            \n",
    "            try:\n",
    "                model = LGBMRegressor(**params)\n",
    "                model.fit(X_train_clean, y_train.reset_index(drop=True))\n",
    "                pred = model.predict(X_val_clean)\n",
    "                scores.append(rmse(y_val.reset_index(drop=True), pred))\n",
    "            except Exception as e:\n",
    "                return float('inf')  # Return worst score if error\n",
    "                \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def optimize_xgb(X, y, cv_folds):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1500, step=50),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "            'random_state': SEED,\n",
    "            'n_jobs': 1,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "        \n",
    "        scores = []\n",
    "        for train_idx, val_idx in cv_folds.split(X, make_target_bins_improved(y, n_bins=8)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            try:\n",
    "                model = XGBRegressor(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                pred = model.predict(X_val)\n",
    "                scores.append(rmse(y_val, pred))\n",
    "            except Exception as e:\n",
    "                return float('inf')\n",
    "                \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def optimize_catboost(X, y, cv_folds):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1500, step=50),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'depth': trial.suggest_int('depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'random_state': SEED,\n",
    "            'verbose': False,\n",
    "            'loss_function': 'RMSE'\n",
    "        }\n",
    "        \n",
    "        scores = []\n",
    "        for train_idx, val_idx in cv_folds.split(X, make_target_bins_improved(y, n_bins=8)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            try:\n",
    "                model = CatBoostRegressor(**params)\n",
    "                model.fit(X_train.astype(np.float32), y_train)\n",
    "                pred = model.predict(X_val.astype(np.float32))\n",
    "                scores.append(rmse(y_val, pred))\n",
    "            except Exception as e:\n",
    "                return float('inf')\n",
    "                \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def optimize_random_forest(X, y, cv_folds):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'random_state': SEED,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        scores = []\n",
    "        for train_idx, val_idx in cv_folds.split(X, make_target_bins_improved(y, n_bins=8)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            try:\n",
    "                model = RandomForestRegressor(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                pred = model.predict(X_val)\n",
    "                scores.append(rmse(y_val, pred))\n",
    "            except Exception as e:\n",
    "                return float('inf')\n",
    "                \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def run_optuna_optimization(X, y, model_name):\n",
    "    print(f\"\\n[Optuna] Optimizing {model_name} hyperparameters...\")\n",
    "    \n",
    "    # Create CV folds for optimization\n",
    "    strat_bins = make_target_bins_improved(y, n_bins=8)\n",
    "    cv_folds = StratifiedKFold(n_splits=OPTUNA_CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Select objective function based on model\n",
    "    if model_name == \"LGBM\" and HAS_LGB:\n",
    "        objective_func = optimize_lgbm(X, y, cv_folds)\n",
    "    elif model_name == \"XGB\" and HAS_XGB:\n",
    "        objective_func = optimize_xgb(X, y, cv_folds)\n",
    "    elif model_name == \"CAT\" and HAS_CAT:\n",
    "        objective_func = optimize_catboost(X, y, cv_folds)\n",
    "    elif model_name == \"RF\":\n",
    "        objective_func = optimize_random_forest(X, y, cv_folds)\n",
    "    else:\n",
    "        return None  # Skip optimization for this model\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=TPESampler(seed=SEED),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    # Optimize\n",
    "    study.optimize(\n",
    "        objective_func,\n",
    "        n_trials=OPTUNA_N_TRIALS,\n",
    "        timeout=OPTUNA_TIMEOUT,\n",
    "        n_jobs=OPTUNA_N_JOBS,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(f\"[Optuna] {model_name} - Best score: {study.best_value:.5f}\")\n",
    "    print(f\"[Optuna] {model_name} - Best params: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# -------------------- Load -------------------- #\n",
    "seed_everything(SEED)\n",
    "\n",
    "train_path = \"./MiNDAT.csv\"\n",
    "test_path = \"./MiNDAT_UNK.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train = clean_columns(train)\n",
    "\n",
    "test = pd.read_csv(test_path)\n",
    "test = clean_columns(test)\n",
    "\n",
    "# Basic hygiene\n",
    "if TARGET not in train.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET}' not in training data.\")\n",
    "\n",
    "# Drop rows with missing target\n",
    "train = train.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "# Remove outliers via IQR (robust)\n",
    "Q1, Q3 = train[TARGET].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lb, ub = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "train = train[(train[TARGET] >= lb) & (train[TARGET] <= ub)].reset_index(drop=True)\n",
    "\n",
    "# Identify columns (keep features simple: no extra row stats or poly features)\n",
    "features = [c for c in train.columns if c not in [TARGET, ID_COL]]\n",
    "X_full = train[features].copy()\n",
    "y_full = train[TARGET].copy()\n",
    "\n",
    "if test is not None:\n",
    "    X_test_full = test[features].copy()\n",
    "else:\n",
    "    X_test_full = None\n",
    "\n",
    "# Replace infs\n",
    "for df in [X_full] + ([X_test_full] if X_test_full is not None else []):\n",
    "    if df is None:\n",
    "        continue\n",
    "    numc = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numc] = df[numc].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Advanced memory optimization\n",
    "X_full = advanced_downcast(X_full)\n",
    "if X_test_full is not None:\n",
    "    X_test_full = advanced_downcast(X_test_full)\n",
    "\n",
    "# Split types\n",
    "cat_cols = X_full.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Simple imputers (per column)\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "X_full[num_cols] = num_imputer.fit_transform(X_full[num_cols])\n",
    "if X_test_full is not None:\n",
    "    X_test_full[num_cols] = num_imputer.transform(X_test_full[num_cols])\n",
    "\n",
    "X_full[cat_cols] = cat_imputer.fit_transform(X_full[cat_cols])\n",
    "if X_test_full is not None:\n",
    "    X_test_full[cat_cols] = cat_imputer.transform(X_test_full[cat_cols])\n",
    "\n",
    "# Cast categoricals to 'category' dtype (memory);\n",
    "for c in cat_cols:\n",
    "    X_full[c] = X_full[c].astype(\"category\")\n",
    "    if X_test_full is not None:\n",
    "        X_test_full[c] = X_test_full[c].astype(\"category\")\n",
    "\n",
    "# -------------------- Target Encoding Only (No Feature Engineering) -------------------- #\n",
    "# KFold Target Encoding (leakage-safe)\n",
    "if len(cat_cols) > 0:\n",
    "    print(\"Performing enhanced target encoding...\")\n",
    "    X_te, X_test_te = enhanced_target_encode(\n",
    "        X_full,\n",
    "        y_full,\n",
    "        X_test_full if X_test_full is not None else X_full.iloc[:0],\n",
    "        cat_cols,\n",
    "    )\n",
    "else:\n",
    "    X_te, X_test_te = X_full.copy(), (X_test_full.copy() if X_test_full is not None else None)\n",
    "\n",
    "# >>> IMPORTANT: Drop original categorical columns post-TE so models get numeric-only features <<<\n",
    "X_te = X_te.drop(columns=cat_cols, errors=\"ignore\")\n",
    "if X_test_te is not None:\n",
    "    X_test_te = X_test_te.drop(columns=cat_cols, errors=\"ignore\")\n",
    "\n",
    "# -------------------- Advanced Feature Engineering -------------------- #\n",
    "print(\"\\n[Advanced Features] Creating enhanced features...\")\n",
    "\n",
    "# Get current numerical columns\n",
    "current_num_cols = X_te.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "if USE_ADVANCED_FEATURES:\n",
    "    print(\"  Creating interaction features...\")\n",
    "    if USE_INTERACTION_FEATURES:\n",
    "        X_te = create_interaction_features(X_te, current_num_cols[:8], max_interactions=15)\n",
    "        if X_test_te is not None:\n",
    "            X_test_te = create_interaction_features(X_test_te, current_num_cols[:8], max_interactions=15)\n",
    "    \n",
    "    print(\"  Creating polynomial features...\")\n",
    "    X_te = create_polynomial_features(X_te, current_num_cols[:6], degree=2, max_features=8)\n",
    "    if X_test_te is not None:\n",
    "        X_test_te = create_polynomial_features(X_test_te, current_num_cols[:6], degree=2, max_features=8)\n",
    "    \n",
    "    print(\"  Creating binning features...\")\n",
    "    X_te = create_binning_features(X_te, current_num_cols[:8], n_bins=5)\n",
    "    if X_test_te is not None:\n",
    "        X_test_te = create_binning_features(X_test_te, current_num_cols[:8], n_bins=5)\n",
    "    \n",
    "    print(\"  Creating cluster features...\")\n",
    "    X_te = create_cluster_features(X_te, current_num_cols, n_clusters=8)\n",
    "    if X_test_te is not None:\n",
    "        X_test_te = create_cluster_features(X_test_te, current_num_cols, n_clusters=8)\n",
    "\n",
    "if USE_NOISE_INJECTION:\n",
    "    print(\"  Adding noise features for regularization...\")\n",
    "    X_te = add_noise_features(X_te, noise_level=NOISE_LEVEL, n_features=3)\n",
    "    if X_test_te is not None:\n",
    "        X_test_te = add_noise_features(X_test_te, noise_level=NOISE_LEVEL, n_features=3)\n",
    "\n",
    "print(f\"  Advanced feature engineering complete. New shape: {X_te.shape}\")\n",
    "\n",
    "# Clean any infinite values introduced by feature engineering\n",
    "for df in [X_te] + ([X_test_te] if X_test_te is not None else []):\n",
    "    if df is None:\n",
    "        continue\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Feature selection\n",
    "if USE_FEATURE_SELECTION and len(X_te.columns) > SELECT_TOP_K_FEATURES:\n",
    "    print(f\"Selecting top {SELECT_TOP_K_FEATURES} features...\")\n",
    "    selected_features = advanced_feature_selection(X_te, y_full, SELECT_TOP_K_FEATURES)\n",
    "    X_te = X_te[selected_features]\n",
    "    if X_test_te is not None:\n",
    "        X_test_te = X_test_te[selected_features]\n",
    "    print(f\"Selected {len(selected_features)} features\")\n",
    "\n",
    "# Enhanced scaling for linear models\n",
    "scaler = QuantileTransformer(output_distribution='normal', random_state=SEED)\n",
    "X_lin = X_te.copy()\n",
    "lin_num_cols = X_lin.select_dtypes(include=[np.number]).columns\n",
    "X_lin[lin_num_cols] = scaler.fit_transform(X_lin[lin_num_cols])\n",
    "\n",
    "if X_test_te is not None:\n",
    "    X_test_lin = X_test_te.copy()\n",
    "    lin_test_num = X_test_lin.select_dtypes(include=[np.number]).columns\n",
    "    X_test_lin[lin_test_num] = scaler.transform(X_test_lin[lin_test_num])\n",
    "else:\n",
    "    X_test_lin = None\n",
    "\n",
    "# Optional target log transform\n",
    "use_log = False\n",
    "if AUTO_LOG_TARGET:\n",
    "    skew_val = pd.Series(y_full).skew()\n",
    "    if skew_val > 1.0:\n",
    "        use_log = True\n",
    "        y_work = np.log1p(y_full)\n",
    "    else:\n",
    "        y_work = y_full.copy()\n",
    "else:\n",
    "    y_work = y_full.copy()\n",
    "\n",
    "# Stratified folds by target bins (for stability)\n",
    "strat_bins = make_target_bins_improved(y_full, n_bins=8)\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "# -------------------- Base Models with Optuna Optimization -------------------- #\n",
    "base_models: Dict[str, Tuple[object, str]] = {}\n",
    "\n",
    "# Run Optuna optimization for key models if enabled\n",
    "optimized_params = {}\n",
    "if USE_OPTUNA:\n",
    "    print(\"\\n=== OPTUNA HYPERPARAMETER OPTIMIZATION ===\")\n",
    "    \n",
    "    # List of models to optimize\n",
    "    models_to_optimize = []\n",
    "    if HAS_LGB:\n",
    "        models_to_optimize.append(\"LGBM\")\n",
    "    if HAS_XGB:\n",
    "        models_to_optimize.append(\"XGB\")\n",
    "    if HAS_CAT:\n",
    "        models_to_optimize.append(\"CAT\")\n",
    "    models_to_optimize.append(\"RF\")  # Always available\n",
    "    \n",
    "    # Run optimization for each model\n",
    "    for model_name in models_to_optimize:\n",
    "        best_params = run_optuna_optimization(X_te, y_work, model_name)\n",
    "        if best_params is not None:\n",
    "            optimized_params[model_name] = best_params\n",
    "    \n",
    "    print(f\"\\n[Optuna] Optimization complete. Optimized {len(optimized_params)} models.\")\n",
    "\n",
    "# Build models with optimized or default parameters\n",
    "if HAS_LGB:\n",
    "    if \"LGBM\" in optimized_params:\n",
    "        lgb_params = optimized_params[\"LGBM\"]\n",
    "        lgb_params.update({\n",
    "            'random_state': SEED,\n",
    "            'verbose': -1,\n",
    "            'n_jobs': 1,\n",
    "            'objective': 'regression',\n",
    "            'boosting_type': 'gbdt'\n",
    "        })\n",
    "    else:\n",
    "        lgb_params = {\n",
    "            'n_estimators': 800,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 6,\n",
    "            'num_leaves': 31,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 1.0,\n",
    "            'min_child_samples': 20,\n",
    "            'random_state': SEED,\n",
    "            'verbose': -1,\n",
    "            'n_jobs': 1,\n",
    "            'objective': 'regression',\n",
    "            'boosting_type': 'gbdt'\n",
    "        }\n",
    "    \n",
    "    base_models[\"LGBM\"] = (LGBMRegressor(**lgb_params), \"tree\")\n",
    "\n",
    "if HAS_XGB:\n",
    "    if \"XGB\" in optimized_params:\n",
    "        xgb_params = optimized_params[\"XGB\"]\n",
    "        xgb_params.update({\n",
    "            'random_state': SEED,\n",
    "            'n_jobs': 1,\n",
    "            'tree_method': 'hist'\n",
    "        })\n",
    "    else:\n",
    "        xgb_params = {\n",
    "            'n_estimators': 1200,\n",
    "            'learning_rate': 0.03,\n",
    "            'max_depth': 8,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.0,\n",
    "            'reg_lambda': 1.0,\n",
    "            'random_state': SEED,\n",
    "            'n_jobs': 1,\n",
    "            'tree_method': 'hist'\n",
    "        }\n",
    "    \n",
    "    base_models[\"XGB\"] = (XGBRegressor(**xgb_params), \"tree\")\n",
    "\n",
    "if HAS_CAT:\n",
    "    if \"CAT\" in optimized_params:\n",
    "        cat_params = optimized_params[\"CAT\"]\n",
    "        cat_params.update({\n",
    "            'random_state': SEED,\n",
    "            'verbose': False,\n",
    "            'loss_function': 'RMSE'\n",
    "        })\n",
    "    else:\n",
    "        cat_params = {\n",
    "            'depth': 8,\n",
    "            'learning_rate': 0.03,\n",
    "            'iterations': 1500,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': SEED,\n",
    "            'loss_function': 'RMSE',\n",
    "            'verbose': False\n",
    "        }\n",
    "    \n",
    "    base_models[\"CAT\"] = (CatBoostRegressor(**cat_params), \"tree\")\n",
    "\n",
    "# Random Forest with optimization\n",
    "if \"RF\" in optimized_params:\n",
    "    rf_params = optimized_params[\"RF\"]\n",
    "    rf_params.update({\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "else:\n",
    "    rf_params = {\n",
    "        'n_estimators': 800,\n",
    "        'max_depth': None,\n",
    "        'max_features': 'sqrt',\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "base_models[\"RF\"] = (RandomForestRegressor(**rf_params), \"tree\")\n",
    "\n",
    "# Strong classical ensembles (not optimized with Optuna for simplicity and to prevent overfitting)\n",
    "base_models[\"ET\"] = (\n",
    "    ExtraTreesRegressor(\n",
    "        n_estimators=800,\n",
    "        max_depth=None,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"GBDT\"] = (\n",
    "    GradientBoostingRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=3,\n",
    "        random_state=SEED,\n",
    "    ),\n",
    "    \"tree\",\n",
    ")\n",
    "\n",
    "base_models[\"Ridge\"] = (\n",
    "    Ridge(alpha=5.0, random_state=SEED),\n",
    "    \"linear\",\n",
    ")\n",
    "base_models[\"Lasso\"] = (\n",
    "    Lasso(alpha=0.0005, random_state=SEED, max_iter=20000),\n",
    "    \"linear\",\n",
    ")\n",
    "\n",
    "# -------------- OOF Training / Prediction -------------- #\n",
    "models_oof = {}\n",
    "models_test_pred = {}\n",
    "models_fold_objs = {}\n",
    "fold_scores = []\n",
    "\n",
    "X_tree = X_te\n",
    "X_tree_test = X_test_te\n",
    "\n",
    "X_linear = X_lin\n",
    "X_linear_test = X_test_lin\n",
    "\n",
    "for name, (model, mtype) in base_models.items():\n",
    "    print(f\"\\n[Model={name}] training with {NFOLDS}-fold CV ...\")\n",
    "    oof = np.zeros(len(X_tree))\n",
    "    test_preds_folds = []\n",
    "    fold_importances = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_tree, strat_bins)):\n",
    "        if mtype == \"tree\":\n",
    "            Xtr, Xva = X_tree.iloc[tr_idx], X_tree.iloc[va_idx]\n",
    "        else:\n",
    "            Xtr, Xva = X_linear.iloc[tr_idx], X_linear.iloc[va_idx]\n",
    "\n",
    "        ytr, yva = y_work.iloc[tr_idx], y_work.iloc[va_idx]\n",
    "\n",
    "        # Fresh instance per fold\n",
    "        m = type(model)(**model.get_params())\n",
    "\n",
    "        # Special handling for different model types\n",
    "        if name == \"LGBM\" and HAS_LGB:\n",
    "            # LightGBM specific handling with clean feature names and reset index\n",
    "            Xtr_clean = Xtr.copy().reset_index(drop=True)\n",
    "            Xva_clean = Xva.copy().reset_index(drop=True)\n",
    "            ytr_clean = ytr.reset_index(drop=True)\n",
    "            yva_clean = yva.reset_index(drop=True)\n",
    "            \n",
    "            # Ensure all data is numeric and clean feature names\n",
    "            Xtr_clean = Xtr_clean.select_dtypes(include=[np.number])\n",
    "            Xva_clean = Xva_clean.select_dtypes(include=[np.number])\n",
    "            \n",
    "            # Create simple sequential feature names to avoid any naming conflicts\n",
    "            n_features = len(Xtr_clean.columns)\n",
    "            simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "            \n",
    "            Xtr_clean.columns = simple_names\n",
    "            Xva_clean.columns = simple_names\n",
    "            \n",
    "            try:\n",
    "                # Use minimal parameters to avoid conflicts\n",
    "                lgb_params = {\n",
    "                    'n_estimators': 500,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'max_depth': 6,\n",
    "                    'num_leaves': 31,\n",
    "                    'subsample': 0.8,\n",
    "                    'colsample_bytree': 0.8,\n",
    "                    'random_state': SEED,\n",
    "                    'verbose': -1,\n",
    "                    'n_jobs': 1\n",
    "                }\n",
    "                m = LGBMRegressor(**lgb_params)\n",
    "                \n",
    "                # Fit without eval_set to avoid feature name conflicts\n",
    "                m.fit(Xtr_clean, ytr_clean)\n",
    "                pred_va = m.predict(Xva_clean)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: LGBM fold {fold} failed: {e}\")\n",
    "                pred_va = np.full(len(yva_clean), ytr_clean.mean())  # Fallback prediction\n",
    "                \n",
    "        elif name == \"CAT\" and HAS_CAT:\n",
    "            # CatBoost requires numeric input here (since we dropped original categorical cols).\n",
    "            m.fit(Xtr.astype(np.float32), ytr, eval_set=(Xva.astype(np.float32), yva), verbose=False)\n",
    "            pred_va = m.predict(Xva.astype(np.float32))\n",
    "        else:\n",
    "            m.fit(Xtr, ytr)\n",
    "            pred_va = m.predict(Xva)\n",
    "\n",
    "        oof[va_idx] = pred_va\n",
    "\n",
    "        if X_tree_test is not None:\n",
    "            Xte = X_tree_test if mtype == \"tree\" else X_linear_test\n",
    "            if name == \"LGBM\" and HAS_LGB:\n",
    "                # Apply same feature name cleaning for test predictions\n",
    "                Xte_clean = Xte.copy().reset_index(drop=True)\n",
    "                Xte_clean = Xte_clean.select_dtypes(include=[np.number])\n",
    "                \n",
    "                # Use same simple sequential names\n",
    "                n_features = len(Xte_clean.columns)\n",
    "                simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "                Xte_clean.columns = simple_names\n",
    "                \n",
    "                try:\n",
    "                    test_preds_folds.append(m.predict(Xte_clean))\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: LGBM test prediction failed: {e}\")\n",
    "                    test_preds_folds.append(np.full(len(Xte_clean), oof[oof != 0].mean() if len(oof[oof != 0]) > 0 else 0))\n",
    "            elif name == \"CAT\" and HAS_CAT:\n",
    "                test_preds_folds.append(m.predict(Xte.astype(np.float32)))\n",
    "            else:\n",
    "                test_preds_folds.append(m.predict(Xte))\n",
    "\n",
    "        # Save feature importances when available\n",
    "        if SAVE_FEATURE_IMPORTANCES and hasattr(m, \"feature_importances_\"):\n",
    "            fi = pd.DataFrame({\n",
    "                \"feature\": Xtr.columns,\n",
    "                \"importance\": m.feature_importances_,\n",
    "                \"fold\": fold,\n",
    "                \"model\": name,\n",
    "            })\n",
    "            fold_importances.append(fi)\n",
    "\n",
    "        models_fold_objs.setdefault(name, []).append(m)\n",
    "\n",
    "    # Back-transform if log was used\n",
    "    oof_final = np.expm1(oof) if use_log else oof\n",
    "    rmse_score = rmse(y_full, oof_final)\n",
    "\n",
    "    models_oof[name] = oof_final\n",
    "    if test_preds_folds:\n",
    "        pred_test_mean = np.mean(test_preds_folds, axis=0)\n",
    "        pred_test_final = np.expm1(pred_test_mean) if use_log else pred_test_mean\n",
    "        models_test_pred[name] = pred_test_final\n",
    "\n",
    "    fold_scores.append({\"model\": name, \"cv_rmse\": rmse_score})\n",
    "    print(f\"[Model={name}] CV RMSE: {rmse_score:.5f}\")\n",
    "\n",
    "    if SAVE_FEATURE_IMPORTANCES and len(fold_importances) > 0:\n",
    "        imp_df = pd.concat(fold_importances, ignore_index=True)\n",
    "        imp_df.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).head(50).to_csv(\n",
    "            f\"feature_importances_{name}.csv\", index=True\n",
    "        )\n",
    "\n",
    "# -------------------- Advanced Multi-Level Stacking -------------------- #\n",
    "print(\"\\n=== ADVANCED MULTI-LEVEL STACKING ===\")\n",
    "\n",
    "# Pseudo-labeling for semi-supervised learning\n",
    "if USE_PSEUDO_LABELING and X_test_te is not None:\n",
    "    print(\"\\n[Pseudo-Labeling] Creating high-confidence pseudo-labels...\")\n",
    "    pseudo_X, pseudo_y = create_pseudo_labels(X_test_te, models_oof, models_test_pred, PSEUDO_LABEL_THRESHOLD)\n",
    "    \n",
    "    if len(pseudo_y) > 0:\n",
    "        print(f\"  Generated {len(pseudo_y)} pseudo-labels with confidence >= {PSEUDO_LABEL_THRESHOLD}\")\n",
    "        \n",
    "        # Add pseudo-labels to training data\n",
    "        X_extended = pd.concat([X_te, pseudo_X], ignore_index=True)\n",
    "        y_extended = pd.concat([y_full, pseudo_y], ignore_index=True)\n",
    "        \n",
    "        # Retrain top 3 models with pseudo-labels\n",
    "        top_models = sorted(fold_scores, key=lambda d: d[\"cv_rmse\"])[:3]\n",
    "        for model_info in top_models:\n",
    "            model_name = model_info[\"model\"]\n",
    "            if model_name in base_models:\n",
    "                print(f\"  Retraining {model_name} with pseudo-labels...\")\n",
    "                model, mtype = base_models[model_name]\n",
    "                \n",
    "                # Create fresh model instance\n",
    "                m = type(model)(**model.get_params())\n",
    "                \n",
    "                if mtype == \"tree\":\n",
    "                    X_train_data = X_extended\n",
    "                else:\n",
    "                    # Scale extended data for linear models\n",
    "                    scaler_extended = QuantileTransformer(output_distribution='normal', random_state=SEED)\n",
    "                    X_train_data = X_extended.copy()\n",
    "                    numeric_cols = X_train_data.select_dtypes(include=[np.number]).columns\n",
    "                    X_train_data[numeric_cols] = scaler_extended.fit_transform(X_train_data[numeric_cols])\n",
    "                \n",
    "                # Special handling for different model types\n",
    "                if model_name == \"LGBM\" and HAS_LGB:\n",
    "                    X_clean = X_train_data.select_dtypes(include=[np.number]).reset_index(drop=True)\n",
    "                    n_features = len(X_clean.columns)\n",
    "                    simple_names = [f'feature_{i}' for i in range(n_features)]\n",
    "                    X_clean.columns = simple_names\n",
    "                    try:\n",
    "                        m.fit(X_clean, y_extended.reset_index(drop=True))\n",
    "                        # Update test predictions\n",
    "                        X_test_clean = X_test_te.select_dtypes(include=[np.number]).reset_index(drop=True)\n",
    "                        X_test_clean.columns = simple_names\n",
    "                        models_test_pred[model_name] = m.predict(X_test_clean)\n",
    "                    except:\n",
    "                        print(f\"    Warning: {model_name} pseudo-label retraining failed\")\n",
    "                elif model_name == \"CAT\" and HAS_CAT:\n",
    "                    m.fit(X_train_data.astype(np.float32), y_extended)\n",
    "                    models_test_pred[model_name] = m.predict(X_test_te.astype(np.float32))\n",
    "                else:\n",
    "                    m.fit(X_train_data, y_extended)\n",
    "                    test_data = X_test_te if mtype == \"tree\" else X_test_lin\n",
    "                    if test_data is not None:\n",
    "                        models_test_pred[model_name] = m.predict(test_data)\n",
    "    else:\n",
    "        print(\"  No high-confidence pseudo-labels generated\")\n",
    "\n",
    "# Level 1: Base model predictions (already computed)\n",
    "oof_df = pd.DataFrame(models_oof)\n",
    "print(f\"\\n[Level 1] Base models OOF shape: {oof_df.shape}\")\n",
    "\n",
    "# Level 2: Meta-features and diverse meta-learners\n",
    "if USE_MULTI_LEVEL_STACKING:\n",
    "    print(\"\\n[Level 2] Training diverse meta-learners...\")\n",
    "    \n",
    "    # Create enhanced meta-features\n",
    "    meta_features = create_meta_features(X_te, models_oof)\n",
    "    \n",
    "    # Multiple meta-learners for diversity\n",
    "    meta_models = {\n",
    "        'GBDT_meta': GradientBoostingRegressor(\n",
    "            n_estimators=300, learning_rate=0.05, max_depth=3, \n",
    "            subsample=0.8, random_state=SEED\n",
    "        ),\n",
    "        'RF_meta': RandomForestRegressor(\n",
    "            n_estimators=200, max_depth=8, random_state=SEED, n_jobs=-1\n",
    "        ),\n",
    "        'Ridge_meta': Ridge(alpha=10.0, random_state=SEED),\n",
    "    }\n",
    "    \n",
    "    meta_oof_preds = {}\n",
    "    meta_test_preds = {}\n",
    "    \n",
    "    for meta_name, meta_model in meta_models.items():\n",
    "        print(f\"  Training {meta_name}...\")\n",
    "        \n",
    "        # Train on OOF predictions\n",
    "        meta_model.fit(oof_df, y_full)\n",
    "        meta_oof = meta_model.predict(oof_df)\n",
    "        meta_oof_preds[meta_name] = meta_oof\n",
    "        \n",
    "        # Predict on test set\n",
    "        if X_test_te is not None and len(models_test_pred) > 0:\n",
    "            test_meta_df = pd.DataFrame(models_test_pred)\n",
    "            meta_test_preds[meta_name] = meta_model.predict(test_meta_df)\n",
    "        \n",
    "        rmse_score = rmse(y_full, meta_oof)\n",
    "        print(f\"    {meta_name} CV RMSE: {rmse_score:.5f}\")\n",
    "    \n",
    "    # Level 3: Final ensemble of meta-learners\n",
    "    print(\"\\n[Level 3] Final ensemble...\")\n",
    "    \n",
    "    if USE_ADAPTIVE_WEIGHTS:\n",
    "        # Calculate adaptive weights based on performance\n",
    "        model_scores = [{\"model\": name, \"cv_rmse\": rmse(y_full, pred)} \n",
    "                       for name, pred in meta_oof_preds.items()]\n",
    "        adaptive_weights = calculate_adaptive_weights([{m[\"model\"]: m[\"cv_rmse\"] for m in model_scores}])\n",
    "        \n",
    "        print(\"  Adaptive weights:\")\n",
    "        for model, weight in adaptive_weights.items():\n",
    "            print(f\"    {model}: {weight:.3f}\")\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        final_oof = np.zeros(len(y_full))\n",
    "        final_test = np.zeros(len(X_test_te) if X_test_te is not None else 0)\n",
    "        \n",
    "        for meta_name, weight in adaptive_weights.items():\n",
    "            if meta_name in meta_oof_preds:\n",
    "                final_oof += weight * meta_oof_preds[meta_name]\n",
    "                if meta_name in meta_test_preds:\n",
    "                    final_test += weight * meta_test_preds[meta_name]\n",
    "    else:\n",
    "        # Simple average ensemble\n",
    "        final_oof = np.mean(list(meta_oof_preds.values()), axis=0)\n",
    "        if meta_test_preds:\n",
    "            final_test = np.mean(list(meta_test_preds.values()), axis=0)\n",
    "    \n",
    "    final_rmse = rmse(y_full, final_oof)\n",
    "    print(f\"  Final ensemble CV RMSE: {final_rmse:.5f}\")\n",
    "    \n",
    "    # Store final predictions\n",
    "    meta_oof = final_oof\n",
    "    stack_rmse = final_rmse\n",
    "    \n",
    "else:\n",
    "    # Original simple stacking\n",
    "    print(\"\\n[Simple Stacking] Training single meta-model...\")\n",
    "    meta_model = GradientBoostingRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, max_depth=3, \n",
    "        subsample=0.8, random_state=SEED\n",
    "    )\n",
    "    meta_model.fit(oof_df, y_full)\n",
    "    meta_oof = meta_model.predict(oof_df)\n",
    "    stack_rmse = rmse(y_full, meta_oof)\n",
    "    print(f\"  Meta CV RMSE: {stack_rmse:.5f}\")\n",
    "\n",
    "# Weighted Blend of the best individual models (for comparison)\n",
    "sorted_models = sorted(fold_scores, key=lambda d: d[\"cv_rmse\"])[: min(3, len(fold_scores))]\n",
    "weights = []\n",
    "for d in sorted_models:\n",
    "    w = 1.0 / max(d[\"cv_rmse\"], 1e-6)\n",
    "    weights.append(w)\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "print(\"\\n[Traditional Blending] Top models & weights:\")\n",
    "for (d, w) in zip(sorted_models, weights):\n",
    "    print(f\"  - {d['model']}: weight={w:.3f}, cv_rmse={d['cv_rmse']:.5f}\")\n",
    "\n",
    "# -------------------- Export OOF / Test Predictions -------------------- #\n",
    "pd.DataFrame({\"id\": train[ID_COL], \"y\": y_full, **models_oof}).to_csv(\"oof_predictions.csv\", index=False)\n",
    "pd.DataFrame(fold_scores).to_csv(\"fold_scores.csv\", index=False)\n",
    "\n",
    "# Final test prediction (if test exists)\n",
    "if X_test_full is not None and len(models_test_pred) > 0:\n",
    "    if USE_MULTI_LEVEL_STACKING and 'final_test' in locals():\n",
    "        # Use advanced multi-level ensemble prediction\n",
    "        final_pred = final_test\n",
    "        print(f\"\\nUsing advanced multi-level ensemble prediction\")\n",
    "    else:\n",
    "        # Fallback to traditional stacking\n",
    "        test_stack_mat = pd.DataFrame(models_test_pred)\n",
    "        \n",
    "        if 'meta_model' in locals():\n",
    "            stack_pred = meta_model.predict(test_stack_mat)\n",
    "        else:\n",
    "            stack_pred = test_stack_mat.mean(axis=1).values\n",
    "        \n",
    "        # Weighted blend of top models\n",
    "        blend_pred = np.zeros(len(test_stack_mat))\n",
    "        for (d, w) in zip(sorted_models, weights):\n",
    "            if d[\"model\"] in test_stack_mat.columns:\n",
    "                blend_pred += w * test_stack_mat[d[\"model\"]].values\n",
    "        \n",
    "        # Final combo: average of stack & blend (often stabilizes)\n",
    "        final_pred = 0.6 * stack_pred + 0.4 * blend_pred  # Slightly favor stacking\n",
    "        print(f\"\\nUsing traditional stacking + blending\")\n",
    "\n",
    "    sub = pd.DataFrame({ID_COL: test[ID_COL], TARGET: final_pred})\n",
    "    sub.to_csv(\"submission_tuned.csv\", index=False)\n",
    "    print(\"Saved submission_tuned.csv\")\n",
    "\n",
    "    print(\"\\n=== ULTRA-TUNED PIPELINE SUMMARY ===\")\n",
    "    print(f\"Advanced Stack CV RMSE: {stack_rmse:.5f}\")\n",
    "    print(f\"Feature Engineering: {X_te.shape[1]} total features\")\n",
    "    print(f\"Pseudo-labeling: {'Enabled' if USE_PSEUDO_LABELING else 'Disabled'}\")\n",
    "    print(f\"Multi-level Stacking: {'Enabled' if USE_MULTI_LEVEL_STACKING else 'Disabled'}\")\n",
    "    print(f\"Adaptive Weights: {'Enabled' if USE_ADAPTIVE_WEIGHTS else 'Disabled'}\")\n",
    "    \n",
    "    print(\"\\nIndividual Model Performance:\")\n",
    "    for d in sorted(fold_scores, key=lambda d: d[\"cv_rmse\"]):\n",
    "        print(f\"{d['model']:<8} | CV RMSE: {d['cv_rmse']:.5f}\")\n",
    "    \n",
    "    print(f\"\\nSubmission Details:\")\n",
    "    print(f\"Shape: {sub.shape}\")\n",
    "    print(f\"Target stats: mean={final_pred.mean():.4f} | std={final_pred.std():.4f}\")\n",
    "    print(f\"Range: [{final_pred.min():.4f}, {final_pred.max():.4f}]\")\n",
    "    \n",
    "    # Save additional analysis\n",
    "    analysis = {\n",
    "        'advanced_stack_rmse': stack_rmse,\n",
    "        'total_features': X_te.shape[1],\n",
    "        'pseudo_labeling': USE_PSEUDO_LABELING,\n",
    "        'multi_level_stacking': USE_MULTI_LEVEL_STACKING,\n",
    "        'adaptive_weights': USE_ADAPTIVE_WEIGHTS,\n",
    "        'individual_scores': {d['model']: d['cv_rmse'] for d in fold_scores}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb191a",
   "metadata": {},
   "source": [
    "# Approach using NNs (which didn't work out...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "HAS_XGB = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "NFOLDS = 5\n",
    "TARGET = \"CORRUCYSTIC_DENSITY\"\n",
    "ID_COL = \"LOCAL_IDENTIFIER\"\n",
    "USE_LOG_TARGET = False\n",
    "SMOOTHING_M = 10.0\n",
    "\n",
    "# Neural Network Configuration\n",
    "USE_MPS = torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "DEVICE = \"mps\" if USE_MPS else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NN_EPOCHS = 100\n",
    "NN_BATCH_SIZE = 256\n",
    "NN_LEARNING_RATE = 0.001\n",
    "NN_PATIENCE = 15\n",
    "\n",
    "CREATE_ADVANCED_FEATURES = True\n",
    "N_CLUSTERS = 5\n",
    "N_PCA_COMPONENTS = 8\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if USE_MPS:\n",
    "    print(\"✅ MPS (Apple Silicon GPU) acceleration enabled!\")\n",
    "\n",
    "def seed_everything(seed: int = SEED):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def read_first_existing(paths: List[str]) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(c)).strip(\"_\") for c in df.columns]\n",
    "    df.columns = [re.sub(r\"_+\", \"_\", c) for c in df.columns]\n",
    "    df.columns = [f\"col_{i}\" if c == \"\" else c for i, c in enumerate(df.columns)]\n",
    "    return df\n",
    "\n",
    "def safe_imputation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        # Replace infinite values first\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Simple median imputation\n",
    "        if df[col].isnull().any():\n",
    "            median_val = df[col].median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = 0.0\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in categorical_cols:\n",
    "        mode_val = df[col].mode()\n",
    "        if len(mode_val) > 0:\n",
    "            df[col] = df[col].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df[col] = df[col].fillna(\"UNKNOWN\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_simple_features(df: pd.DataFrame, target_col: str = None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col and target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    \n",
    "    if len(numeric_cols) == 0:\n",
    "        return df\n",
    "    \n",
    "    print(\"Creating simple features...\")\n",
    "    \n",
    "    # Basic statistics (robust)\n",
    "    df['feat_mean'] = df[numeric_cols].mean(axis=1)\n",
    "    df['feat_std'] = df[numeric_cols].std(axis=1).fillna(0)\n",
    "    df['feat_median'] = df[numeric_cols].median(axis=1)\n",
    "    df['feat_min'] = df[numeric_cols].min(axis=1)\n",
    "    df['feat_max'] = df[numeric_cols].max(axis=1)\n",
    "    df['feat_range'] = df['feat_max'] - df['feat_min']\n",
    "    \n",
    "    # Count features\n",
    "    df['feat_positive'] = (df[numeric_cols] > 0).sum(axis=1)\n",
    "    df['feat_negative'] = (df[numeric_cols] < 0).sum(axis=1)\n",
    "    df['feat_zero'] = (df[numeric_cols] == 0).sum(axis=1)\n",
    "    \n",
    "    # Simple ratios\n",
    "    df['feat_pos_ratio'] = df['feat_positive'] / len(numeric_cols)\n",
    "    df['feat_neg_ratio'] = df['feat_negative'] / len(numeric_cols)\n",
    "    \n",
    "    # Handle any remaining infinite/nan values\n",
    "    new_cols = ['feat_mean', 'feat_std', 'feat_median', 'feat_min', 'feat_max', \n",
    "                'feat_range', 'feat_positive', 'feat_negative', 'feat_zero',\n",
    "                'feat_pos_ratio', 'feat_neg_ratio']\n",
    "    \n",
    "    for col in new_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def enhanced_target_encode(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    n_splits: int = NFOLDS,\n",
    "    smoothing_m: float = SMOOTHING_M,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if len(cat_cols) == 0:\n",
    "        return X.copy(), X_test.copy()\n",
    "    \n",
    "    X = X.copy()\n",
    "    X_test = X_test.copy()\n",
    "    global_mean = y.mean()\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        print(f\"Target encoding {col}...\")\n",
    "        oof_encoded = np.full(len(X), global_mean)\n",
    "        test_encoded_list = []\n",
    "        \n",
    "        for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            tr_y = y.iloc[tr_idx]\n",
    "            tr_col = X.iloc[tr_idx][col].astype(str).fillna(\"MISSING\")\n",
    "            val_col = X.iloc[val_idx][col].astype(str).fillna(\"MISSING\")\n",
    "            \n",
    "            # Calculate target mean for each category\n",
    "            target_mean = tr_y.groupby(tr_col).mean()\n",
    "            target_count = tr_y.groupby(tr_col).count()\n",
    "            \n",
    "            # Apply smoothing\n",
    "            smoothed_mean = (target_mean * target_count + global_mean * smoothing_m) / (target_count + smoothing_m)\n",
    "            \n",
    "            # Encode validation set\n",
    "            oof_encoded[val_idx] = val_col.map(smoothed_mean).fillna(global_mean)\n",
    "            \n",
    "            # Encode test set\n",
    "            test_col = X_test[col].astype(str).fillna(\"MISSING\")\n",
    "            test_encoded = test_col.map(smoothed_mean).fillna(global_mean)\n",
    "            test_encoded_list.append(test_encoded)\n",
    "        \n",
    "        # Final encoding\n",
    "        X[f'{col}_te'] = oof_encoded\n",
    "        X_test[f'{col}_te'] = np.mean(test_encoded_list, axis=0)\n",
    "    \n",
    "    return X, X_test\n",
    "\n",
    "class SimpleTabularNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [256, 128, 64]):\n",
    "        super(SimpleTabularNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "def train_simple_nn(X_train, y_train, X_val, y_val, epochs=NN_EPOCHS):\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = SimpleTabularNN(input_dim, [256, 128, 64]).to(DEVICE)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(DEVICE)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(DEVICE)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(DEVICE)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(DEVICE)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=NN_BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=NN_LEARNING_RATE)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= NN_PATIENCE:\n",
    "                break\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss={train_loss/len(train_loader):.6f}, Val Loss={val_loss:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_simple_nn(model, X):\n",
    "    model.eval()\n",
    "    X_tensor = torch.FloatTensor(X).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_tensor).cpu().numpy()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "seed_everything(SEED)\n",
    "print(\"🚀 Enhanced Robust Pipeline Starting...\")\n",
    "\n",
    "# Load data\n",
    "train_path = \"./MiNDAT.csv\"\n",
    "test_path = \"./MiNDAT_UNK.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "train = clean_columns(train)\n",
    "\n",
    "test = pd.read_csv(test_path)\n",
    "test = clean_columns(test)\n",
    "\n",
    "# Data cleaning\n",
    "if TARGET not in train.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET}' not found!\")\n",
    "\n",
    "# Remove rows with missing target\n",
    "train = train.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "\n",
    "# Simple outlier removal using IQR\n",
    "Q1, Q3 = train[TARGET].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "train = train[(train[TARGET] >= lower_bound) & (train[TARGET] <= upper_bound)].reset_index(drop=True)\n",
    "\n",
    "# Prepare features\n",
    "features = [c for c in train.columns if c not in [TARGET, ID_COL]]\n",
    "X_full = train[features].copy()\n",
    "y_full = train[TARGET].copy()\n",
    "\n",
    "if test is not None:\n",
    "    X_test_full = test[features].copy()\n",
    "else:\n",
    "    X_test_full = X_full.iloc[:0].copy()\n",
    "\n",
    "# Safe imputation\n",
    "print(\"🔧 Performing safe imputation...\")\n",
    "X_full = safe_imputation(X_full)\n",
    "if test is not None:\n",
    "    X_test_full = safe_imputation(X_test_full)\n",
    "\n",
    "# Identify column types\n",
    "cat_cols = X_full.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = X_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Target encoding\n",
    "if len(cat_cols) > 0:\n",
    "    print(\"Performing target encoding...\")\n",
    "    X_full, X_test_full = enhanced_target_encode(X_full, y_full, X_test_full, cat_cols)\n",
    "    # Remove original categorical columns\n",
    "    X_full = X_full.drop(columns=cat_cols)\n",
    "    X_test_full = X_test_full.drop(columns=cat_cols)\n",
    "\n",
    "# Feature engineering\n",
    "if CREATE_ADVANCED_FEATURES:\n",
    "    print(\"Creating features...\")\n",
    "    X_full = create_simple_features(X_full, TARGET)\n",
    "    if test is not None:\n",
    "        X_test_full = create_simple_features(X_test_full, TARGET)\n",
    "\n",
    "print(f\"🚀 Final feature count: {X_full.shape[1]}\")\n",
    "\n",
    "# Final safety check and scaling\n",
    "X_full = safe_imputation(X_full)\n",
    "X_test_full = safe_imputation(X_test_full)\n",
    "\n",
    "# Feature scaling\n",
    "print(\"📏 Scaling features...\")\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "X_test_scaled = scaler.transform(X_test_full)\n",
    "\n",
    "# Cross-validation\n",
    "print(f\"\\n🔄 Starting {NFOLDS}-fold cross-validation...\")\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Initialize results storage\n",
    "models = {}\n",
    "oof_predictions = {}\n",
    "test_predictions = {}\n",
    "cv_scores = {}\n",
    "\n",
    "# ==================== Neural Network Training ==================== #\n",
    "print(\"\\n🧠 Training Neural Networks...\")\n",
    "\n",
    "nn_oof = np.zeros(len(X_full))\n",
    "nn_test_preds = []\n",
    "nn_fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    print(f\"\\n📊 Neural Net Fold {fold + 1}/{NFOLDS}\")\n",
    "    \n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y_full.iloc[train_idx].values, y_full.iloc[val_idx].values\n",
    "    \n",
    "    # Train model\n",
    "    model = train_simple_nn(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred = predict_simple_nn(model, X_val)\n",
    "    nn_oof[val_idx] = val_pred\n",
    "    \n",
    "    # Test predictions\n",
    "    if test is not None:\n",
    "        test_pred = predict_simple_nn(model, X_test_scaled)\n",
    "        nn_test_preds.append(test_pred)\n",
    "    \n",
    "    # Score\n",
    "    fold_rmse = rmse(y_val, val_pred)\n",
    "    nn_fold_scores.append(fold_rmse)\n",
    "    print(f\"🎯 Fold {fold + 1} Neural Net RMSE: {fold_rmse:.6f}\")\n",
    "\n",
    "nn_cv_score = rmse(y_full, nn_oof)\n",
    "print(f\"\\n🧠 Neural Network CV RMSE: {nn_cv_score:.6f}\")\n",
    "\n",
    "# Store neural network results\n",
    "oof_predictions['NeuralNet'] = nn_oof\n",
    "cv_scores['NeuralNet'] = nn_cv_score\n",
    "\n",
    "if test is not None and len(nn_test_preds) > 0:\n",
    "    test_predictions['NeuralNet'] = np.mean(nn_test_preds, axis=0)\n",
    "\n",
    "# ==================== Classical ML Models ==================== #\n",
    "print(\"\\n🔧 Training Classical ML Models...\")\n",
    "\n",
    "# Prepare data for tree models (unscaled)\n",
    "X_tree = X_full.values\n",
    "X_tree_test = X_test_full.values\n",
    "\n",
    "# 1. Random Forest\n",
    "print(\"🌲 Training Random Forest...\")\n",
    "rf_oof = np.zeros(len(X_full))\n",
    "rf_test_preds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_tree)):\n",
    "    X_train, X_val = X_tree[train_idx], X_tree[val_idx]\n",
    "    y_train, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    val_pred = rf.predict(X_val)\n",
    "    rf_oof[val_idx] = val_pred\n",
    "    \n",
    "    if test is not None:\n",
    "        rf_test_preds.append(rf.predict(X_tree_test))\n",
    "\n",
    "rf_cv_score = rmse(y_full, rf_oof)\n",
    "print(f\"🌲 Random Forest CV RMSE: {rf_cv_score:.6f}\")\n",
    "\n",
    "oof_predictions['RandomForest'] = rf_oof\n",
    "cv_scores['RandomForest'] = rf_cv_score\n",
    "if test is not None:\n",
    "    test_predictions['RandomForest'] = np.mean(rf_test_preds, axis=0)\n",
    "\n",
    "# 2. XGBoost (if available)\n",
    "if HAS_XGB:\n",
    "    print(\"🚀 Training XGBoost...\")\n",
    "    xgb_oof = np.zeros(len(X_full))\n",
    "    xgb_test_preds = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_tree)):\n",
    "        X_train, X_val = X_tree[train_idx], X_tree[val_idx]\n",
    "        y_train, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n",
    "        \n",
    "        xgb = XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        xgb.fit(X_train, y_train)\n",
    "        \n",
    "        val_pred = xgb.predict(X_val)\n",
    "        xgb_oof[val_idx] = val_pred\n",
    "        \n",
    "        if test is not None:\n",
    "            xgb_test_preds.append(xgb.predict(X_tree_test))\n",
    "    \n",
    "    xgb_cv_score = rmse(y_full, xgb_oof)\n",
    "    print(f\"🚀 XGBoost CV RMSE: {xgb_cv_score:.6f}\")\n",
    "    \n",
    "    oof_predictions['XGBoost'] = xgb_oof\n",
    "    cv_scores['XGBoost'] = xgb_cv_score\n",
    "    if test is not None:\n",
    "        test_predictions['XGBoost'] = np.mean(xgb_test_preds, axis=0)\n",
    "\n",
    "# 3. Ridge Regression\n",
    "print(\"📈 Training Ridge Regression...\")\n",
    "ridge_oof = np.zeros(len(X_full))\n",
    "ridge_test_preds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n",
    "    \n",
    "    ridge = Ridge(alpha=1.0, random_state=SEED)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    val_pred = ridge.predict(X_val)\n",
    "    ridge_oof[val_idx] = val_pred\n",
    "    \n",
    "    if test is not None:\n",
    "        ridge_test_preds.append(ridge.predict(X_test_scaled))\n",
    "\n",
    "ridge_cv_score = rmse(y_full, ridge_oof)\n",
    "print(f\"📈 Ridge CV RMSE: {ridge_cv_score:.6f}\")\n",
    "\n",
    "oof_predictions['Ridge'] = ridge_oof\n",
    "cv_scores['Ridge'] = ridge_cv_score\n",
    "if test is not None:\n",
    "    test_predictions['Ridge'] = np.mean(ridge_test_preds, axis=0)\n",
    "\n",
    "# ==================== Ensemble ==================== #\n",
    "print(\"\\n🎯 Creating Ensemble...\")\n",
    "\n",
    "# Simple average ensemble\n",
    "if len(oof_predictions) > 1:\n",
    "    ensemble_oof = np.mean(list(oof_predictions.values()), axis=0)\n",
    "    ensemble_cv_score = rmse(y_full, ensemble_oof)\n",
    "    print(f\"🎯 Ensemble CV RMSE: {ensemble_cv_score:.6f}\")\n",
    "    \n",
    "    if test is not None and len(test_predictions) > 0:\n",
    "        ensemble_test = np.mean(list(test_predictions.values()), axis=0)\n",
    "        test_predictions['Ensemble'] = ensemble_test\n",
    "    \n",
    "    oof_predictions['Ensemble'] = ensemble_oof\n",
    "    cv_scores['Ensemble'] = ensemble_cv_score\n",
    "\n",
    "# ==================== Results Summary ==================== #\n",
    "print(\"\\n📊 Final Results Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for model_name, score in sorted(cv_scores.items(), key=lambda x: x[1]):\n",
    "    print(f\"{model_name:<15}: {score:.6f} RMSE\")\n",
    "\n",
    "# OOF predictions\n",
    "oof_df = pd.DataFrame({\n",
    "    'id': train[ID_COL],\n",
    "    'y_true': y_full,\n",
    "    **oof_predictions\n",
    "})\n",
    "oof_df.to_csv('enhanced_oof_predictions.csv', index=False)\n",
    "\n",
    "# Test predictions\n",
    "if test is not None and len(test_predictions) > 0:\n",
    "    # Use best model for final submission\n",
    "    best_model = min(cv_scores.items(), key=lambda x: x[1])[0]\n",
    "    print(f\"🏆 Best model: {best_model} (RMSE: {cv_scores[best_model]:.6f})\")\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        ID_COL: test[ID_COL],\n",
    "        TARGET: test_predictions[best_model]\n",
    "    })\n",
    "    \n",
    "    # Safety check for submission\n",
    "    submission[TARGET] = submission[TARGET].clip(\n",
    "        train[TARGET].quantile(0.01),\n",
    "        train[TARGET].quantile(0.99)\n",
    "    )\n",
    "    \n",
    "    submission.to_csv('enhanced_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db17de",
   "metadata": {},
   "source": [
    "# After this there was another pipeline where i tried optimisign the neural netwrok with Optuna and added a better Meta Learner plus a more robust stacking with both simpler models and advanced models like LGBM, Catboost... however the code for that is something i am not able to find... 🥲"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
